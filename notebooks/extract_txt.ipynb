{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.anthropic.com/news/claude-3-5-sonnet\")\n",
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://readmedium.com/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\")\n",
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in soup.find_all(class_=\"!my-2\"):\n",
    "    e.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover\" name=\"viewport\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/0*RjxEMaY5JU6N_EsV.jpg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*KqpicOFO7jh7FXGjoJ2Bcg.jpeg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*MKcWgwWFa6EL01_JmxPAPQ.jpeg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*LDjQS3c-G1gsojOf24ijGg@2x.jpeg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*cwYWYCjbeXNc_pAtTeq_Zg.jpeg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*TIj_sKKvGVPMjdPRCZvPMg.jpeg\" rel=\"preload\"/>\n",
      "  <link as=\"image\" href=\"https://miro.readmedium.com/v2/resize:fill:88:88/0*-DLO733P-PebUsO_\" rel=\"preload\"/>\n",
      "  <link data-precedence=\"next\" href=\"/_next/static/css/7e012bdb4f33f65e.css\" rel=\"stylesheet\"/>\n",
      "  <link data-precedence=\"next\" href=\"/_next/static/css/8819903482fa0ce9.css\" rel=\"stylesheet\"/>\n",
      "  <link as=\"script\" fetchpriority=\"low\" href=\"/_next/static/chunks/webpack-7c0906a643bf5d80.js\" rel=\"preload\"/>\n",
      "  <script async=\"\" src=\"/_next/static/chunks/fd9d1056-b59a6f32ac74cecf.js\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"/_next/static/chunks/864-fc79a795ba0d747c.js\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"/_next/static/chunks/main-app-d37764a0d4c9aeed.js\">\n",
      "  </script>\n",
      "  <link as=\"script\" href=\"https://www.googletagmanager.com/gtag/js?id=G-YSBN5EJVBL\" rel=\"preload\"/>\n",
      "  <link as=\"script\" href=\"https://static.addtoany.com/menu/page.js\" rel=\"preload\"/>\n",
      "  <link href=\"/favicon.ico\" rel=\"icon\" sizes=\"32x32\"/>\n",
      "  <link href=\"/favicon.svg\" rel=\"icon\" type=\"image/svg+xml\"/>\n",
      "  <title>\n",
      "   Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\n",
      "  </title>\n",
      "  <meta content=\"How to add context to chunks ? Are rerankers a good choice ?\" name=\"description\"/>\n",
      "  <link href=\"https://readmedium.com/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\" rel=\"canonical\"/>\n",
      "  <meta content=\"Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\" property=\"og:title\"/>\n",
      "  <meta content=\"How to add context to chunks ? Are rerankers a good choice ?\" property=\"og:description\"/>\n",
      "  <meta content=\"https://readmedium.com/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\" property=\"og:url\"/>\n",
      "  <meta content=\"summary_large_image\" name=\"twitter:card\"/>\n",
      "  <meta content=\"@readmedium\" name=\"twitter:creator\"/>\n",
      "  <meta content=\"Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\" name=\"twitter:title\"/>\n",
      "  <meta content=\"How to add context to chunks ? Are rerankers a good choice ?\" name=\"twitter:description\"/>\n",
      "  <script>\n",
      "   (self.__next_s=self.__next_s||[]).push([0,{\"children\":\"\\n  (function () {\\n    function getImplicitPreference() {\\n      var mediaQuery = '(prefers-color-scheme: dark)'\\n      var mql = window.matchMedia(mediaQuery)\\n      var hasImplicitPreference = typeof mql.matches === 'boolean'\\n\\n      if (hasImplicitPreference) {\\n        return mql.matches ? 'dark' : 'light'\\n      }\\n\\n      return null\\n    }\\n\\n    function themeIsValid(theme) {\\n      return theme === 'light' || theme === 'dark'\\n    }\\n\\n    var themeToSet = 'light'\\n    var preference = window.localStorage.getItem('payload-theme')\\n\\n    if (themeIsValid(preference)) {\\n      themeToSet = preference\\n    } else {\\n      var implicitPreference = getImplicitPreference()\\n\\n      if (implicitPreference) {\\n        themeToSet = implicitPreference\\n      }\\n    }\\n\\n    // payload 主题控制\\n    document.documentElement.setAttribute('data-theme', themeToSet)\\n    // tailwind主题控制\\n    if(themeToSet === 'dark'){\\n      document.documentElement.classList.add('dark')\\n    }\\n  })();\\n  \"}])\n",
      "  </script>\n",
      "  <script nomodule=\"\" src=\"/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js\">\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <nav class=\"fixed z-10 h-[54px] w-screen flex flex-grow-0 items-center justify-between bg-white dark:bg-black px-4 shadow-md dark:shadow-neutral-900\">\n",
      "   <a class=\"scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition\" href=\"/\">\n",
      "    <img alt=\"Read Medium logo\" class=\"mt-2 mb-2 h-6 w-auto sm:px-3\" data-nimg=\"1\" decoding=\"async\" height=\"37\" loading=\"lazy\" src=\"/_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=384&amp;q=75\" srcset=\"/_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=256&amp;q=75 1x, /_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=384&amp;q=75 2x\" style=\"color:transparent\" width=\"141\"/>\n",
      "   </a>\n",
      "   <div class=\"px-3 flex justify-center items-center\">\n",
      "    <div class=\"h-8 text-black sm:h-9 relative flex items-center justify-center w-full sm:w-60 bg-gray-100 dark:bg-gray-900 rounded-[16px] border-[none]\">\n",
      "     <div class=\"px-1 sm:px-3\">\n",
      "      <svg class=\"text-gray-300\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\">\n",
      "       <path clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\" fill-rule=\"evenodd\">\n",
      "       </path>\n",
      "      </svg>\n",
      "     </div>\n",
      "     <input aria-controls=\"searchResults\" aria-expanded=\"false\" aria-label=\"search\" class='w-full leading-5 [font-family:sohne,\"Helvetica_Neue\",Helvetica,Arial,sans-serif] [outline:none] text-sm bg-transparent text-[rgb(36,36,36)] dark:text-white m-0 pl-0 pr-5 py-2.5 border-[none]' data-testid=\"headerSearchInput\" placeholder=\"Search\" role=\"combobox\" value=\"\"/>\n",
      "     <div class=\"invisible self-center fixed left-3 right-3 w-[calc(100vw - 1.5rem)] top-[54px] sm:absolute sm:left-[auto] sm:right-[auto] sm:w-[680px] sm:max-w-[80vw] overflow-y-auto color:text-black dark:text-white bg-white dark:bg-black border-gray-300 dark:border-gray-400 border-[1px] rounded-lg p-3 max-w-[680px] min-h-[32px] max-h-[80vh] sm:max-h-[90vh] flex flex-col justify-start items-center\">\n",
      "      <div class=\"divide-y divide-gray-200 dark:divide-gray-500\">\n",
      "      </div>\n",
      "      <span>\n",
      "       No Results\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "   <div class=\"flex h-full items-center shrink-0\">\n",
      "    <div class=\"scale-90 sm:scale-100 h-9 flex justify-center items-center text-black dark:text-white\">\n",
      "     <span class=\"mr-1\">\n",
      "     </span>\n",
      "     <div class=\"w-9 sm:w-[100px] relative h-full\">\n",
      "      <button aria-expanded=\"false\" aria-haspopup=\"listbox\" class=\"sm:px-2 cursor-pointer h-9 flex justify-center sm:justify-between items-center border-2 border-gray-200 dark:border-gray-700 relative w-full rounded-lg bg-white dark:bg-black py-2 text-left focus:outline-none focus-visible:border-indigo-500 focus-visible:ring-2 focus-visible:ring-white/75 focus-visible:ring-offset-2 focus-visible:ring-offset-orange-300 sm:text-sm\" data-headlessui-state=\"\" id=\"headlessui-listbox-button-:Rqmqqqlla:\" type=\"button\">\n",
      "       <span class=\"truncate text-black dark:text-white hidden sm:block\">\n",
      "        Origin\n",
      "       </span>\n",
      "       <span class=\"block truncate text-black dark:text-white sm:hidden\">\n",
      "        OG\n",
      "       </span>\n",
      "       <svg class=\"w-4 h-4 hidden sm:block text-gray-950 dark:text-gray-500\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.5\" viewbox=\"0 0 20 20\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "        <path d=\"M19.5 8.25l-7.5 7.5-7.5-7.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
      "        </path>\n",
      "       </svg>\n",
      "      </button>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </nav>\n",
      "  <div class=\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx pt-[72px] flex flex-col justify-start items-center bg-white dark:bg-black text-black dark:text-white min-h-full\">\n",
      "   <div class=\"read-medium-post max-w-[680px] max-w-full md:max-w-[680px] pb-16\">\n",
      "    <div class=\"flex justify-between items-center\">\n",
      "     <div class=\"flex justify-start items-center space-x-2\">\n",
      "      <img alt=\"avatar\" class=\"w-11 h-11 rounded-full border border-solid border-[rgba(0,0,0,0.05)]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/0*RjxEMaY5JU6N_EsV.jpg\"/>\n",
      "      <span class=\"text-gray-950 dark:text-white text-xl max-w-[240px] md:max-w-[400px]\">\n",
      "       Andre Camille\n",
      "      </span>\n",
      "     </div>\n",
      "     <div class=\"cursor-pointer relative\">\n",
      "      <div class=\"relative z-10\" data-headlessui-state=\"\">\n",
      "       <button aria-expanded=\"false\" class=\"text-white/90 group inline-flex items-center rounded-md bg-transparent px-3 py-2 text-base font-medium hover:text-white focus:outline-none\" data-headlessui-state=\"\" type=\"button\">\n",
      "        <svg class=\"text-black dark:text-white\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\">\n",
      "         <path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\">\n",
      "         </path>\n",
      "        </svg>\n",
      "       </button>\n",
      "       <div class=\"invisible bg-gray-200 dark:bg-gray-400 w-14 rounded h-auto block absolute left-1/2 z-1 max-w-sm -translate-x-1/2 transform sm:px-0 lg:max-w-3xl\" data-headlessui-state=\"\" id=\"headlessui-popover-panel-:R9kbaqqlla:\" tabindex=\"-1\">\n",
      "        <div class=\"absolute z-[-1] top-0 left-1/2 transform -translate-x-1/2 -mt-[4px]\">\n",
      "         <div class=\"w-3 h-3 bg-gray-200 dark:bg-gray-400 transform rotate-45\">\n",
      "         </div>\n",
      "        </div>\n",
      "        <div class=\"py-3 pb-0 flex flex-col justify-around items-center overflow-hidden rounded-lg\">\n",
      "         <div class=\"w-full space-y-3 flex flex-col justify-around items-center a2a_kit a2a_kit_size_32 a2a_default_style\">\n",
      "          <a class=\"a2a_button_twitter\">\n",
      "          </a>\n",
      "          <a class=\"a2a_button_facebook\">\n",
      "          </a>\n",
      "          <a class=\"a2a_button_linkedin\">\n",
      "          </a>\n",
      "          <a class=\"a2a_button_wechat\">\n",
      "          </a>\n",
      "          <a class=\"a2a_button_qzone\">\n",
      "          </a>\n",
      "         </div>\n",
      "        </div>\n",
      "       </div>\n",
      "      </div>\n",
      "      <div style=\"position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none\">\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"relative mt-4\">\n",
      "     <div class=\"h-[180px] lg:h-[240px] overflow-hidden pb-4 bg-white dark:bg-black w-full px-4 pt-4 rounded-lg shadow-md\">\n",
      "      <div class=\"prose break-words dark:prose-invert prose-p:leading-relaxed prose-pre:p-0 text-[var(--theme-text)]\">\n",
      "       <ul>\n",
      "       </ul>\n",
      "      </div>\n",
      "     </div>\n",
      "     <div class=\"h-[100px] absolute flex items-center justify-center w-full bottom-0 left-0 text-center rounded-lg leading-[100%] cursor-pointer bg-gradient-to-b from-[rgba(255,255,255,0.16)] dark:from-[rgba(0,0,0,0.16)] to-white dark:to-black\">\n",
      "      <svg class=\"mt-[70px] ml-1 w-4 h-4 text-gray-950 dark:text-gray-500\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.5\" viewbox=\"0 0 20 20\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "       <path d=\"M19.5 8.25l-7.5 7.5-7.5-7.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
      "       </path>\n",
      "      </svg>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div>\n",
      "     <body>\n",
      "      <article>\n",
      "       <h1 id=\"c905\">\n",
      "        Enhancing Language Model Performance: Insights into Retrieval-Augmented Generation and Chunking Techniques\n",
      "       </h1>\n",
      "       <figure id=\"8c7a\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4EjerlC79zcw-_zuyKioEw.jpeg\"/>\n",
      "        <figcaption>\n",
      "         Photo de\n",
      "         <a href=\"https://unsplash.com/fr/@impatrickt?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\">\n",
      "          Patrick Tomasso\n",
      "         </a>\n",
      "         sur\n",
      "         <a href=\"https://unsplash.com/fr/photos/lot-a-livre-ouvert-Oaqk7qqNh_c?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\">\n",
      "          Unsplash\n",
      "         </a>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <h1 id=\"a693\">\n",
      "        Introduction\n",
      "       </h1>\n",
      "       <p id=\"e659\">\n",
      "        Large Language Models (LLMs) are now everywhere, offering powerful tools that can be seamlessly integrated with your data thanks to the magic of Retrieval-Augmented Generation (RAG). In this article, we’re diving into some key areas to see how retrieval performance can be boosted with:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         Embedding models: E5, BGE and Solon\n",
      "        </li>\n",
      "        <li>\n",
      "         Chunking augmentation techniques: How to add ‘context’ with minimal efforts to our chunks\n",
      "        </li>\n",
      "        <li>\n",
      "         Reranking: Are rerankers a good addition to a RAG pipeline ?\n",
      "        </li>\n",
      "       </ul>\n",
      "       <p id=\"8853\">\n",
      "        By putting these components to the test, we hope to uncover some valuable insights into how to make the most of language models in real-world applications.\n",
      "       </p>\n",
      "       <h1 id=\"f253\">\n",
      "        Chunk Augmentation: Adding Context for Enhanced Document Retrieval\n",
      "       </h1>\n",
      "       <p id=\"7e3a\">\n",
      "        <i>\n",
      "         The data we’ll be using for this article comes from\n",
      "         <a href=\"https://www.service-public.fr\">\n",
      "          service-public.fr\n",
      "         </a>\n",
      "         and mainly contains guides and answers to French administrative questions. All the data used can be found\n",
      "         <a href=\"https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json\">\n",
      "          here\n",
      "         </a>\n",
      "         .\n",
      "        </i>\n",
      "       </p>\n",
      "       <p id=\"4e11\">\n",
      "        For the preprocessing, we only have to read our json, declare and format the columns we are going to use for the chunking:\n",
      "       </p>\n",
      "       <div id=\"1d43\">\n",
      "        <pre><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n",
      "\n",
      "df =pd.read_json(<span class=\"hljs-string\">\"data/fiches-travail.json\"</span>)\n",
      "<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">prep_specific</span>(<span class=\"hljs-params\">df</span>):\n",
      "    df[<span class=\"hljs-string\">\"sections_all\"</span>] = df[<span class=\"hljs-string\">\"sections\"</span>].astype(<span class=\"hljs-built_in\">str</span>).apply(literal_eval)\n",
      "    df = df.explode(<span class=\"hljs-string\">\"sections_all\"</span>).reset_index(drop=<span class=\"hljs-literal\">True</span>)\n",
      "    df[<span class=\"hljs-string\">\"text_section\"</span>] = df[<span class=\"hljs-string\">\"sections_all\"</span>].apply(<span class=\"hljs-keyword\">lambda</span> x : x[<span class=\"hljs-string\">\"text\"</span>])\n",
      "    df[<span class=\"hljs-string\">\"anchor\"</span>] = df[<span class=\"hljs-string\">\"sections_all\"</span>].apply(<span class=\"hljs-keyword\">lambda</span> x : x[<span class=\"hljs-string\">\"anchor\"</span>])\n",
      "    <span class=\"hljs-keyword\">return</span> df\n",
      "\n",
      "df = prep_specific(df)\n",
      "\n",
      "<span class=\"hljs-comment\"># Column of interest</span>\n",
      "text_col = <span class=\"hljs-string\">'text_section'</span>\n",
      "<span class=\"hljs-comment\"># Metadata columns</span>\n",
      "metadata_cols = [<span class=\"hljs-string\">\"pubId\"</span>, <span class=\"hljs-string\">\"title\"</span>, <span class=\"hljs-string\">\"date\"</span>, <span class=\"hljs-string\">\"anchor\"</span>]\n",
      "\n",
      "df[text_col] = df[text_col].astype(<span class=\"hljs-built_in\">str</span>).<span class=\"hljs-built_in\">str</span>.lower()\n",
      "\n",
      "df[metadata_cols] = df[metadata_cols].astype(<span class=\"hljs-built_in\">str</span>) <span class=\"hljs-comment\">#Ensuring metadata columns are in string format aswell</span>\n",
      "<span class=\"hljs-comment\"># Fast cleaning</span>\n",
      "df = df[~df[<span class=\"hljs-string\">'pubId'</span>].isna()]\n",
      "df = df[~df[<span class=\"hljs-string\">'date'</span>].isna()].reset_index(drop=<span class=\"hljs-literal\">True</span>)\n",
      "\n",
      "df[text_col+<span class=\"hljs-string\">'_clean'</span>] = df[text_col].apply(tools.remove_french_stopwords)\n",
      "<span class=\"hljs-comment\"># Corpus for keywords retrieval</span>\n",
      "corpus = df[text_col+<span class=\"hljs-string\">\"_clean\"</span>]\n",
      "<span class=\"hljs-comment\"># Keywords retrieval (code on git)</span>\n",
      "df[<span class=\"hljs-string\">'keywords'</span>] = tools.extract_keywords_tfidf(df[text_col+<span class=\"hljs-string\">'_clean'</span>], corpus=df[text_col+<span class=\"hljs-string\">'_clean'</span>], top_n=<span class=\"hljs-number\">10</span>)\n",
      "\n",
      "df.to_csv(<span class=\"hljs-string\">'data.csv'</span>)\n",
      "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"df shape: <span class=\"hljs-subst\">{df.shape}</span>\"</span>)</pre>\n",
      "       </div>\n",
      "       <p id=\"0718\">\n",
      "        Chunking is a natural language processing technique that involves breaking down a text into smaller, meaningful units or “chunks”. These chunks typically consist of words or phrases that convey a specific idea or concept within the context of the text. Chunking helps organize and structure information, making it easier for models to analyze and understand the content.\n",
      "       </p>\n",
      "       <figure id=\"ad04\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NMCaeZ07OH3mNdAF0fVhwA.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig1. Visualisation of chunking a text (\n",
      "          <a href=\"https://huggingface.co/spaces/m-ric/chunk_visualizer\">\n",
      "           https://huggingface.co/spaces/m-ric/chunk_visualizer\n",
      "          </a>\n",
      "          )\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"9eef\">\n",
      "        For the preliminary chunking, we are using the Langchain function `RecursiveCharacterTextSplitter` with a maximum character length of 4000, which seems a reasonable size for a future LLM (at the end of the RAG pipeline) with a context length of 8k tokens, to which we’ll be able to give 4–5 chunks as context per request.\n",
      "       </p>\n",
      "       <p id=\"3d83\">\n",
      "        To create each RAG, we will start with a common base, which is a dataframe structured as follows:\n",
      "       </p>\n",
      "       <div id=\"ad8a\">\n",
      "        <pre><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n",
      "<span class=\"hljs-keyword\">from</span> tqdm <span class=\"hljs-keyword\">import</span> tqdm\n",
      "<span class=\"hljs-keyword\">from</span> langchain.document_loaders <span class=\"hljs-keyword\">import</span> DataFrameLoader\n",
      "<span class=\"hljs-keyword\">from</span> langchain_text_splitters <span class=\"hljs-keyword\">import</span> RecursiveCharacterTextSplitter\n",
      "\n",
      "df = pd.read_csv(<span class=\"hljs-string\">'data.csv'</span>)\n",
      "\n",
      "<span class=\"hljs-comment\"># Create every rags from the same base</span>\n",
      "text_col = <span class=\"hljs-string\">'text_section'</span>\n",
      "metadata_cols = [<span class=\"hljs-string\">\"pubId\"</span>, <span class=\"hljs-string\">\"title\"</span>, <span class=\"hljs-string\">\"date\"</span>, <span class=\"hljs-string\">\"keywords\"</span>, <span class=\"hljs-string\">\"anchor\"</span>, <span class=\"hljs-string\">\"url\"</span>]\n",
      "<span class=\"hljs-comment\"># Make a document list with every columns we could need in the metadatas</span>\n",
      "df_loader = DataFrameLoader(df[[text_col]+metadata_cols], page_content_column=text_col)\n",
      "df_document = df_loader.load()\n",
      "<span class=\"hljs-comment\">#Slipt document in chunks</span>\n",
      "text_splitter = RecursiveCharacterTextSplitter(\n",
      "    chunk_size=<span class=\"hljs-number\">4000</span>,\n",
      "    chunk_overlap=<span class=\"hljs-number\">50</span>,\n",
      "    length_function=<span class=\"hljs-built_in\">len</span>,\n",
      "    separators=[<span class=\"hljs-string\">'\\n\\n'</span>, <span class=\"hljs-string\">'. '</span>] <span class=\"hljs-comment\">#Dont cut sentences</span>\n",
      ")\n",
      "baseline_docs = text_splitter.split_documents(df_document)\n",
      "\n",
      "chunk_ids = []\n",
      "chunk_contents = []\n",
      "chunk_metadatas = []\n",
      "<span class=\"hljs-comment\"># Add a chunk_id and potential questions to metadatas</span>\n",
      "llm = tools.load_llm()\n",
      "<span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> tqdm(<span class=\"hljs-built_in\">enumerate</span>(baseline_docs), total=<span class=\"hljs-built_in\">len</span>(baseline_docs), desc=<span class=\"hljs-string\">\"Processing documents\"</span>):\n",
      "    doc.metadata[<span class=\"hljs-string\">'chunk_id'</span>]=i\n",
      "    doc.metadata[<span class=\"hljs-string\">'potential_questions'</span>] = tools.run_question_rag(llm, <span class=\"hljs-string\">\"\"</span>, doc.page_content, prompt=tools.prompt_create_questions)\n",
      "    chunk_ids.append(i)\n",
      "    chunk_contents.append(doc.page_content)\n",
      "    chunk_metadatas.append(doc.metadata)\n",
      "\n",
      "df_chunks = pd.DataFrame()\n",
      "df_chunks[<span class=\"hljs-string\">\"chunk_content\"</span>] = chunk_contents\n",
      "df_chunks[<span class=\"hljs-string\">\"chunk_metadata\"</span>] = chunk_metadatas\n",
      "\n",
      "<span class=\"hljs-comment\"># Recreate the columns from metadatas</span>\n",
      "metadata_df = df_chunks[<span class=\"hljs-string\">'chunk_metadata'</span>].apply(pd.Series)\n",
      "<span class=\"hljs-comment\"># This dataframe will be the base for every rags</span>\n",
      "df_chunks = pd.concat([df_chunks.drop(columns=[<span class=\"hljs-string\">'chunk_metadata'</span>]), metadata_df], axis=<span class=\"hljs-number\">1</span>)</pre>\n",
      "       </div>\n",
      "       <p id=\"b7e8\">\n",
      "        To evaluate the performance of different types of chunks, we will try the following:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         Baseline: The baseline will consist of simple chunks without any added headers or informations\n",
      "        </li>\n",
      "        <li>\n",
      "         Augmented baseline: Every other RAGs will consist of the baseline to which will be added some informations we have on each article of our database. This way, we will add as a header to each chunk data such as\n",
      "         <i>\n",
      "          parent document title\n",
      "         </i>\n",
      "         ,\n",
      "         <i>\n",
      "          parent document date\n",
      "         </i>\n",
      "         ,\n",
      "         <i>\n",
      "          parent document url\n",
      "         </i>\n",
      "         .\n",
      "        </li>\n",
      "        <li>\n",
      "         Keywords: The RAGs we will evaluate that will have keywords in their names will consist of the augmented baseline + Top 10 keywords (found using TF-IDF) of the parent document (again, added in headers).\n",
      "        </li>\n",
      "        <li>\n",
      "         Potential questions: Questions a chunk should be able to answer.\n",
      "        </li>\n",
      "       </ul>\n",
      "       <p id=\"627e\">\n",
      "        The main goal of a RAG is to find relevant documents that should help a model to answer a given question. Given this, we will try a third approach consisting in adding « potential questions » — questions that a given chunk can answer — to each chunk, as always, in header.\n",
      "       </p>\n",
      "       <p id=\"e0da\">\n",
      "        To generate these questions, we gave each chunk we created before to a LLM (mistral 7B — OpenHermes), with the following prompt:\n",
      "       </p>\n",
      "       <blockquote id=\"d4f3\">\n",
      "        <p>\n",
      "         « En t’inspirant du texte ci-dessous, réponds uniquement avec une liste de 2–3 questions très précises et détaillées que l’on pourrait poser sur ce texte et dont la réponse se trouve dans ce texte. Si le texte est très court ne réponds qu’avec une ou deux questions. N’inventes pas des questions qui ne veulent rien dire, si tu n’as qu’une seule question c’est très bien aussi. Les questions ne doivent pas être vagues, et pouvoir être associées très facilement à ce texte si on les comparait à d’autres questions pour d’autres textes. Si une question porte sur l’article de manière générale, le numéro ou l’id de l’article doit être contenu dans la question. Les questions doivent contenir les éléments importants comme des dates ou des numéros si besoin. Voici des exemples pour t’aider:\n",
      "        </p>\n",
      "       </blockquote>\n",
      "       <blockquote id=\"6e7b\">\n",
      "        <p>\n",
      "         Mauvaises questions qu’on ne veut pas: [“De quoi parle ce texte ?”, “Qui est mentionné ici ?”] (trop vague)\n",
      "        </p>\n",
      "       </blockquote>\n",
      "       <blockquote id=\"2991\">\n",
      "        <p>\n",
      "         Bonne question: [“Quel groupe est concerné par la décision XX-XXX-XX ?”] (net et précis)\n",
      "        </p>\n",
      "       </blockquote>\n",
      "       <blockquote id=\"91b3\">\n",
      "        <p>\n",
      "         Voilà le texte: {article} » Prompt 1 — Create potential questions\n",
      "        </p>\n",
      "       </blockquote>\n",
      "       <p id=\"45dd\">\n",
      "        We are now ready to test our baseline and our augmented baseline to which we can add — or not — keywords and/or questions making a total of 5 different types of chunks:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         Baseline\n",
      "        </li>\n",
      "        <li>\n",
      "         Without keywords — without questions (Augmented baseline)\n",
      "        </li>\n",
      "        <li>\n",
      "         With keywords — without questions\n",
      "        </li>\n",
      "        <li>\n",
      "         Without keywords — with questions\n",
      "        </li>\n",
      "        <li>\n",
      "         With keywords — with questions\n",
      "        </li>\n",
      "       </ul>\n",
      "       <h1 id=\"d873\">\n",
      "        Embedding Models : Text representation\n",
      "       </h1>\n",
      "       <p id=\"8f29\">\n",
      "        Text embedding is the process of representing text data, such as words, sentences, or documents, as dense numerical vectors in a high-dimensional vector space. These vector representations aim to capture the semantic and contextual meanings of the text, where similar texts are mapped to vectors that are close together in the vector space.\n",
      "       </p>\n",
      "       <figure id=\"c3ab\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5-GVldd6K4sHULqMhG78bQ.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig2. Embeddings: from text to vectors\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"67d2\">\n",
      "        In this paper, we will compare 3 embedding models against each other which are the following:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         <a href=\"https://huggingface.co/intfloat/multilingual-e5-large\">\n",
      "          intfloat/e5-large\n",
      "         </a>\n",
      "         :\n",
      "         <a href=\"https://arxiv.org/pdf/2402.05672\">\n",
      "          https://arxiv.org/pdf/2402.05672\n",
      "         </a>\n",
      "        </li>\n",
      "        <li>\n",
      "         <a href=\"https://huggingface.co/BAAI/bge-m3\">\n",
      "          BAAI/bge-m3\n",
      "         </a>\n",
      "         :\n",
      "         <a href=\"https://arxiv.org/pdf/2402.03216\">\n",
      "          https://arxiv.org/pdf/2402.03216\n",
      "         </a>\n",
      "        </li>\n",
      "        <li>\n",
      "         <a href=\"https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1\">\n",
      "          OrdalieTech/Solon-embeddings-large-0.1\n",
      "         </a>\n",
      "         : SOTA Open source french embedding model.\n",
      "        </li>\n",
      "       </ul>\n",
      "       <p id=\"308b\">\n",
      "        The e5-large model is a text embedding model developed by Microsoft that uses weakly-supervised contrastive pre-training to generate high-quality embeddings. The model is trained on a mixture of datasets and can be used for various natural language processing tasks such as semantic similarity, information retrieval, and text clustering by encoding input text into dense vector representations.\n",
      "       </p>\n",
      "       <p id=\"41a2\">\n",
      "        The bge-m3 model is a multilingual text embedding model developed by the Beijing Academy of Artificial Intelligence (BAAI). It supports over 100 languages and can handle long input sequences up to 8192 tokens. This makes it suitable for embedding multi-lingual and long-form text data.\n",
      "       </p>\n",
      "       <p id=\"ec61\">\n",
      "        Finally, the Solon model is a State-Of-The-Art Open source french embedding model. Data used in the paper being in French, we had to test it against robust multilingual models.\n",
      "       </p>\n",
      "       <h1 id=\"9170\">\n",
      "        Rerankers\n",
      "       </h1>\n",
      "       <p id=\"3ff7\">\n",
      "        During our tests, we’ll also explore reranking, a method employing a reranking model, often referred to as a cross-encoder. This model assesses the similarity between each query-document pair, enabling us to rearrange the documents according to their relevance to the query. After the initial search using similarity scores, the reranker is then applied, allowing for a broader selection of documents. For instance, we might retrieve the 20 most similar documents, rerank them, and select only the top 5 to provide as context for our LLM.\n",
      "       </p>\n",
      "       <p id=\"02bd\">\n",
      "        The reranker model used in this article is\n",
      "        <a href=\"https://huggingface.co/antoinelouis/crossencoder-electra-base-mmarcoFR\">\n",
      "         crossencoder-electra-base-french-mmarcoFR\n",
      "        </a>\n",
      "        , a model trained on French training samples from the\n",
      "        <a href=\"https://huggingface.co/datasets/unicamp-dl/mmarco\">\n",
      "         mMARCO\n",
      "        </a>\n",
      "        dataset.\n",
      "       </p>\n",
      "       <div id=\"328a\">\n",
      "        <pre><span class=\"hljs-comment\"># Load reranker</span>\n",
      "<span class=\"hljs-keyword\">from</span> sentence_transformers <span class=\"hljs-keyword\">import</span> CrossEncoder\n",
      "model = CrossEncoder(<span class=\"hljs-string\">'antoinelouis/crossencoder-electra-base-french-mmarcoFR'</span>)</pre>\n",
      "       </div>\n",
      "       <h1 id=\"acf1\">\n",
      "        Evaluation\n",
      "       </h1>\n",
      "       <p id=\"fbbf\">\n",
      "        To evaluate the performance of each chunking techniques and embeddings models, we will use two different sets of questions:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         Synthetic: Out of the 1733 chunks we have in our database, 100 were given to a LLM which output a specific question that should be answerable with the chunk content\n",
      "        </li>\n",
      "        <li>\n",
      "         Human: 20 chunks of the database were chosen randomly by us from which we created more complex questions that should be answerable with the chunk content\n",
      "        </li>\n",
      "       </ul>\n",
      "       <p id=\"8d68\">\n",
      "        We adopted a similar prompt to\n",
      "        <i>\n",
      "         Prompt 1\n",
      "        </i>\n",
      "        for generating synthetic questions. Interestingly, one might notice similarities or even nearly identical questions between the set of synthetic questions and the potential questions added in the header for RAGs containing potential questions. While this observation holds true, it’s important to note that in real-life scenarios, some questions will inevitably overlap with those pre-generated by the potential questions in the header. This overlap serves as one of their primary objectives.\n",
      "       </p>\n",
      "       <p id=\"a05c\">\n",
      "        The accuracy score in graphs represents the % of True (relevant) documents found in top 1, top 2 and so on.\n",
      "       </p>\n",
      "       <h1 id=\"d28b\">\n",
      "        Setting up vector databases\n",
      "       </h1>\n",
      "       <p id=\"ad9a\">\n",
      "        Introduced in part « Augmented Chunking », we will test 5 different RAGs, one for each chunk configuration. Added to that, we will also test 2 techniques using the keywords for the 2 RAGs containing keywords lists in header of chunks:\n",
      "       </p>\n",
      "       <ul>\n",
      "        <li>\n",
      "         Filtered\n",
      "        </li>\n",
      "        <li>\n",
      "         Softkeywords\n",
      "        </li>\n",
      "       </ul>\n",
      "       <p id=\"e3d7\">\n",
      "        For the retrieval, we will use the Langchain function `\n",
      "        <a href=\"https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.similarity_search_with_score\">\n",
      "         similarity_search_with_score\n",
      "        </a>\n",
      "        `. The\n",
      "        <i>\n",
      "         Filtered\n",
      "        </i>\n",
      "        technique will consist as initiating a filter on documents to extract only those containing at least one keyword from the question within their header’s keyword list. Subsequently, the similarity score is calculated solely for the documents that share keywords with the question posed. This approach ensures that the similarity assessment is conducted exclusively within the context of relevant keywords shared between the question and the documents.\n",
      "       </p>\n",
      "       <p id=\"8875\">\n",
      "        To create each vectorial database we use these functions :\n",
      "       </p>\n",
      "       <div id=\"21ee\">\n",
      "        <pre><span class=\"hljs-keyword\">from</span> langchain.vectorstores <span class=\"hljs-keyword\">import</span> Chroma\n",
      "<span class=\"hljs-keyword\">from</span> langchain.document_loaders <span class=\"hljs-keyword\">import</span> DataFrameLoader\n",
      "\n",
      "<span class=\"hljs-comment\"># Create baseline</span>\n",
      "<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_baseline_db</span>(<span class=\"hljs-params\">df, persist_directory, embedder=embedder</span>):\n",
      "    df_loader = DataFrameLoader(df, page_content_column=<span class=\"hljs-string\">'chunk_content'</span>)\n",
      "    df_document = df_loader.load()\n",
      "    db = Chroma.from_documents(df_document, embedder, persist_directory=persist_directory)\n",
      "    <span class=\"hljs-keyword\">return</span> db\n",
      "<span class=\"hljs-comment\"># Create other rags</span>\n",
      "<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_documents_db</span>(<span class=\"hljs-params\">df_chunks, keywords = <span class=\"hljs-literal\">False</span>, questions=<span class=\"hljs-literal\">False</span>, name=<span class=\"hljs-string\">\"\"</span>, embedder = embedder</span>):\n",
      "    df_loader = DataFrameLoader(df_chunks, page_content_column=<span class=\"hljs-string\">'chunk_content'</span>)\n",
      "    df_document = df_loader.load()\n",
      "\n",
      "    <span class=\"hljs-comment\"># Add info to chunk content for augmented baseline</span>\n",
      "    <span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> tqdm(<span class=\"hljs-built_in\">enumerate</span>(df_document), total=<span class=\"hljs-built_in\">len</span>(df_document), desc=<span class=\"hljs-string\">\"Processing documents\"</span>):\n",
      "        doc.page_content = <span class=\"hljs-string\">f\"(article  pubId: <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'pubId'</span>]}</span> \\n \\\n",
      "    title: <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'title'</span>]}</span> \\n \\\n",
      "    date: <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'date'</span>]}</span> \\n \\\n",
      "    anchor: <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'anchor'</span>]}</span> \\n \"</span> + <span class=\"hljs-string\">\"extract of doc: \"</span> + doc.page_content\n",
      "\n",
      "    <span class=\"hljs-keyword\">if</span> keywords:\n",
      "        <span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> tqdm(<span class=\"hljs-built_in\">enumerate</span>(df_document), total=<span class=\"hljs-built_in\">len</span>(df_document), desc=<span class=\"hljs-string\">\"Processing documents\"</span>):\n",
      "            doc.page_content = <span class=\"hljs-string\">f\"keywords:  <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'keywords'</span>]}</span> \\n\"</span> + doc.page_content\n",
      "    <span class=\"hljs-keyword\">else</span>: <span class=\"hljs-keyword\">pass</span>\n",
      "    <span class=\"hljs-keyword\">if</span> questions:\n",
      "        <span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> tqdm(<span class=\"hljs-built_in\">enumerate</span>(df_document), total=<span class=\"hljs-built_in\">len</span>(df_document), desc=<span class=\"hljs-string\">\"Processing documents\"</span>):\n",
      "            doc.page_content = <span class=\"hljs-string\">f\"questions:  <span class=\"hljs-subst\">{doc.metadata[<span class=\"hljs-string\">'potential_questions'</span>]}</span> \\n\"</span> + doc.page_content\n",
      "    <span class=\"hljs-keyword\">else</span>: <span class=\"hljs-keyword\">pass</span>\n",
      "    <span class=\"hljs-comment\"># Embed document one by one to see progression</span>\n",
      "    db = <span class=\"hljs-literal\">None</span>\n",
      "    <span class=\"hljs-keyword\">with</span> tqdm(total=<span class=\"hljs-built_in\">len</span>(df_document), desc=<span class=\"hljs-string\">\"Ingesting documents\"</span>) <span class=\"hljs-keyword\">as</span> pbar:\n",
      "        <span class=\"hljs-keyword\">for</span> d <span class=\"hljs-keyword\">in</span> df_document:\n",
      "            <span class=\"hljs-keyword\">if</span> db:\n",
      "                db.add_documents([d])\n",
      "            <span class=\"hljs-keyword\">else</span>:\n",
      "                db = Chroma.from_documents([d], embedder, persist_directory=name) \n",
      "            pbar.update(<span class=\"hljs-number\">1</span>)\n",
      "    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{name}</span> Done.\"</span>)\n",
      "\n",
      "    <span class=\"hljs-keyword\">return</span> db</pre>\n",
      "       </div>\n",
      "       <p id=\"c0ff\">\n",
      "        The function used to retrieve relevant documents is :\n",
      "       </p>\n",
      "       <div id=\"93ac\">\n",
      "        <pre><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">ensure_list_length</span>(<span class=\"hljs-params\">lst, target_length=<span class=\"hljs-number\">5</span></span>):\n",
      "    <span class=\"hljs-comment\"># Calculate the number of elements to add</span>\n",
      "    elements_to_add = target_length - <span class=\"hljs-built_in\">len</span>(lst)\n",
      "    <span class=\"hljs-comment\"># Extend the list with None for any missing elements, if necessary</span>\n",
      "    <span class=\"hljs-keyword\">if</span> elements_to_add &gt; <span class=\"hljs-number\">0</span>:\n",
      "        lst.extend([<span class=\"hljs-literal\">None</span>] * elements_to_add)\n",
      "    <span class=\"hljs-keyword\">return</span> lst\n",
      "\n",
      "<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_similar_docs</span>(<span class=\"hljs-params\">df_eval, rag, prefix, questions_eval, softkeyword=<span class=\"hljs-literal\">False</span>, remove_stops=<span class=\"hljs-literal\">False</span>, rerank=<span class=\"hljs-literal\">False</span>, model=model, k=<span class=\"hljs-number\">5</span></span>):\n",
      "    <span class=\"hljs-string\">\"\"\"\n",
      "    Retrieve similar documents for a list of evaluation questions.\n",
      "\n",
      "    Args:\n",
      "        df_eval (DataFrame): The DataFrame containing evaluation data (questions and truth chunk id).\n",
      "        rag (RAG): The RAG for document retrieval.\n",
      "        prefix (str): The prefix to use for the column names in the output DataFrame.\n",
      "        questions_eval (list): A list of evaluation questions.\n",
      "        softkeyword (bool, optional): Flag to indicate if soft keyword matching should be used. Defaults to False.\n",
      "        remove_stops (bool, optional): Flag to indicate if stopwords should be removed from the questions. Defaults to False.\n",
      "        rerank (bool, optional): Flag to indicate if documents should be reranked based on a model score. Defaults to False.\n",
      "        model (Model, optional): The reranking model. Defaults to the global model \"model\".\n",
      "        k (int, optional): The maximum number of similar documents to retrieve. Defaults to 5.\n",
      "\n",
      "    Returns:\n",
      "        DataFrame: The DataFrame with added columns containing the IDs of similar documents for each question.\n",
      "\n",
      "    \"\"\"</span>\n",
      "    list_ids = []\n",
      "    <span class=\"hljs-keyword\">for</span> question <span class=\"hljs-keyword\">in</span> questions_eval:\n",
      "        <span class=\"hljs-keyword\">if</span> softkeyword:\n",
      "            where_document={}\n",
      "        <span class=\"hljs-keyword\">else</span>: \n",
      "            <span class=\"hljs-comment\">#Keywords retrivial on the question is based on the whole rag corpus minus stopwords</span>\n",
      "            where_document={<span class=\"hljs-string\">\"$or\"</span>:[{<span class=\"hljs-string\">\"$contains\"</span>:x} <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> tools.extract_keywords_tfidf(tools.remove_french_stopwords(question).lower(), \n",
      "                                                                                      corpus=[tools.remove_french_stopwords(x) <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> rag.get()[<span class=\"hljs-string\">'documents'</span>]], \n",
      "                                                                                      top_n=<span class=\"hljs-number\">5</span>)]} <span class=\"hljs-comment\">#corpus</span>\n",
      "        <span class=\"hljs-keyword\">if</span> remove_stops:\n",
      "            question = tools.remove_french_stopwords(question.lower())\n",
      "        <span class=\"hljs-keyword\">else</span>:\n",
      "            question = question.lower()\n",
      "        docs = rag.similarity_search_with_score(question, where_document=where_document, k=k)\n",
      "        \n",
      "        <span class=\"hljs-keyword\">if</span> rerank: <span class=\"hljs-comment\">#Predict a new score on question/chunk pair, take only raw content as input (after 'extract of doc: ')</span>\n",
      "            scores = model.predict([(question, doc[<span class=\"hljs-number\">0</span>].page_content.split(<span class=\"hljs-string\">'extract of doc: '</span>)[-<span class=\"hljs-number\">1</span>]) <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> docs])\n",
      "            docs = <span class=\"hljs-built_in\">sorted</span>([(doc[<span class=\"hljs-number\">0</span>], score) <span class=\"hljs-keyword\">for</span> doc, score <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(docs, scores)], key=<span class=\"hljs-keyword\">lambda</span> x: x[<span class=\"hljs-number\">1</span>], reverse=<span class=\"hljs-literal\">True</span>)\n",
      "        <span class=\"hljs-keyword\">else</span>:\n",
      "            <span class=\"hljs-keyword\">pass</span>\n",
      "            \n",
      "        ids = []\n",
      "        <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> docs[:<span class=\"hljs-number\">5</span>]: <span class=\"hljs-comment\">#5 docs max</span>\n",
      "            doc_id = doc[<span class=\"hljs-number\">0</span>].metadata[<span class=\"hljs-string\">\"chunk_id\"</span>]\n",
      "            ids.append(doc_id)\n",
      "        <span class=\"hljs-comment\">#Ensure list is 5 docs long</span>\n",
      "        ids = ensure_list_length(ids)\n",
      "        list_ids.append(ids)\n",
      "    df_eval[[<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>_doc1\"</span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>_doc2\"</span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>_doc3\"</span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>_doc4\"</span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>_doc5\"</span>]] = list_ids\n",
      "    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Done : <span class=\"hljs-subst\">{prefix}</span>\"</span>)\n",
      "    <span class=\"hljs-keyword\">return</span> df_eval</pre>\n",
      "       </div>\n",
      "       <p id=\"26b3\">\n",
      "        The\n",
      "        <i>\n",
      "         softkeywords\n",
      "        </i>\n",
      "        will consist on chunks containing keywords in headers without using them for filtering pre-scoring. For the latter, keywords are only impacting the embedding step of the process.\n",
      "       </p>\n",
      "       <p id=\"6455\">\n",
      "        Here are the 7 different RAGs:\n",
      "       </p>\n",
      "       <figure id=\"0e76\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LaWTz7Y-s0ud1L4CkHTwYA.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig3. 7 different types of chunks.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <figure id=\"56ed\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qTmqZVdwed9O0a9RaTDL1A.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Table1. Summary of RAGs tested.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <h1 id=\"c52b\">\n",
      "        Results\n",
      "       </h1>\n",
      "       <p id=\"d035\">\n",
      "        <b>\n",
      "         <i>\n",
      "          E5 Embeddings\n",
      "         </i>\n",
      "        </b>\n",
      "       </p>\n",
      "       <p id=\"b1fe\">\n",
      "        Using the E5 embedding model we created the 7 RAGs we show in the previous section and ran the 100 synthetic questions on the search similarity function of Lanchain. For each of our results, the accuracy is defined by wether or not the « true » chunk was found in the 1st, 2d, 3rd, 4th, 5th position or not at all (6+ position).\n",
      "       </p>\n",
      "       <figure id=\"2255\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wM2o0dTH_xOoqz2Gv_CJDg.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig4. E5 — Synthetic questions evaluation. (See appendix for a detailed summary of results)\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"873c\">\n",
      "        The terms ‘\n",
      "        <i>\n",
      "         raw\n",
      "        </i>\n",
      "        ’ and ‘\n",
      "        <i>\n",
      "         clean\n",
      "        </i>\n",
      "        ’, displayed beneath the graph, represent whether the question was used in its original form or with stop words removed before entering the pipeline. Various tests indicate that removing stop words (‘\n",
      "        <i>\n",
      "         clean\n",
      "        </i>\n",
      "        ’) from the question did not significantly influence the results. Therefore, we have chosen not to dwell extensively on this aspect.\n",
      "       </p>\n",
      "       <figure id=\"f70f\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NyUBhJAwKvq-oyOpxGgwzA.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig5. E5 — Synthetic questions evaluation — Cumulative sum 5 first retrieved documents.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"2a25\">\n",
      "        First, we observe that the baseline (simple chunks without additional information) is outperformed by every other type of chunk. Adding keywords, particularly with potential questions, appears to significantly enhance performance, with keyword filtering outperforming its «\n",
      "        <i>\n",
      "         softkeywords\n",
      "        </i>\n",
      "        » counterpart. However, it’s worth noting that this technique (keyword filtering) is inherently ‘destructive’, as numerous documents are disregarded if they fail to contain any of the question’s keywords.\n",
      "       </p>\n",
      "       <p id=\"b7eb\">\n",
      "        Top results are the rags containing potential questions + keywords in headers with and without pre filtering using the keywords. Their results are respectively 95.1 and 92.1 accuracy score for chunk retrieval in the top 5 documents using similarity score.\n",
      "       </p>\n",
      "       <p id=\"e0b3\">\n",
      "        <b>\n",
      "         <i>\n",
      "          BGE Embeddings\n",
      "         </i>\n",
      "        </b>\n",
      "       </p>\n",
      "       <p id=\"b81c\">\n",
      "        Using the same preset as the previous section, we ran the 100 synthetic questions on the BGE embedded RAGs.\n",
      "       </p>\n",
      "       <figure id=\"3166\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aS7dA1zJgxGK5KVFIewlxw.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig6. BGE — Synthetic questions evaluation.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"9369\">\n",
      "        Results here are pretty self explanatory, as BGE seems to outperform E5 on every type of chunking. We will show a better comparison in a next section. Like E5, baseline seems to be the less effective, validating the fact that adding informations and even keywords and potential questions does help in the retrieval step.\n",
      "       </p>\n",
      "       <p id=\"64ea\">\n",
      "        The cumulative sum of the accuracy for the first 5 documents is shown below.\n",
      "       </p>\n",
      "       <figure id=\"ab07\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*moEP4flOEZIiCmABJStgGA.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig7. BGE — Synthetic questions evaluation — Cumulative sum.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"5b7e\">\n",
      "        Looking at the top 5 of documents retrieved, every RAGs — excluding the baseline — seem to perform about the same. On the top 2, the best RAGs are the augmented baseline (baseline + title etc) and augmented baseline with potential questions.\n",
      "       </p>\n",
      "       <p id=\"e42a\">\n",
      "        Running the same RAGs on the human questions set, we get the following results:\n",
      "       </p>\n",
      "       <figure id=\"61e8\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8l9jJ1oUaWDCH4W4c3dkow.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig8. BGE — Human questions evaluation — Cumulative sum.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"35a8\">\n",
      "        Interestingly enough, the potential questions added doesn’t seem to have the impact it had with E5. However, keywords have the same positive impact, followed closely by soft keywords.\n",
      "       </p>\n",
      "       <p id=\"0217\">\n",
      "        <b>\n",
      "         <i>\n",
      "          Solon Embeddings\n",
      "         </i>\n",
      "        </b>\n",
      "       </p>\n",
      "       <p id=\"3943\">\n",
      "        For Solon, results are very close to BGE so we won’t go into details as the graph is quite self-explanatory.\n",
      "       </p>\n",
      "       <figure id=\"4d23\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KoSssEKmHdhxlFxPX2QQmw.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig9. Solon — Synthetic questions evaluation.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"54ec\">\n",
      "        Overall, BGE is better than both of the other embedding, for almost all types of RAGs.\n",
      "       </p>\n",
      "       <p id=\"43df\">\n",
      "        <b>\n",
      "         <i>\n",
      "          E5 Vs BGE — With and without Reranker\n",
      "         </i>\n",
      "        </b>\n",
      "       </p>\n",
      "       <p id=\"fafb\">\n",
      "        To compare E5 and BGE, we choose to keep the baselines and the questions + keywords RAGs. More, we also apply a soft keywords approach for those, so no filtering is done before similarity search. For the reranker, we choose to get 20 documents from the similarity search, and keep the 5 best score from the reranker out of them.\n",
      "       </p>\n",
      "       <figure id=\"f1de\">\n",
      "        <img src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*y6GpcLijBeEoIO5o574OVA.png\"/>\n",
      "        <figcaption>\n",
      "         <i>\n",
      "          Fig10. E5 vs BGE — With and without Reranker.\n",
      "         </i>\n",
      "        </figcaption>\n",
      "       </figure>\n",
      "       <p id=\"8569\">\n",
      "        BGE baseline outperformed every E5 RAGs on the first document retrieval and is holding very well on the whole top 5. Furthermore, the addition of a reranker proved particularly beneficial, notably enhancing performance for BGE models with question and keywords added in the chunks headers.\n",
      "       </p>\n",
      "       <h1 id=\"7288\">\n",
      "        Conclusion\n",
      "       </h1>\n",
      "       <p id=\"070c\">\n",
      "        In this research, we embarked on a comprehensive exploration of leveraging Retrieval-Augmented Generation (RAG) and chunking techniques to enhance language model performance. The widespread utilization of Large Language Models (LLMs) in various applications underscores the importance of optimizing their capabilities for efficient data processing. Our study focused on three key aspects: embedding models, chunk augmentation techniques, and reranking methodologies.\n",
      "       </p>\n",
      "       <p id=\"c939\">\n",
      "        For embedding models, we conducted a comparative analysis of three prominent models: e5, BGE, and Solon. Each model exhibited unique characteristics and strengths, with BGE demonstrating superior performance across different chunking configurations. The evaluation of chunking techniques revealed that incorporating contextual information, such as keywords and potential questions, significantly improved document retrieval accuracy. However, it’s essential to acknowledge the inherent trade-off associated with keyword filtering, which can lead to the exclusion of relevant documents.\n",
      "       </p>\n",
      "       <p id=\"67fe\">\n",
      "        Furthermore, our investigation into reranking mechanisms highlighted their efficacy in refining search results based on relevance to the query. The application of reranking, particularly in conjunction with BGE embeddings and augmented chunking, yielded notable improvements in document retrieval accuracy.\n",
      "       </p>\n",
      "       <p id=\"bce3\">\n",
      "        Overall, our findings underscore the importance of considering various factors, including embedding models, chunking strategies, and reranking techniques, in optimizing LLM performance for data-driven tasks. By shedding light on the nuances of these approaches, our research provides useful insights into how they can help improve information retrieval.\n",
      "       </p>\n",
      "       <h2 id=\"6a12\">\n",
      "        Future researches\n",
      "       </h2>\n",
      "       <p id=\"76d0\">\n",
      "        In future research, we could delve into additional variations of chunking techniques to further refine our understanding of their effectiveness. For instance, exploring the potential of one-sentence chunking and embedding could provide insights into the granularity of information necessary for optimal document retrieval. Additionally, investigating the selective chunking of “potential questions”, either with or without accompanying keywords, offers an opportunity to tailor contextual information more precisely to the language model’s needs. By systematically evaluating these alternative approaches, we can gain a deeper understanding of their implications for enhancing LLM performance and information retrieval accuracy.\n",
      "       </p>\n",
      "       <p id=\"564d\">\n",
      "        Ressources: Code and Notebook available on\n",
      "        <a href=\"https://gitlab.com/etalab-datalab/llm/challenge-datascience/-/tree/main/chunking_augmentation\">\n",
      "         GitLab\n",
      "        </a>\n",
      "       </p>\n",
      "       <p id=\"84f3\">\n",
      "        <a href=\"https://www.etalab.gouv.fr\">\n",
      "         <i>\n",
      "          Camille André — Data Scientist @ Etalab — Direction Interministérielle du Numérique\n",
      "         </i>\n",
      "        </a>\n",
      "       </p>\n",
      "      </article>\n",
      "     </body>\n",
      "    </div>\n",
      "    <div class=\"text-sm font-bold mt-12 space-y-[4px]\">\n",
      "     <div class=\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\">\n",
      "      Llm\n",
      "     </div>\n",
      "     <div class=\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\">\n",
      "      Rags\n",
      "     </div>\n",
      "     <div class=\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\">\n",
      "      NLP\n",
      "     </div>\n",
      "     <div class=\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\">\n",
      "      Chunk\n",
      "     </div>\n",
      "     <div class=\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\">\n",
      "      Embedding\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "   <span class=\"flex justify-center items-center w-full py-12 text-2xl divide-y border-t border-gray-300 dark:border-gray-700\">\n",
      "    Recommended from ReadMedium\n",
      "   </span>\n",
      "   <div class=\"max-w-[680px]\">\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*KqpicOFO7jh7FXGjoJ2Bcg.jpeg\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        Dominik Polzer\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       A collection of RAG techniques to help you develop your RAG app into something robust that will last\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       24 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*MKcWgwWFa6EL01_JmxPAPQ.jpeg\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        zhaozhiming\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       Advanced RAG Retrieval Strategies: Self-RAG\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       Introduction to Self-RAG’s Implementation and Practical Application\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       13 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*LDjQS3c-G1gsojOf24ijGg@2x.jpeg\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        Vipra Singh\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       Building LLM Applications: Advanced RAG (Part 10)\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       48 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*cwYWYCjbeXNc_pAtTeq_Zg.jpeg\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        Alexander Nguyen\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       The resume that got a software engineer a $300,000 job at Google.\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       1-page. Well-formatted.\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       4 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/1*TIj_sKKvGVPMjdPRCZvPMg.jpeg\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        Nishan Jain\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       Chunking PDFs and Multimodal Documents: Efficient Methods for Handling Text, Tables, and Images for…\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       In this article, you will learn how to chunk documents like PDF, Word, and other multimodal documents for RAG applications.\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       4 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"w-full py-8\">\n",
      "     <div class=\"flex justify-start items-center mb-4\">\n",
      "      <img alt=\"avatar\" class=\"w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]\" src=\"https://miro.readmedium.com/v2/resize:fill:88:88/0*-DLO733P-PebUsO_\">\n",
      "       <span class=\"ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white\">\n",
      "        Kamal Dhungana\n",
      "       </span>\n",
      "      </img>\n",
      "     </div>\n",
      "     <a href=\"\">\n",
      "      <span class=\"text-[20px] font-bold leading-[24px]\">\n",
      "       Advanced RAG: Multi-Query Retriever Approach\n",
      "      </span>\n",
      "      <p class=\"text-[16px] font-[400] my-1\">\n",
      "       A Simple Retrieval-Augmented Generation (RAG) generates final results through a two-step process. First, the query is transformed into an…\n",
      "      </p>\n",
      "     </a>\n",
      "     <div class=\"mt-6\">\n",
      "      <span>\n",
      "       6 min read\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      "  <script async=\"\" src=\"/_next/static/chunks/webpack-7c0906a643bf5d80.js\">\n",
      "  </script>\n",
      "  <script>\n",
      "   (self.__next_f=self.__next_f||[]).push([0])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"0:\\\"$L1\\\"\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"2:HL[\\\"/_next/static/css/7e012bdb4f33f65e.css\\\",\\\"style\\\"]\\n3:HL[\\\"/_next/static/css/8819903482fa0ce9.css\\\",\\\"style\\\"]\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"4:I{\\\"id\\\":6054,\\\"chunks\\\":[\\\"272:static/chunks/webpack-7c0906a643bf5d80.js\\\",\\\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\\\",\\\"864:static/chunks/864-fc79a795ba0d747c.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n6:I{\\\"id\\\":1729,\\\"chunks\\\":[\\\"272:static/chunks/webpack-7c0906a643bf5d80.js\\\",\\\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\\\",\\\"864:static/chunks/864-fc79a795ba0d747c.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"1:[[],[\\\"$\\\",\\\"$L4\\\",null,{\\\"buildId\\\":\\\"_qr9aABrMi534liMUsg8I\\\",\\\"assetPrefix\\\":\\\"\\\",\\\"initialCanonicalUrl\\\":\\\"/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\",\\\"initialTree\\\":[\\\"\\\",{\\\"children\\\":[\\\"(pages)\\\",{\\\"children\\\":[[\\\"slug\\\",\\\"enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\",\\\"c\\\"],{\\\"children\\\":[\\\"__PAGE__\\\",{}]}]}]},\\\"$undefined\\\",\\\"$undefined\\\",true],\\\"initialHead\\\":[false,\\\"$L5\\\"],\\\"globalErrorComponent\\\":\\\"$6\\\",\\\"children\\\":[null,\\\"$L7\\\",null]}]]\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"8:I{\\\"id\\\":1443,\\\"chunks\\\":[\\\"272:static/chunks/webpack-7c0906a643bf5d80.js\\\",\\\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\\\",\\\"864:static/chunks/864-fc79a795ba0d747c.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n9:I{\\\"id\\\":8639,\\\"chunks\\\":[\\\"272:static/chunks/webpack-7c0906a643bf5d80.js\\\",\\\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\\\",\\\"864:static/chunks/864-fc79a795ba0d747c.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"7:[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\"],\\\"loading\\\":\\\"$undefined\\\",\\\"loadingStyles\\\":\\\"$undefined\\\",\\\"hasLoading\\\":false,\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"notFound\\\":[[\\\"$\\\",\\\"title\\\",null,{\\\"children\\\":\\\"404: This page could not be found.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontFamily\\\":\\\"system-ui,\\\\\\\"Segoe UI\\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\\"Apple Color Emoji\\\\\\\",\\\\\\\"Segoe UI Emoji\\\\\\\"\\\",\\\"height\\\":\\\"100vh\\\",\\\"textAlign\\\":\\\"center\\\",\\\"display\\\":\\\"flex\\\",\\\"flexDirection\\\":\\\"column\\\",\\\"alignItems\\\":\\\"center\\\",\\\"justifyContent\\\":\\\"center\\\"},\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"style\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\\"}}],[\\\"$\\\",\\\"h1\\\",null,{\\\"className\\\":\\\"next-error-h1\\\",\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\",\\\"margin\\\":\\\"0 20px 0 0\\\",\\\"padding\\\":\\\"0 23px 0 0\\\",\\\"fontSize\\\":24,\\\"fontWeight\\\":500,\\\"verticalAlign\\\":\\\"top\\\",\\\"lineHeight\\\":\\\"49px\\\"},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\"},\\\"children\\\":[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"fontSize\\\":14,\\\"fontWeight\\\":400,\\\"lineHeight\\\":\\\"49px\\\",\\\"margin\\\":0},\\\"children\\\":\\\"This page could not be found.\\\"}]}]]}]}]],\\\"notFoundStyles\\\":[],\\\"childProp\\\":{\\\"current\\\":[null,\\\"$La\\\",null],\\\"segment\\\":\\\"(pages)\\\"},\\\"styles\\\":[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/7e012bdb4f33f65e.css\\\",\\\"precedence\\\":\\\"next\\\"}]]}]\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"b:I{\\\"id\\\":4244,\\\"chunks\\\":[\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"845:static/chunks/845-a25742f4af4e91ff.js\\\",\\\"962:static/chunks/app/(pages)/layout-9960097673d395e9.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\nc:I{\\\"id\\\":3050,\\\"chunks\\\":[\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"845:static/chunks/845-a25742f4af4e91ff.js\\\",\\\"962:static/chunks/app/(pages)/layout-9960097673d395e9.js\\\"],\\\"name\\\":\\\"Providers\\\",\\\"async\\\":false}\\nd:I{\\\"id\\\":4780,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"Button\\\",\\\"async\\\":false}\\n10:I{\\\"id\\\":9891,\\\"chunks\\\":[\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"845:static/chunks/845-a25742f4af4e91ff.js\\\",\\\"962:static/chunks/app/(pages)\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"/layout-9960097673d395e9.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"a:[\\\"$\\\",\\\"html\\\",null,{\\\"lang\\\":\\\"en\\\",\\\"suppressHydrationWarning\\\":true,\\\"children\\\":[[\\\"$\\\",\\\"head\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"$Lb\\\",null,{\\\"id\\\":\\\"theme-script\\\",\\\"strategy\\\":\\\"beforeInteractive\\\",\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"\\\\n  (function () {\\\\n    function getImplicitPreference() {\\\\n      var mediaQuery = '(prefers-color-scheme: dark)'\\\\n      var mql = window.matchMedia(mediaQuery)\\\\n      var hasImplicitPreference = typeof mql.matches === 'boolean'\\\\n\\\\n      if (hasImplicitPreference) {\\\\n        return mql.matches ? 'dark' : 'light'\\\\n      }\\\\n\\\\n      return null\\\\n    }\\\\n\\\\n    function themeIsValid(theme) {\\\\n      return theme === 'light' || theme === 'dark'\\\\n    }\\\\n\\\\n    var themeToSet = 'light'\\\\n    var preference = window.localStorage.getItem('payload-theme')\\\\n\\\\n    if (themeIsValid(preference)) {\\\\n      themeToSet = preference\\\\n    } else {\\\\n      var implicitPreference = getImplicitPreference()\\\\n\\\\n      if (implicitPreference) {\\\\n        themeToSet = implicitPreference\\\\n      }\\\\n    }\\\\n\\\\n    // payload 主题控制\\\\n    document.documentElement.setAttribute('data-theme', themeToSet)\\\\n    // tailwind主题控制\\\\n    if(themeToSet === 'dark'){\\\\n      document.documentElement.classList.add('dark')\\\\n    }\\\\n  })();\\\\n  \\\"}}],[\\\"$\\\",\\\"link\\\",null,{\\\"rel\\\":\\\"icon\\\",\\\"href\\\":\\\"/favicon.ico\\\",\\\"sizes\\\":\\\"32x32\\\"}],[\\\"$\\\",\\\"link\\\",null,{\\\"rel\\\":\\\"icon\\\",\\\"href\\\":\\\"/favicon.svg\\\",\\\"type\\\":\\\"image/svg+xml\\\"}]]}],[\\\"$\\\",\\\"body\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$Lc\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(pages)\\\",\\\"children\\\"],\\\"loading\\\":\\\"$undefined\\\",\\\"loadingStyles\\\":\\\"$undefined\\\",\\\"hasLoading\\\":false,\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"notFound\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx\\\",\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"VerticalPadding_bottom-large__W6wnm\\\",\\\"children\\\":[[\\\"$\\\",\\\"h1\\\",null,{\\\"style\\\":{\\\"marginBottom\\\":0},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"p\\\",null,{\\\"children\\\":\\\"This page could not be found.\\\"}],[\\\"$\\\",\\\"$Ld\\\",null,{\\\"href\\\":\\\"/\\\",\\\"label\\\":\\\"Go Home\\\",\\\"appearance\\\":\\\"primary\\\"}]]}]}],\\\"notFoundStyles\\\":[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/8991c87d0e85bb86.css\\\",\\\"precedence\\\":\\\"next\\\"}]],\\\"childProp\\\":{\\\"current\\\":[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"(pages)\\\",\\\"children\\\",[\\\"slug\\\",\\\"enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\",\\\"c\\\"],\\\"children\\\"],\\\"loading\\\":\\\"$undefined\\\",\\\"loadingStyles\\\":\\\"$undefined\\\",\\\"hasLoading\\\":false,\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"notFoundStyles\\\":\\\"$undefined\\\",\\\"childProp\\\":{\\\"current\\\":[\\\"$Le\\\",\\\"$Lf\\\",null],\\\"segment\\\":\\\"__PAGE__\\\"},\\\"styles\\\":[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/8819903482fa0ce9.css\\\",\\\"precedence\\\":\\\"next\\\"}]]}],\\\"segment\\\":[\\\"slug\\\",\\\"enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\",\\\"c\\\"]},\\\"styles\\\":[]}]}]}],[\\\"$\\\",\\\"$L10\\\",null,{}]]}]\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"5:[[\\\"$\\\",\\\"meta\\\",\\\"0\\\",{\\\"charSet\\\":\\\"utf-8\\\"}],[\\\"$\\\",\\\"title\\\",\\\"1\\\",{\\\"children\\\":\\\"Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"2\\\",{\\\"name\\\":\\\"description\\\",\\\"content\\\":\\\"How to add context to chunks ? Are rerankers a good choice ?\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"3\\\",{\\\"name\\\":\\\"viewport\\\",\\\"content\\\":\\\"width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover\\\"}],[\\\"$\\\",\\\"link\\\",\\\"4\\\",{\\\"rel\\\":\\\"canonical\\\",\\\"href\\\":\\\"https://readmedium.com/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"5\\\",{\\\"property\\\":\\\"og:title\\\",\\\"content\\\":\\\"Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"6\\\",{\\\"property\\\":\\\"og:description\\\",\\\"content\\\":\\\"How to add context to chunks ? Are rerankers a good choice ?\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"7\\\",{\\\"property\\\":\\\"og:url\\\",\\\"content\\\":\\\"https://readmedium.com/enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"8\\\",{\\\"name\\\":\\\"twitter:card\\\",\\\"content\\\":\\\"summary_large_image\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"9\\\",{\\\"name\\\":\\\"twitter:creator\\\",\\\"content\\\":\\\"@readmedium\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"10\\\",{\\\"name\\\":\\\"twitter:title\\\",\\\"content\\\":\\\"Enhancing Language Model Performance: Insights into RAG and Chunking Augmentation Techniques\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"11\\\",{\\\"name\\\":\\\"twitter:description\\\",\\\"content\\\":\\\"How to add context to chunks ? Are rerankers a good choice ?\\\"}]]\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"e:null\\n\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"11:I{\\\"id\\\":4724,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n12:I{\\\"id\\\":6964,\\\"chunks\\\":[\\\"411:static/ch\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"unks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"Image\\\",\\\"async\\\":false}\\n13:I{\\\"id\\\":210,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n14:I{\\\"id\\\":2681,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\".js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"ThemeSelector\\\",\\\"async\\\":false}\\n15:I{\\\"id\\\":9044,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/84\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"8-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"LanguageSelector\\\",\\\"async\\\":false}\\n16:I{\\\"id\\\":5362,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n17:I{\\\"id\\\":8577,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\".js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n1b:I{\\\"id\\\":2911,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"78918.js\\\",\\\"62:static/chunks/62-bd0c3d6303ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n1c:I{\\\"id\\\":5077,\\\"chunks\\\":[\\\"411:static/chunks/8318a8be-b8e7397c242123f5.js\\\",\\\"724:static/chunks/724-1461eaeda7f61205.js\\\",\\\"848:static/chunks/848-bede78ffaf63de2d.js\\\",\\\"475:static/chunks/475-36c9ef78992d7529.js\\\",\\\"490:static/chunks/490-f12e4d1d7bb78918.js\\\",\\\"62:static/chunks/62-bd0c3d63\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"03ae814e.js\\\",\\\"317:static/chunks/317-b2419f48f3019a69.js\\\",\\\"278:static/chunks/278-b9ce59cd632661b2.js\\\",\\\"284:static/chunks/app/(pages)/[...slug]/page-aa721e81249786bd.js\\\"],\\\"name\\\":\\\"\\\",\\\"async\\\":false}\\n18:T8e72,\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"\\u003cbody\\u003e\\u003carticle\\u003e\\u003ch1 id=\\\"c905\\\"\\u003eEnhancing Language Model Performance: Insights into Retrieval-Augmented Generation and Chunking Techniques\\u003c/h1\\u003e\\u003cfigure id=\\\"8c7a\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4EjerlC79zcw-_zuyKioEw.jpeg\\\"\\u003e\\u003cfigcaption\\u003ePhoto de \\u003ca href=\\\"https://unsplash.com/fr/@impatrickt?utm_content=creditCopyText\\u0026amp;utm_medium=referral\\u0026amp;utm_source=unsplash\\\"\\u003ePatrick Tomasso\\u003c/a\\u003e sur \\u003ca href=\\\"https://unsplash.com/fr/photos/lot-a-livre-ouvert-Oaqk7qqNh_c?utm_content=creditCopyText\\u0026amp;utm_medium=referral\\u0026amp;utm_source=unsplash\\\"\\u003eUnsplash\\u003c/a\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003ch1 id=\\\"a693\\\"\\u003eIntroduction\\u003c/h1\\u003e\\u003cp id=\\\"e659\\\"\\u003eLarge Language Models (LLMs) are now everywhere, offering powerful tools that can be seamlessly integrated with your data thanks to the magic of Retrieval-Augmented Generation (RAG). In this article, we’re diving into some key areas to see how retrieval performance can be boosted with:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eEmbedding models: E5, BGE and Solon\\u003c/li\\u003e\\u003cli\\u003eChunking augmentation techniques: How to add ‘context’ with minimal efforts to our chunks\\u003c/li\\u003e\\u003cli\\u003eReranking: Are rerankers a good addition to a RAG pipeline ?\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"8853\\\"\\u003eBy putting these components to the test, we hope to uncover some valuable insights into how to make the most of language models in real-world applications.\\u003c/p\\u003e\\u003ch1 id=\\\"f253\\\"\\u003eChunk Augmentation: Adding Context for Enhanced Document Retrieval\\u003c/h1\\u003e\\u003cp id=\\\"7e3a\\\"\\u003e\\u003ci\\u003eThe data we’ll be using for this article comes from \\u003ca href=\\\"https://www.service-public.fr\\\"\\u003eservice-public.fr\\u003c/a\\u003e and mainly contains guides and answers to French administrative questions. All the data used can be found \\u003ca href=\\\"https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/i\\u003e\\u003c/p\\u003e\\u003cp id=\\\"4e11\\\"\\u003eFor the preprocessing, we only have to read our json, declare and format the columns we are going to use for the chunking:\\u003c/p\\u003e\\u003cdiv id=\\\"1d43\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e pandas \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pd\\n\\ndf =pd.read_json(\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"data/fiches-travail.json\\\"\\u003c/span\\u003e)\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eprep_specific\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf\\u003c/span\\u003e):\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections\\\"\\u003c/span\\u003e].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e).apply(literal_eval)\\n    df = df.explode(\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e).reset_index(drop=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"text_section\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e].apply(\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x : x[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"text\\\"\\u003c/span\\u003e])\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e].apply(\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x : x[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e])\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e df\\n\\ndf = prep_specific(df)\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Column of interest\\u003c/span\\u003e\\ntext_col = \\u003cspan class=\\\"hljs-string\\\"\\u003e'text_section'\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Metadata columns\\u003c/span\\u003e\\nmetadata_cols = [\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"pubId\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"date\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e]\\n\\ndf[text_col] = df[text_col].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e).\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e.lower()\\n\\ndf[metadata_cols] = df[metadata_cols].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e) \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Ensuring metadata columns are in string format aswell\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Fast cleaning\\u003c/span\\u003e\\ndf = df[~df[\\u003cspan class=\\\"hljs-string\\\"\\u003e'pubId'\\u003c/span\\u003e].isna()]\\ndf = df[~df[\\u003cspan class=\\\"hljs-string\\\"\\u003e'date'\\u003c/span\\u003e].isna()].reset_index(drop=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\ndf[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e] = df[text_col].apply(tools.remove_french_stopwords)\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Corpus for keywords retrieval\\u003c/span\\u003e\\ncorpus = df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"_clean\\\"\\u003c/span\\u003e]\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Keywords retrieval (code on git)\\u003c/span\\u003e\\ndf[\\u003cspan class=\\\"hljs-string\\\"\\u003e'keywords'\\u003c/span\\u003e] = tools.extract_keywords_tfidf(df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e], corpus=df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e], top_n=\\u003cspan class=\\\"hljs-number\\\"\\u003e10\\u003c/span\\u003e)\\n\\ndf.to_csv(\\u003cspan class=\\\"hljs-string\\\"\\u003e'data.csv'\\u003c/span\\u003e)\\n\\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"df shape: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{df.shape}\\u003c/span\\u003e\\\"\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"0718\\\"\\u003eChunking is a natural language processing technique that involves breaking down a text into smaller, meaningful units or “chunks”. These chunks typically consist of words or phrases that convey a specific idea or concept within the context of the text. Chunking helps organize and structure information, making it easier for models to analyze and understand the content.\\u003c/p\\u003e\\u003cfigure id=\\\"ad04\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NMCaeZ07OH3mNdAF0fVhwA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig1. Visualisation of chunking a text (\\u003ca href=\\\"https://huggingface.co/spaces/m-ric/chunk_visualizer\\\"\\u003ehttps://huggingface.co/spaces/m-ric/chunk_visualizer\\u003c/a\\u003e)\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"9eef\\\"\\u003eFor the preliminary chunking, we are using the Langchain function `RecursiveCharacterTextSplitter` with a maximum character length of 4000, which seems a reasonable size for a future LLM (at the end of the RAG pipeline) with a context length of 8k tokens, to which we’ll be able to give 4–5 chunks as context per request.\\u003c/p\\u003e\\u003cp id=\\\"3d83\\\"\\u003eTo create each RAG, we will start with a common base, which is a dataframe structured as follows:\\u003c/p\\u003e\\u003cdiv id=\\\"ad8a\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e pandas \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pd\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e tqdm \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e tqdm\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.document_loaders \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e DataFrameLoader\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain_text_splitters \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e RecursiveCharacterTextSplitter\\n\\ndf = pd.read_csv(\\u003cspan class=\\\"hljs-string\\\"\\u003e'data.csv'\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create every rags from the same base\\u003c/span\\u003e\\ntext_col = \\u003cspan class=\\\"hljs-string\\\"\\u003e'text_section'\\u003c/span\\u003e\\nmetadata_cols = [\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"pubId\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"date\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"keywords\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"url\\\"\\u003c/span\\u003e]\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Make a document list with every columns we could need in the metadatas\\u003c/span\\u003e\\ndf_loader = DataFrameLoader(df[[text_col]+metadata_cols], page_content_column=text_col)\\ndf_document = df_loader.load()\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e#Slipt document in chunks\\u003c/span\\u003e\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=\\u003cspan class=\\\"hljs-number\\\"\\u003e4000\\u003c/span\\u003e,\\n    chunk_overlap=\\u003cspan class=\\\"hljs-number\\\"\\u003e50\\u003c/span\\u003e,\\n    length_function=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e,\\n    separators=[\\u003cspan class=\\\"hljs-string\\\"\\u003e'\\\\n\\\\n'\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e'. '\\u003c/span\\u003e] \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Dont cut sentences\\u003c/span\\u003e\\n)\\nbaseline_docs = text_splitter.split_documents(df_document)\\n\\nchunk_ids = []\\nchunk_contents = []\\nchunk_metadatas = []\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Add a chunk_id and potential questions to metadatas\\u003c/span\\u003e\\nllm = tools.load_llm()\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(baseline_docs), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(baseline_docs), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n    doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_id'\\u003c/span\\u003e]=i\\n    doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'potential_questions'\\u003c/span\\u003e] = tools.run_question_rag(llm, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\u003c/span\\u003e, doc.page_content, prompt=tools.prompt_create_questions)\\n    chunk_ids.append(i)\\n    chunk_contents.append(doc.page_content)\\n    chunk_metadatas.append(doc.metadata)\\n\\ndf_chunks = pd.DataFrame()\\ndf_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_content\\\"\\u003c/span\\u003e] = chunk_contents\\ndf_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_metadata\\\"\\u003c/span\\u003e] = chunk_metadatas\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Recreate the columns from metadatas\\u003c/span\\u003e\\nmetadata_df = df_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_metadata'\\u003c/span\\u003e].apply(pd.Series)\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# This dataframe will be the base for every rags\\u003c/span\\u003e\\ndf_chunks = pd.concat([df_chunks.drop(columns=[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_metadata'\\u003c/span\\u003e]), metadata_df], axis=\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"b7e8\\\"\\u003eTo evaluate the performance of different types of chunks, we will try the following:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eBaseline: The baseline will consist of simple chunks without any added headers or informations\\u003c/li\\u003e\\u003cli\\u003eAugmented baseline: Every other RAGs will consist of the baseline to which will be added some informations we have on each article of our database. This way, we will add as a header to each chunk data such as \\u003ci\\u003eparent document title\\u003c/i\\u003e, \\u003ci\\u003eparent document date\\u003c/i\\u003e, \\u003ci\\u003eparent document url\\u003c/i\\u003e.\\u003c/li\\u003e\\u003cli\\u003eKeywords: The RAGs we will evaluate that will have keywords in their names will consist of the augmented baseline + Top 10 keywords (found using TF-IDF) of the parent document (again, added in headers).\\u003c/li\\u003e\\u003cli\\u003ePotential questions: Questions a chunk should be able to answer.\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"627e\\\"\\u003eThe main goal of a RAG is to find relevant documents that should help a model to answer a given question. Given this, we will try a third approach consisting in adding « potential questions » — questions that a given chunk can answer — to each chunk, as always, in header.\\u003c/p\\u003e\\u003cp id=\\\"e0da\\\"\\u003eTo generate these questions, we gave each chunk we created before to a LLM (mistral 7B — OpenHermes), with the following prompt:\\u003c/p\\u003e\\u003cblockquote id=\\\"d4f3\\\"\\u003e\\u003cp\\u003e« En t’inspirant du texte ci-dessous, réponds uniquement avec une liste de 2–3 questions très précises et détaillées que l’on pourrait poser sur ce texte et dont la réponse se trouve dans ce texte. Si le texte est très court ne réponds qu’avec une ou deux questions. N’inventes pas des questions qui ne veulent rien dire, si tu n’as qu’une seule question c’est très bien aussi. Les questions ne doivent pas être vagues, et pouvoir être associées très facilement à ce texte si on les comparait à d’autres questions pour d’autres textes. Si une question porte sur l’article de manière générale, le numéro ou l’id de l’article doit être contenu dans la question. Les questions doivent contenir les éléments importants comme des dates ou des numéros si besoin. Voici des exemples pour t’aider:\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"6e7b\\\"\\u003e\\u003cp\\u003eMauvaises questions qu’on ne veut pas: [“De quoi parle ce texte ?”, “Qui est mentionné ici ?”] (trop vague)\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"2991\\\"\\u003e\\u003cp\\u003eBonne question: [“Quel groupe est concerné par la décision XX-XXX-XX ?”] (net et précis)\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"91b3\\\"\\u003e\\u003cp\\u003eVoilà le texte: {article} » Prompt 1 — Create potential questions\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cp id=\\\"45dd\\\"\\u003eWe are now ready to test our baseline and our augmented baseline to which we can add — or not — keywords and/or questions making a total of 5 different types of chunks:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eBaseline\\u003c/li\\u003e\\u003cli\\u003eWithout keywords — without questions (Augmented baseline)\\u003c/li\\u003e\\u003cli\\u003eWith keywords — without questions\\u003c/li\\u003e\\u003cli\\u003eWithout keywords — with questions\\u003c/li\\u003e\\u003cli\\u003eWith keywords — with questions\\u003c/li\\u003e\\u003c/ul\\u003e\\u003ch1 id=\\\"d873\\\"\\u003eEmbedding Models : Text representation\\u003c/h1\\u003e\\u003cp id=\\\"8f29\\\"\\u003eText embedding is the process of representing text data, such as words, sentences, or documents, as dense numerical vectors in a high-dimensional vector space. These vector representations aim to capture the semantic and contextual meanings of the text, where similar texts are mapped to vectors that are close together in the vector space.\\u003c/p\\u003e\\u003cfigure id=\\\"c3ab\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5-GVldd6K4sHULqMhG78bQ.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig2. Embeddings: from text to vectors\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"67d2\\\"\\u003eIn this paper, we will compare 3 embedding models against each other which are the following:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/intfloat/multilingual-e5-large\\\"\\u003eintfloat/e5-large\\u003c/a\\u003e: \\u003ca href=\\\"https://arxiv.org/pdf/2402.05672\\\"\\u003ehttps://arxiv.org/pdf/2402.05672\\u003c/a\\u003e\\u003c/li\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/BAAI/bge-m3\\\"\\u003eBAAI/bge-m3\\u003c/a\\u003e: \\u003ca href=\\\"https://arxiv.org/pdf/2402.03216\\\"\\u003ehttps://arxiv.org/pdf/2402.03216\\u003c/a\\u003e\\u003c/li\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1\\\"\\u003eOrdalieTech/Solon-embeddings-large-0.1\\u003c/a\\u003e: SOTA Open source french embedding model.\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"308b\\\"\\u003eThe e5-large model is a text embedding model developed by Microsoft that uses weakly-supervised contrastive pre-training to generate high-quality embeddings. The model is trained on a mixture of datasets and can be used for various natural language processing tasks such as semantic similarity, information retrieval, and text clustering by encoding input text into dense vector representations.\\u003c/p\\u003e\\u003cp id=\\\"41a2\\\"\\u003eThe bge-m3 model is a multilingual text embedding model developed by the Beijing Academy of Artificial Intelligence (BAAI). It supports over 100 languages and can handle long input sequences up to 8192 tokens. This makes it suitable for embedding multi-lingual and long-form text data.\\u003c/p\\u003e\\u003cp id=\\\"ec61\\\"\\u003eFinally, the Solon model is a State-Of-The-Art Open source french embedding model. Data used in the paper being in French, we had to test it against robust multilingual models.\\u003c/p\\u003e\\u003ch1 id=\\\"9170\\\"\\u003eRerankers\\u003c/h1\\u003e\\u003cp id=\\\"3ff7\\\"\\u003eDuring our tests, we’ll also explore reranking, a method employing a reranking model, often referred to as a cross-encoder. This model assesses the similarity between each query-document pair, enabling us to rearrange the documents according to their relevance to the query. After the initial search using similarity scores, the reranker is then applied, allowing for a broader selection of documents. For instance, we might retrieve the 20 most similar documents, rerank them, and select only the top 5 to provide as context for our LLM.\\u003c/p\\u003e\\u003cp id=\\\"02bd\\\"\\u003eThe reranker model used in this article is \\u003ca href=\\\"https://huggingface.co/antoinelouis/crossencoder-electra-base-mmarcoFR\\\"\\u003ecrossencoder-electra-base-french-mmarcoFR\\u003c/a\\u003e, a model trained on French training samples from the \\u003ca href=\\\"https://huggingface.co/datasets/unicamp-dl/mmarco\\\"\\u003emMARCO\\u003c/a\\u003e dataset.\\u003c/p\\u003e\\u003cdiv id=\\\"328a\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Load reranker\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e sentence_transformers \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e CrossEncoder\\nmodel = CrossEncoder(\\u003cspan class=\\\"hljs-string\\\"\\u003e'antoinelouis/crossencoder-electra-base-french-mmarcoFR'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003ch1 id=\\\"acf1\\\"\\u003eEvaluation\\u003c/h1\\u003e\\u003cp id=\\\"fbbf\\\"\\u003eTo evaluate the performance of each chunking techniques and embeddings models, we will use two different sets of questions:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eSynthetic: Out of the 1733 chunks we have in our database, 100 were given to a LLM which output a specific question that should be answerable with the chunk content\\u003c/li\\u003e\\u003cli\\u003eHuman: 20 chunks of the database were chosen randomly by us from which we created more complex questions that should be answerable with the chunk content\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"8d68\\\"\\u003eWe adopted a similar prompt to \\u003ci\\u003ePrompt 1\\u003c/i\\u003e for generating synthetic questions. Interestingly, one might notice similarities or even nearly identical questions between the set of synthetic questions and the potential questions added in the header for RAGs containing potential questions. While this observation holds true, it’s important to note that in real-life scenarios, some questions will inevitably overlap with those pre-generated by the potential questions in the header. This overlap serves as one of their primary objectives.\\u003c/p\\u003e\\u003cp id=\\\"a05c\\\"\\u003eThe accuracy score in graphs represents the % of True (relevant) documents found in top 1, top 2 and so on.\\u003c/p\\u003e\\u003ch1 id=\\\"d28b\\\"\\u003eSetting up vector databases\\u003c/h1\\u003e\\u003cp id=\\\"ad9a\\\"\\u003eIntroduced in part « Augmented Chunking », we will test 5 different RAGs, one for each chunk configuration. Added to that, we will also test 2 techniques using the keywords for the 2 RAGs containing keywords lists in header of chunks:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eFiltered\\u003c/li\\u003e\\u003cli\\u003eSoftkeywords\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"e3d7\\\"\\u003eFor the retrieval, we will use the Langchain function `\\u003ca href=\\\"https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.similarity_search_with_score\\\"\\u003esimilarity_search_with_score\\u003c/a\\u003e`. The \\u003ci\\u003eFiltered \\u003c/i\\u003etechnique will consist as initiating a filter on documents to extract only those containing at least one keyword from the question within their header’s keyword list. Subsequently, the similarity score is calculated solely for the documents that share keywords with the question posed. This approach ensures that the similarity assessment is conducted exclusively within the context of relevant keywords shared between the question and the documents.\\u003c/p\\u003e\\u003cp id=\\\"8875\\\"\\u003eTo create each vectorial database we use these functions :\\u003c/p\\u003e\\u003cdiv id=\\\"21ee\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.vectorstores \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e Chroma\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.document_loaders \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e DataFrameLoader\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create baseline\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003ecreate_baseline_db\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf, persist_directory, embedder=embedder\\u003c/span\\u003e):\\n    df_loader = DataFrameLoader(df, page_content_column=\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_content'\\u003c/span\\u003e)\\n    df_document = df_loader.load()\\n    db = Chroma.from_documents(df_document, embedder, persist_directory=persist_directory)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e db\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create other rags\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003ecreate_documents_db\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf_chunks, keywords = \\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, questions=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, name=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\u003c/span\\u003e, embedder = embedder\\u003c/span\\u003e):\\n    df_loader = DataFrameLoader(df_chunks, page_content_column=\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_content'\\u003c/span\\u003e)\\n    df_document = df_loader.load()\\n\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Add info to chunk content for augmented baseline\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n        doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"(article  pubId: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'pubId'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    title: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'title'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    date: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'date'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    anchor: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'anchor'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\"\\u003c/span\\u003e + \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"extract of doc: \\\"\\u003c/span\\u003e + doc.page_content\\n\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e keywords:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n            doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"keywords:  \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'keywords'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n\\\"\\u003c/span\\u003e + doc.page_content\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e questions:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n            doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"questions:  \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'potential_questions'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n\\\"\\u003c/span\\u003e + doc.page_content\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Embed document one by one to see progression\\u003c/span\\u003e\\n    db = \\u003cspan class=\\\"hljs-literal\\\"\\u003eNone\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ewith\\u003c/span\\u003e tqdm(total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Ingesting documents\\\"\\u003c/span\\u003e) \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pbar:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e d \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e df_document:\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e db:\\n                db.add_documents([d])\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n                db = Chroma.from_documents([d], embedder, persist_directory=name) \\n            pbar.update(\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{name}\\u003c/span\\u003e Done.\\\"\\u003c/span\\u003e)\\n\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e db\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"c0ff\\\"\\u003eThe function used to retrieve relevant documents is :\\u003c/p\\u003e\\u003cdiv id=\\\"93ac\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eensure_list_length\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003elst, target_length=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Calculate the number of elements to add\\u003c/span\\u003e\\n    elements_to_add = target_length - \\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(lst)\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Extend the list with None for any missing elements, if necessary\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e elements_to_add \\u0026gt; \\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e:\\n        lst.extend([\\u003cspan class=\\\"hljs-literal\\\"\\u003eNone\\u003c/span\\u003e] * elements_to_add)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e lst\\n\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eget_similar_docs\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf_eval, rag, prefix, questions_eval, softkeyword=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, remove_stops=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, rerank=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, model=model, k=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\\"\\n    Retrieve similar documents for a list of evaluation questions.\\n\\n    Args:\\n        df_eval (DataFrame): The DataFrame containing evaluation data (questions and truth chunk id).\\n        rag (RAG): The RAG for document retrieval.\\n        prefix (str): The prefix to use for the column names in the output DataFrame.\\n        questions_eval (list): A list of evaluation questions.\\n        softkeyword (bool, optional): Flag to indicate if soft keyword matching should be used. Defaults to False.\\n        remove_stops (bool, optional): Flag to indicate if stopwords should be removed from the questions. Defaults to False.\\n        rerank (bool, optional): Flag to indicate if documents should be reranked based on a model score. Defaults to False.\\n        model (Model, optional): The reranking model. Defaults to the global model \\\"model\\\".\\n        k (int, optional): The maximum number of similar documents to retrieve. Defaults to 5.\\n\\n    Returns:\\n        DataFrame: The DataFrame with added columns containing the IDs of similar documents for each question.\\n\\n    \\\"\\\"\\\"\\u003c/span\\u003e\\n    list_ids = []\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e question \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e questions_eval:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e softkeyword:\\n            where_document={}\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\n            \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Keywords retrivial on the question is based on the whole rag corpus minus stopwords\\u003c/span\\u003e\\n            where_document={\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"$or\\\"\\u003c/span\\u003e:[{\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"$contains\\\"\\u003c/span\\u003e:x} \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e x \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tools.extract_keywords_tfidf(tools.remove_french_stopwords(question).lower(), \\n                                                                                      corpus=[tools.remove_french_stopwords(x) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e x \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e rag.get()[\\u003cspan class=\\\"hljs-string\\\"\\u003e'documents'\\u003c/span\\u003e]], \\n                                                                                      top_n=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e)]} \\u003cspan class=\\\"hljs-comment\\\"\\u003e#corpus\\u003c/span\\u003e\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e remove_stops:\\n            question = tools.remove_french_stopwords(question.lower())\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n            question = question.lower()\\n        docs = rag.similarity_search_with_score(question, where_document=where_document, k=k)\\n        \\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e rerank: \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Predict a new score on question/chunk pair, take only raw content as input (after 'extract of doc: ')\\u003c/span\\u003e\\n            scores = model.predict([(question, doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e].page_content.split(\\u003cspan class=\\\"hljs-string\\\"\\u003e'extract of doc: '\\u003c/span\\u003e)[-\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e]) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e docs])\\n            docs = \\u003cspan class=\\\"hljs-built_in\\\"\\u003esorted\\u003c/span\\u003e([(doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e], score) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc, score \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"hljs-built_in\\\"\\u003ezip\\u003c/span\\u003e(docs, scores)], key=\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x: x[\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e], reverse=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n            \\n        ids = []\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e docs[:\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e]: \\u003cspan class=\\\"hljs-comment\\\"\\u003e#5 docs max\\u003c/span\\u003e\\n            doc_id = doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e].metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_id\\\"\\u003c/span\\u003e]\\n            ids.append(doc_id)\\n        \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Ensure list is 5 docs long\\u003c/span\\u003e\\n        ids = ensure_list_length(ids)\\n        list_ids.append(ids)\\n    df_eval[[\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc1\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc2\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc3\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc4\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc5\\\"\\u003c/span\\u003e]] = list_ids\\n    \\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"Done : \\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e\\\"\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e df_eval\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"26b3\\\"\\u003eThe \\u003ci\\u003esoftkeywords\\u003c/i\\u003e will consist on chunks containing keywords in headers without using them for filtering pre-scoring. For the latter, keywords are only impacting the embedding step of the process.\\u003c/p\\u003e\\u003cp id=\\\"6455\\\"\\u003eHere are the 7 different RAGs:\\u003c/p\\u003e\\u003cfigure id=\\\"0e76\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LaWTz7Y-s0ud1L4CkHTwYA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig3. 7 different types of chunks.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cfigure id=\\\"56ed\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qTmqZVdwed9O0a9RaTDL1A.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eTable1. Summary of RAGs tested.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003ch1 id=\\\"c52b\\\"\\u003eResults\\u003c/h1\\u003e\\u003cp id=\\\"d035\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eE5 Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"b1fe\\\"\\u003eUsing the E5 embedding model we created the 7 RAGs we show in the previous section and ran the 100 synthetic questions on the search similarity function of Lanchain. For each of our results, the accuracy is defined by wether or not the « true » chunk was found in the 1st, 2d, 3rd, 4th, 5th position or not at all (6+ position).\\u003c/p\\u003e\\u003cfigure id=\\\"2255\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wM2o0dTH_xOoqz2Gv_CJDg.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig4. E5 — Synthetic questions evaluation. (See appendix for a detailed summary of results)\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"873c\\\"\\u003eThe terms ‘\\u003ci\\u003eraw\\u003c/i\\u003e’ and ‘\\u003ci\\u003eclean\\u003c/i\\u003e’, displayed beneath the graph, represent whether the question was used in its original form or with stop words removed before entering the pipeline. Various tests indicate that removing stop words (‘\\u003ci\\u003eclean\\u003c/i\\u003e’) from the question did not significantly influence the results. Therefore, we have chosen not to dwell extensively on this aspect.\\u003c/p\\u003e\\u003cfigure id=\\\"f70f\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NyUBhJAwKvq-oyOpxGgwzA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig5. E5 — Synthetic questions evaluation — Cumulative sum 5 first retrieved documents.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"2a25\\\"\\u003eFirst, we observe that the baseline (simple chunks without additional information) is outperformed by every other type of chunk. Adding keywords, particularly with potential questions, appears to significantly enhance performance, with keyword filtering outperforming its « \\u003ci\\u003esoftkeywords\\u003c/i\\u003e » counterpart. However, it’s worth noting that this technique (keyword filtering) is inherently ‘destructive’, as numerous documents are disregarded if they fail to contain any of the question’s keywords.\\u003c/p\\u003e\\u003cp id=\\\"b7eb\\\"\\u003eTop results are the rags containing potential questions + keywords in headers with and without pre filtering using the keywords. Their results are respectively 95.1 and 92.1 accuracy score for chunk retrieval in the top 5 documents using similarity score.\\u003c/p\\u003e\\u003cp id=\\\"e0b3\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eBGE Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"b81c\\\"\\u003eUsing the same preset as the previous section, we ran the 100 synthetic questions on the BGE embedded RAGs.\\u003c/p\\u003e\\u003cfigure id=\\\"3166\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aS7dA1zJgxGK5KVFIewlxw.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig6. BGE — Synthetic questions evaluation.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"9369\\\"\\u003eResults here are pretty self explanatory, as BGE seems to outperform E5 on every type of chunking. We will show a better comparison in a next section. Like E5, baseline seems to be the less effective, validating the fact that adding informations and even keywords and potential questions does help in the retrieval step.\\u003c/p\\u003e\\u003cp id=\\\"64ea\\\"\\u003eThe cumulative sum of the accuracy for the first 5 documents is shown below.\\u003c/p\\u003e\\u003cfigure id=\\\"ab07\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*moEP4flOEZIiCmABJStgGA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig7. BGE — Synthetic questions evaluation — Cumulative sum.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"5b7e\\\"\\u003eLooking at the top 5 of documents retrieved, every RAGs — excluding the baseline — seem to perform about the same. On the top 2, the best RAGs are the augmented baseline (baseline + title etc) and augmented baseline with potential questions.\\u003c/p\\u003e\\u003cp id=\\\"e42a\\\"\\u003eRunning the same RAGs on the human questions set, we get the following results:\\u003c/p\\u003e\\u003cfigure id=\\\"61e8\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8l9jJ1oUaWDCH4W4c3dkow.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig8. BGE — Human questions evaluation — Cumulative sum.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"35a8\\\"\\u003eInterestingly enough, the potential questions added doesn’t seem to have the impact it had with E5. However, keywords have the same positive impact, followed closely by soft keywords.\\u003c/p\\u003e\\u003cp id=\\\"0217\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eSolon Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"3943\\\"\\u003eFor Solon, results are very close to BGE so we won’t go into details as the graph is quite self-explanatory.\\u003c/p\\u003e\\u003cfigure id=\\\"4d23\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KoSssEKmHdhxlFxPX2QQmw.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig9. Solon — Synthetic questions evaluation.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"54ec\\\"\\u003eOverall, BGE is better than both of the other embedding, for almost all types of RAGs.\\u003c/p\\u003e\\u003cp id=\\\"43df\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eE5 Vs BGE — With and without Reranker\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"fafb\\\"\\u003eTo compare E5 and BGE, we choose to keep the baselines and the questions + keywords RAGs. More, we also apply a soft keywords approach for those, so no filtering is done before similarity search. For the reranker, we choose to get 20 documents from the similarity search, and keep the 5 best score from the reranker out of them.\\u003c/p\\u003e\\u003cfigure id=\\\"f1de\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*y6GpcLijBeEoIO5o574OVA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig10. E5 vs BGE — With and without Reranker.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"8569\\\"\\u003eBGE baseline outperformed every E5 RAGs on the first document retrieval and is holding very well on the whole top 5. Furthermore, the addition of a reranker proved particularly beneficial, notably enhancing performance for BGE models with question and keywords added in the chunks headers.\\u003c/p\\u003e\\u003ch1 id=\\\"7288\\\"\\u003eConclusion\\u003c/h1\\u003e\\u003cp id=\\\"070c\\\"\\u003eIn this research, we embarked on a comprehensive exploration of leveraging Retrieval-Augmented Generation (RAG) and chunking techniques to enhance language model performance. The widespread utilization of Large Language Models (LLMs) in various applications underscores the importance of optimizing their capabilities for efficient data processing. Our study focused on three key aspects: embedding models, chunk augmentation techniques, and reranking methodologies.\\u003c/p\\u003e\\u003cp id=\\\"c939\\\"\\u003eFor embedding models, we conducted a comparative analysis of three prominent models: e5, BGE, and Solon. Each model exhibited unique characteristics and strengths, with BGE demonstrating superior performance across different chunking configurations. The evaluation of chunking techniques revealed that incorporating contextual information, such as keywords and potential questions, significantly improved document retrieval accuracy. However, it’s essential to acknowledge the inherent trade-off associated with keyword filtering, which can lead to the exclusion of relevant documents.\\u003c/p\\u003e\\u003cp id=\\\"67fe\\\"\\u003eFurthermore, our investigation into reranking mechanisms highlighted their efficacy in refining search results based on relevance to the query. The application of reranking, particularly in conjunction with BGE embeddings and augmented chunking, yielded notable improvements in document retrieval accuracy.\\u003c/p\\u003e\\u003cp id=\\\"bce3\\\"\\u003eOverall, our findings underscore the importance of considering various factors, including embedding models, chunking strategies, and reranking techniques, in optimizing LLM performance for data-driven tasks. By shedding light on the nuances of these approaches, our research provides useful insights into how they can help improve information retrieval.\\u003c/p\\u003e\\u003ch2 id=\\\"6a12\\\"\\u003eFuture researches\\u003c/h2\\u003e\\u003cp id=\\\"76d0\\\"\\u003eIn future research, we could delve into additional variations of chunking techniques to further refine our understanding of their effectiveness. For instance, exploring the potential of one-sentence chunking and embedding could provide insights into the granularity of information necessary for optimal document retrieval. Additionally, investigating the selective chunking of “potential questions”, either with or without accompanying keywords, offers an opportunity to tailor contextual information more precisely to the language model’s needs. By systematically evaluating these alternative approaches, we can gain a deeper understanding of their implications for enhancing LLM performance and information retrieval accuracy.\\u003c/p\\u003e\\u003cp id=\\\"564d\\\"\\u003eRessources: Code and Notebook available on \\u003ca href=\\\"https://gitlab.com/etalab-datalab/llm/challenge-datascience/-/tree/main/chunking_augmentation\\\"\\u003eGitLab\\u003c/a\\u003e\\u003c/p\\u003e\\u003cp id=\\\"84f3\\\"\\u003e\\u003ca href=\\\"https://www.etalab.gouv.fr\\\"\\u003e\\u003ci\\u003eCamille André — Data Scientist @ Etalab — Direction Interministérielle du Numérique\\u003c/i\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\u003c/article\\u003e\\u003c/body\\u003e\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"19:T89c,\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"# Summary\\n\\nThe provided text discusses the enhancement of language model performance through the application of retrieval-augmented generation (RAG) techniques, embedding models, and chunking strategies, with a focus on evaluating their effectiveness using a dataset from service-public.fr.\\n\\n# Abstract\\n\\nThe article delves into the optimization of Large Language Models (LLMs) by exploring the impact of chunking techniques and embedding models on document retrieval. It details the process of chunking text data into meaningful units, enriching these chunks with contextual information such as headers containing metadata, keywords, and potential questions. The study compares the performance of three embedding models—E5, BGE, and Solon—using synthetic and human-generated questions. The results indicate that adding contextual information, especially keywords and potential questions, significantly improves retrieval accuracy. BGE emerges as the top-performing embedding model, and the use of rerankers further refines the search results. The research concludes that these strategies are crucial for enhancing the performance of LLMs in real-world applications, with BGE showing particular promise for French language data.\\n\\n# Opinions\\n\\n\\n- The baseline chunking without additional context is deemed less effective than augmented chunking techniques.\\n- Adding keywords and potential questions to chunks is seen as highly beneficial for improving document retrieval.\\n- Keyword filtering, while effective, is recognized as a 'destructive' method that may exclude relevant documents.\\n- The BGE embedding model consistently outperforms E5 and Solon across various chunking configurations.\\n- Reranking is considered a valuable method for enhancing the relevance of search results, especially when combined with BGE embeddings.\\n- The study emphasizes the importance of context in chunking for better language model performance.\\n- The research suggests that the choice of embedding model and chunking strategy should be tailored to the specific language and use case.\\n- Future research is encouraged to explore alternative chunking granularities and the selective use of potential questions and keywords.\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"1a:T8e72,\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"\\u003cbody\\u003e\\u003carticle\\u003e\\u003ch1 id=\\\"c905\\\"\\u003eEnhancing Language Model Performance: Insights into Retrieval-Augmented Generation and Chunking Techniques\\u003c/h1\\u003e\\u003cfigure id=\\\"8c7a\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4EjerlC79zcw-_zuyKioEw.jpeg\\\"\\u003e\\u003cfigcaption\\u003ePhoto de \\u003ca href=\\\"https://unsplash.com/fr/@impatrickt?utm_content=creditCopyText\\u0026amp;utm_medium=referral\\u0026amp;utm_source=unsplash\\\"\\u003ePatrick Tomasso\\u003c/a\\u003e sur \\u003ca href=\\\"https://unsplash.com/fr/photos/lot-a-livre-ouvert-Oaqk7qqNh_c?utm_content=creditCopyText\\u0026amp;utm_medium=referral\\u0026amp;utm_source=unsplash\\\"\\u003eUnsplash\\u003c/a\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003ch1 id=\\\"a693\\\"\\u003eIntroduction\\u003c/h1\\u003e\\u003cp id=\\\"e659\\\"\\u003eLarge Language Models (LLMs) are now everywhere, offering powerful tools that can be seamlessly integrated with your data thanks to the magic of Retrieval-Augmented Generation (RAG). In this article, we’re diving into some key areas to see how retrieval performance can be boosted with:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eEmbedding models: E5, BGE and Solon\\u003c/li\\u003e\\u003cli\\u003eChunking augmentation techniques: How to add ‘context’ with minimal efforts to our chunks\\u003c/li\\u003e\\u003cli\\u003eReranking: Are rerankers a good addition to a RAG pipeline ?\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"8853\\\"\\u003eBy putting these components to the test, we hope to uncover some valuable insights into how to make the most of language models in real-world applications.\\u003c/p\\u003e\\u003ch1 id=\\\"f253\\\"\\u003eChunk Augmentation: Adding Context for Enhanced Document Retrieval\\u003c/h1\\u003e\\u003cp id=\\\"7e3a\\\"\\u003e\\u003ci\\u003eThe data we’ll be using for this article comes from \\u003ca href=\\\"https://www.service-public.fr\\\"\\u003eservice-public.fr\\u003c/a\\u003e and mainly contains guides and answers to French administrative questions. All the data used can be found \\u003ca href=\\\"https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/i\\u003e\\u003c/p\\u003e\\u003cp id=\\\"4e11\\\"\\u003eFor the preprocessing, we only have to read our json, declare and format the columns we are going to use for the chunking:\\u003c/p\\u003e\\u003cdiv id=\\\"1d43\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e pandas \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pd\\n\\ndf =pd.read_json(\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"data/fiches-travail.json\\\"\\u003c/span\\u003e)\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eprep_specific\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf\\u003c/span\\u003e):\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections\\\"\\u003c/span\\u003e].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e).apply(literal_eval)\\n    df = df.explode(\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e).reset_index(drop=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"text_section\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e].apply(\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x : x[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"text\\\"\\u003c/span\\u003e])\\n    df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e] = df[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"sections_all\\\"\\u003c/span\\u003e].apply(\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x : x[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e])\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e df\\n\\ndf = prep_specific(df)\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Column of interest\\u003c/span\\u003e\\ntext_col = \\u003cspan class=\\\"hljs-string\\\"\\u003e'text_section'\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Metadata columns\\u003c/span\\u003e\\nmetadata_cols = [\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"pubId\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"date\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e]\\n\\ndf[text_col] = df[text_col].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e).\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e.lower()\\n\\ndf[metadata_cols] = df[metadata_cols].astype(\\u003cspan class=\\\"hljs-built_in\\\"\\u003estr\\u003c/span\\u003e) \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Ensuring metadata columns are in string format aswell\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Fast cleaning\\u003c/span\\u003e\\ndf = df[~df[\\u003cspan class=\\\"hljs-string\\\"\\u003e'pubId'\\u003c/span\\u003e].isna()]\\ndf = df[~df[\\u003cspan class=\\\"hljs-string\\\"\\u003e'date'\\u003c/span\\u003e].isna()].reset_index(drop=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\ndf[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e] = df[text_col].apply(tools.remove_french_stopwords)\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Corpus for keywords retrieval\\u003c/span\\u003e\\ncorpus = df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"_clean\\\"\\u003c/span\\u003e]\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Keywords retrieval (code on git)\\u003c/span\\u003e\\ndf[\\u003cspan class=\\\"hljs-string\\\"\\u003e'keywords'\\u003c/span\\u003e] = tools.extract_keywords_tfidf(df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e], corpus=df[text_col+\\u003cspan class=\\\"hljs-string\\\"\\u003e'_clean'\\u003c/span\\u003e], top_n=\\u003cspan class=\\\"hljs-number\\\"\\u003e10\\u003c/span\\u003e)\\n\\ndf.to_csv(\\u003cspan class=\\\"hljs-string\\\"\\u003e'data.csv'\\u003c/span\\u003e)\\n\\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"df shape: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{df.shape}\\u003c/span\\u003e\\\"\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"0718\\\"\\u003eChunking is a natural language processing technique that involves breaking down a text into smaller, meaningful units or “chunks”. These chunks typically consist of words or phrases that convey a specific idea or concept within the context of the text. Chunking helps organize and structure information, making it easier for models to analyze and understand the content.\\u003c/p\\u003e\\u003cfigure id=\\\"ad04\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NMCaeZ07OH3mNdAF0fVhwA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig1. Visualisation of chunking a text (\\u003ca href=\\\"https://huggingface.co/spaces/m-ric/chunk_visualizer\\\"\\u003ehttps://huggingface.co/spaces/m-ric/chunk_visualizer\\u003c/a\\u003e)\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"9eef\\\"\\u003eFor the preliminary chunking, we are using the Langchain function `RecursiveCharacterTextSplitter` with a maximum character length of 4000, which seems a reasonable size for a future LLM (at the end of the RAG pipeline) with a context length of 8k tokens, to which we’ll be able to give 4–5 chunks as context per request.\\u003c/p\\u003e\\u003cp id=\\\"3d83\\\"\\u003eTo create each RAG, we will start with a common base, which is a dataframe structured as follows:\\u003c/p\\u003e\\u003cdiv id=\\\"ad8a\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e pandas \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pd\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e tqdm \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e tqdm\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.document_loaders \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e DataFrameLoader\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain_text_splitters \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e RecursiveCharacterTextSplitter\\n\\ndf = pd.read_csv(\\u003cspan class=\\\"hljs-string\\\"\\u003e'data.csv'\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create every rags from the same base\\u003c/span\\u003e\\ntext_col = \\u003cspan class=\\\"hljs-string\\\"\\u003e'text_section'\\u003c/span\\u003e\\nmetadata_cols = [\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"pubId\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"date\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"keywords\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"anchor\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"url\\\"\\u003c/span\\u003e]\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Make a document list with every columns we could need in the metadatas\\u003c/span\\u003e\\ndf_loader = DataFrameLoader(df[[text_col]+metadata_cols], page_content_column=text_col)\\ndf_document = df_loader.load()\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e#Slipt document in chunks\\u003c/span\\u003e\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=\\u003cspan class=\\\"hljs-number\\\"\\u003e4000\\u003c/span\\u003e,\\n    chunk_overlap=\\u003cspan class=\\\"hljs-number\\\"\\u003e50\\u003c/span\\u003e,\\n    length_function=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e,\\n    separators=[\\u003cspan class=\\\"hljs-string\\\"\\u003e'\\\\n\\\\n'\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003e'. '\\u003c/span\\u003e] \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Dont cut sentences\\u003c/span\\u003e\\n)\\nbaseline_docs = text_splitter.split_documents(df_document)\\n\\nchunk_ids = []\\nchunk_contents = []\\nchunk_metadatas = []\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Add a chunk_id and potential questions to metadatas\\u003c/span\\u003e\\nllm = tools.load_llm()\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(baseline_docs), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(baseline_docs), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n    doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_id'\\u003c/span\\u003e]=i\\n    doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'potential_questions'\\u003c/span\\u003e] = tools.run_question_rag(llm, \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\u003c/span\\u003e, doc.page_content, prompt=tools.prompt_create_questions)\\n    chunk_ids.append(i)\\n    chunk_contents.append(doc.page_content)\\n    chunk_metadatas.append(doc.metadata)\\n\\ndf_chunks = pd.DataFrame()\\ndf_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_content\\\"\\u003c/span\\u003e] = chunk_contents\\ndf_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_metadata\\\"\\u003c/span\\u003e] = chunk_metadatas\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Recreate the columns from metadatas\\u003c/span\\u003e\\nmetadata_df = df_chunks[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_metadata'\\u003c/span\\u003e].apply(pd.Series)\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# This dataframe will be the base for every rags\\u003c/span\\u003e\\ndf_chunks = pd.concat([df_chunks.drop(columns=[\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_metadata'\\u003c/span\\u003e]), metadata_df], axis=\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"b7e8\\\"\\u003eTo evaluate the performance of different types of chunks, we will try the following:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eBaseline: The baseline will consist of simple chunks without any added headers or informations\\u003c/li\\u003e\\u003cli\\u003eAugmented baseline: Every other RAGs will consist of the baseline to which will be added some informations we have on each article of our database. This way, we will add as a header to each chunk data such as \\u003ci\\u003eparent document title\\u003c/i\\u003e, \\u003ci\\u003eparent document date\\u003c/i\\u003e, \\u003ci\\u003eparent document url\\u003c/i\\u003e.\\u003c/li\\u003e\\u003cli\\u003eKeywords: The RAGs we will evaluate that will have keywords in their names will consist of the augmented baseline + Top 10 keywords (found using TF-IDF) of the parent document (again, added in headers).\\u003c/li\\u003e\\u003cli\\u003ePotential questions: Questions a chunk should be able to answer.\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"627e\\\"\\u003eThe main goal of a RAG is to find relevant documents that should help a model to answer a given question. Given this, we will try a third approach consisting in adding « potential questions » — questions that a given chunk can answer — to each chunk, as always, in header.\\u003c/p\\u003e\\u003cp id=\\\"e0da\\\"\\u003eTo generate these questions, we gave each chunk we created before to a LLM (mistral 7B — OpenHermes), with the following prompt:\\u003c/p\\u003e\\u003cblockquote id=\\\"d4f3\\\"\\u003e\\u003cp\\u003e« En t’inspirant du texte ci-dessous, réponds uniquement avec une liste de 2–3 questions très précises et détaillées que l’on pourrait poser sur ce texte et dont la réponse se trouve dans ce texte. Si le texte est très court ne réponds qu’avec une ou deux questions. N’inventes pas des questions qui ne veulent rien dire, si tu n’as qu’une seule question c’est très bien aussi. Les questions ne doivent pas être vagues, et pouvoir être associées très facilement à ce texte si on les comparait à d’autres questions pour d’autres textes. Si une question porte sur l’article de manière générale, le numéro ou l’id de l’article doit être contenu dans la question. Les questions doivent contenir les éléments importants comme des dates ou des numéros si besoin. Voici des exemples pour t’aider:\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"6e7b\\\"\\u003e\\u003cp\\u003eMauvaises questions qu’on ne veut pas: [“De quoi parle ce texte ?”, “Qui est mentionné ici ?”] (trop vague)\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"2991\\\"\\u003e\\u003cp\\u003eBonne question: [“Quel groupe est concerné par la décision XX-XXX-XX ?”] (net et précis)\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cblockquote id=\\\"91b3\\\"\\u003e\\u003cp\\u003eVoilà le texte: {article} » Prompt 1 — Create potential questions\\u003c/p\\u003e\\u003c/blockquote\\u003e\\u003cp id=\\\"45dd\\\"\\u003eWe are now ready to test our baseline and our augmented baseline to which we can add — or not — keywords and/or questions making a total of 5 different types of chunks:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eBaseline\\u003c/li\\u003e\\u003cli\\u003eWithout keywords — without questions (Augmented baseline)\\u003c/li\\u003e\\u003cli\\u003eWith keywords — without questions\\u003c/li\\u003e\\u003cli\\u003eWithout keywords — with questions\\u003c/li\\u003e\\u003cli\\u003eWith keywords — with questions\\u003c/li\\u003e\\u003c/ul\\u003e\\u003ch1 id=\\\"d873\\\"\\u003eEmbedding Models : Text representation\\u003c/h1\\u003e\\u003cp id=\\\"8f29\\\"\\u003eText embedding is the process of representing text data, such as words, sentences, or documents, as dense numerical vectors in a high-dimensional vector space. These vector representations aim to capture the semantic and contextual meanings of the text, where similar texts are mapped to vectors that are close together in the vector space.\\u003c/p\\u003e\\u003cfigure id=\\\"c3ab\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5-GVldd6K4sHULqMhG78bQ.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig2. Embeddings: from text to vectors\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"67d2\\\"\\u003eIn this paper, we will compare 3 embedding models against each other which are the following:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/intfloat/multilingual-e5-large\\\"\\u003eintfloat/e5-large\\u003c/a\\u003e: \\u003ca href=\\\"https://arxiv.org/pdf/2402.05672\\\"\\u003ehttps://arxiv.org/pdf/2402.05672\\u003c/a\\u003e\\u003c/li\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/BAAI/bge-m3\\\"\\u003eBAAI/bge-m3\\u003c/a\\u003e: \\u003ca href=\\\"https://arxiv.org/pdf/2402.03216\\\"\\u003ehttps://arxiv.org/pdf/2402.03216\\u003c/a\\u003e\\u003c/li\\u003e\\u003cli\\u003e\\u003ca href=\\\"https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1\\\"\\u003eOrdalieTech/Solon-embeddings-large-0.1\\u003c/a\\u003e: SOTA Open source french embedding model.\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"308b\\\"\\u003eThe e5-large model is a text embedding model developed by Microsoft that uses weakly-supervised contrastive pre-training to generate high-quality embeddings. The model is trained on a mixture of datasets and can be used for various natural language processing tasks such as semantic similarity, information retrieval, and text clustering by encoding input text into dense vector representations.\\u003c/p\\u003e\\u003cp id=\\\"41a2\\\"\\u003eThe bge-m3 model is a multilingual text embedding model developed by the Beijing Academy of Artificial Intelligence (BAAI). It supports over 100 languages and can handle long input sequences up to 8192 tokens. This makes it suitable for embedding multi-lingual and long-form text data.\\u003c/p\\u003e\\u003cp id=\\\"ec61\\\"\\u003eFinally, the Solon model is a State-Of-The-Art Open source french embedding model. Data used in the paper being in French, we had to test it against robust multilingual models.\\u003c/p\\u003e\\u003ch1 id=\\\"9170\\\"\\u003eRerankers\\u003c/h1\\u003e\\u003cp id=\\\"3ff7\\\"\\u003eDuring our tests, we’ll also explore reranking, a method employing a reranking model, often referred to as a cross-encoder. This model assesses the similarity between each query-document pair, enabling us to rearrange the documents according to their relevance to the query. After the initial search using similarity scores, the reranker is then applied, allowing for a broader selection of documents. For instance, we might retrieve the 20 most similar documents, rerank them, and select only the top 5 to provide as context for our LLM.\\u003c/p\\u003e\\u003cp id=\\\"02bd\\\"\\u003eThe reranker model used in this article is \\u003ca href=\\\"https://huggingface.co/antoinelouis/crossencoder-electra-base-mmarcoFR\\\"\\u003ecrossencoder-electra-base-french-mmarcoFR\\u003c/a\\u003e, a model trained on French training samples from the \\u003ca href=\\\"https://huggingface.co/datasets/unicamp-dl/mmarco\\\"\\u003emMARCO\\u003c/a\\u003e dataset.\\u003c/p\\u003e\\u003cdiv id=\\\"328a\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Load reranker\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e sentence_transformers \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e CrossEncoder\\nmodel = CrossEncoder(\\u003cspan class=\\\"hljs-string\\\"\\u003e'antoinelouis/crossencoder-electra-base-french-mmarcoFR'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\u003ch1 id=\\\"acf1\\\"\\u003eEvaluation\\u003c/h1\\u003e\\u003cp id=\\\"fbbf\\\"\\u003eTo evaluate the performance of each chunking techniques and embeddings models, we will use two different sets of questions:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eSynthetic: Out of the 1733 chunks we have in our database, 100 were given to a LLM which output a specific question that should be answerable with the chunk content\\u003c/li\\u003e\\u003cli\\u003eHuman: 20 chunks of the database were chosen randomly by us from which we created more complex questions that should be answerable with the chunk content\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"8d68\\\"\\u003eWe adopted a similar prompt to \\u003ci\\u003ePrompt 1\\u003c/i\\u003e for generating synthetic questions. Interestingly, one might notice similarities or even nearly identical questions between the set of synthetic questions and the potential questions added in the header for RAGs containing potential questions. While this observation holds true, it’s important to note that in real-life scenarios, some questions will inevitably overlap with those pre-generated by the potential questions in the header. This overlap serves as one of their primary objectives.\\u003c/p\\u003e\\u003cp id=\\\"a05c\\\"\\u003eThe accuracy score in graphs represents the % of True (relevant) documents found in top 1, top 2 and so on.\\u003c/p\\u003e\\u003ch1 id=\\\"d28b\\\"\\u003eSetting up vector databases\\u003c/h1\\u003e\\u003cp id=\\\"ad9a\\\"\\u003eIntroduced in part « Augmented Chunking », we will test 5 different RAGs, one for each chunk configuration. Added to that, we will also test 2 techniques using the keywords for the 2 RAGs containing keywords lists in header of chunks:\\u003c/p\\u003e\\u003cul\\u003e\\u003cli\\u003eFiltered\\u003c/li\\u003e\\u003cli\\u003eSoftkeywords\\u003c/li\\u003e\\u003c/ul\\u003e\\u003cp id=\\\"e3d7\\\"\\u003eFor the retrieval, we will use the Langchain function `\\u003ca href=\\\"https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.similarity_search_with_score\\\"\\u003esimilarity_search_with_score\\u003c/a\\u003e`. The \\u003ci\\u003eFiltered \\u003c/i\\u003etechnique will consist as initiating a filter on documents to extract only those containing at least one keyword from the question within their header’s keyword list. Subsequently, the similarity score is calculated solely for the documents that share keywords with the question posed. This approach ensures that the similarity assessment is conducted exclusively within the context of relevant keywords shared between the question and the documents.\\u003c/p\\u003e\\u003cp id=\\\"8875\\\"\\u003eTo create each vectorial database we use these functions :\\u003c/p\\u003e\\u003cdiv id=\\\"21ee\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.vectorstores \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e Chroma\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003efrom\\u003c/span\\u003e langchain.document_loaders \\u003cspan class=\\\"hljs-keyword\\\"\\u003eimport\\u003c/span\\u003e DataFrameLoader\\n\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create baseline\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003ecreate_baseline_db\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf, persist_directory, embedder=embedder\\u003c/span\\u003e):\\n    df_loader = DataFrameLoader(df, page_content_column=\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_content'\\u003c/span\\u003e)\\n    df_document = df_loader.load()\\n    db = Chroma.from_documents(df_document, embedder, persist_directory=persist_directory)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e db\\n\\u003cspan class=\\\"hljs-comment\\\"\\u003e# Create other rags\\u003c/span\\u003e\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003ecreate_documents_db\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf_chunks, keywords = \\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, questions=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, name=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\u003c/span\\u003e, embedder = embedder\\u003c/span\\u003e):\\n    df_loader = DataFrameLoader(df_chunks, page_content_column=\\u003cspan class=\\\"hljs-string\\\"\\u003e'chunk_content'\\u003c/span\\u003e)\\n    df_document = df_loader.load()\\n\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Add info to chunk content for augmented baseline\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n        doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"(article  pubId: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'pubId'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    title: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'title'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    date: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'date'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\\\\n    anchor: \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'anchor'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n \\\"\\u003c/span\\u003e + \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"extract of doc: \\\"\\u003c/span\\u003e + doc.page_content\\n\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e keywords:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n            doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"keywords:  \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'keywords'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n\\\"\\u003c/span\\u003e + doc.page_content\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e questions:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e i, doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tqdm(\\u003cspan class=\\\"hljs-built_in\\\"\\u003eenumerate\\u003c/span\\u003e(df_document), total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Processing documents\\\"\\u003c/span\\u003e):\\n            doc.page_content = \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"questions:  \\u003cspan class=\\\"hljs-subst\\\"\\u003e{doc.metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e'potential_questions'\\u003c/span\\u003e]}\\u003c/span\\u003e \\\\n\\\"\\u003c/span\\u003e + doc.page_content\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Embed document one by one to see progression\\u003c/span\\u003e\\n    db = \\u003cspan class=\\\"hljs-literal\\\"\\u003eNone\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ewith\\u003c/span\\u003e tqdm(total=\\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(df_document), desc=\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"Ingesting documents\\\"\\u003c/span\\u003e) \\u003cspan class=\\\"hljs-keyword\\\"\\u003eas\\u003c/span\\u003e pbar:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e d \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e df_document:\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e db:\\n                db.add_documents([d])\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n                db = Chroma.from_documents([d], embedder, persist_directory=name) \\n            pbar.update(\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{name}\\u003c/span\\u003e Done.\\\"\\u003c/span\\u003e)\\n\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e db\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"c0ff\\\"\\u003eThe function used to retrieve relevant documents is :\\u003c/p\\u003e\\u003cdiv id=\\\"93ac\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eensure_list_length\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003elst, target_length=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Calculate the number of elements to add\\u003c/span\\u003e\\n    elements_to_add = target_length - \\u003cspan class=\\\"hljs-built_in\\\"\\u003elen\\u003c/span\\u003e(lst)\\n    \\u003cspan class=\\\"hljs-comment\\\"\\u003e# Extend the list with None for any missing elements, if necessary\\u003c/span\\u003e\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e elements_to_add \\u0026gt; \\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e:\\n        lst.extend([\\u003cspan class=\\\"hljs-literal\\\"\\u003eNone\\u003c/span\\u003e] * elements_to_add)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e lst\\n\\n\\u003cspan class=\\\"hljs-keyword\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"hljs-title function_\\\"\\u003eget_similar_docs\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-params\\\"\\u003edf_eval, rag, prefix, questions_eval, softkeyword=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, remove_stops=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, rerank=\\u003cspan class=\\\"hljs-literal\\\"\\u003eFalse\\u003c/span\\u003e, model=model, k=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"\\\"\\\"\\n    Retrieve similar documents for a list of evaluation questions.\\n\\n    Args:\\n        df_eval (DataFrame): The DataFrame containing evaluation data (questions and truth chunk id).\\n        rag (RAG): The RAG for document retrieval.\\n        prefix (str): The prefix to use for the column names in the output DataFrame.\\n        questions_eval (list): A list of evaluation questions.\\n        softkeyword (bool, optional): Flag to indicate if soft keyword matching should be used. Defaults to False.\\n        remove_stops (bool, optional): Flag to indicate if stopwords should be removed from the questions. Defaults to False.\\n        rerank (bool, optional): Flag to indicate if documents should be reranked based on a model score. Defaults to False.\\n        model (Model, optional): The reranking model. Defaults to the global model \\\"model\\\".\\n        k (int, optional): The maximum number of similar documents to retrieve. Defaults to 5.\\n\\n    Returns:\\n        DataFrame: The DataFrame with added columns containing the IDs of similar documents for each question.\\n\\n    \\\"\\\"\\\"\\u003c/span\\u003e\\n    list_ids = []\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e question \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e questions_eval:\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e softkeyword:\\n            where_document={}\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e: \\n            \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Keywords retrivial on the question is based on the whole rag corpus minus stopwords\\u003c/span\\u003e\\n            where_document={\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"$or\\\"\\u003c/span\\u003e:[{\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"$contains\\\"\\u003c/span\\u003e:x} \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e x \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e tools.extract_keywords_tfidf(tools.remove_french_stopwords(question).lower(), \\n                                                                                      corpus=[tools.remove_french_stopwords(x) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e x \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e rag.get()[\\u003cspan class=\\\"hljs-string\\\"\\u003e'documents'\\u003c/span\\u003e]], \\n                                                                                      top_n=\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e)]} \\u003cspan class=\\\"hljs-comment\\\"\\u003e#corpus\\u003c/span\\u003e\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e remove_stops:\\n            question = tools.remove_french_stopwords(question.lower())\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n            question = question.lower()\\n        docs = rag.similarity_search_with_score(question, where_document=where_document, k=k)\\n        \\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eif\\u003c/span\\u003e rerank: \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Predict a new score on question/chunk pair, take only raw content as input (after 'extract of doc: ')\\u003c/span\\u003e\\n            scores = model.predict([(question, doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e].page_content.split(\\u003cspan class=\\\"hljs-string\\\"\\u003e'extract of doc: '\\u003c/span\\u003e)[-\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e]) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e docs])\\n            docs = \\u003cspan class=\\\"hljs-built_in\\\"\\u003esorted\\u003c/span\\u003e([(doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e], score) \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc, score \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"hljs-built_in\\\"\\u003ezip\\u003c/span\\u003e(docs, scores)], key=\\u003cspan class=\\\"hljs-keyword\\\"\\u003elambda\\u003c/span\\u003e x: x[\\u003cspan class=\\\"hljs-number\\\"\\u003e1\\u003c/span\\u003e], reverse=\\u003cspan class=\\\"hljs-literal\\\"\\u003eTrue\\u003c/span\\u003e)\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003eelse\\u003c/span\\u003e:\\n            \\u003cspan class=\\\"hljs-keyword\\\"\\u003epass\\u003c/span\\u003e\\n            \\n        ids = []\\n        \\u003cspan class=\\\"hljs-keyword\\\"\\u003efor\\u003c/span\\u003e doc \\u003cspan class=\\\"hljs-keyword\\\"\\u003ein\\u003c/span\\u003e docs[:\\u003cspan class=\\\"hljs-number\\\"\\u003e5\\u003c/span\\u003e]: \\u003cspan class=\\\"hljs-comment\\\"\\u003e#5 docs max\\u003c/span\\u003e\\n            doc_id = doc[\\u003cspan class=\\\"hljs-number\\\"\\u003e0\\u003c/span\\u003e].metadata[\\u003cspan class=\\\"hljs-string\\\"\\u003e\\\"chunk_id\\\"\\u003c/span\\u003e]\\n            ids.append(doc_id)\\n        \\u003cspan class=\\\"hljs-comment\\\"\\u003e#Ensure list is 5 docs long\\u003c/span\\u003e\\n        ids = ensure_list_length(ids)\\n        list_ids.append(ids)\\n    df_eval[[\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc1\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc2\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc3\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc4\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"\\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e_doc5\\\"\\u003c/span\\u003e]] = list_ids\\n    \\u003cspan class=\\\"hljs-built_in\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"hljs-string\\\"\\u003ef\\\"Done : \\u003cspan class=\\\"hljs-subst\\\"\\u003e{prefix}\\u003c/span\\u003e\\\"\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"hljs-keyword\\\"\\u003ereturn\\u003c/span\\u003e df_eval\\u003c/pre\\u003e\\u003c/div\\u003e\\u003cp id=\\\"26b3\\\"\\u003eThe \\u003ci\\u003esoftkeywords\\u003c/i\\u003e will consist on chunks containing keywords in headers without using them for filtering pre-scoring. For the latter, keywords are only impacting the embedding step of the process.\\u003c/p\\u003e\\u003cp id=\\\"6455\\\"\\u003eHere are the 7 different RAGs:\\u003c/p\\u003e\\u003cfigure id=\\\"0e76\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LaWTz7Y-s0ud1L4CkHTwYA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig3. 7 different types of chunks.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cfigure id=\\\"56ed\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qTmqZVdwed9O0a9RaTDL1A.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eTable1. Summary of RAGs tested.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003ch1 id=\\\"c52b\\\"\\u003eResults\\u003c/h1\\u003e\\u003cp id=\\\"d035\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eE5 Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"b1fe\\\"\\u003eUsing the E5 embedding model we created the 7 RAGs we show in the previous section and ran the 100 synthetic questions on the search similarity function of Lanchain. For each of our results, the accuracy is defined by wether or not the « true » chunk was found in the 1st, 2d, 3rd, 4th, 5th position or not at all (6+ position).\\u003c/p\\u003e\\u003cfigure id=\\\"2255\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wM2o0dTH_xOoqz2Gv_CJDg.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig4. E5 — Synthetic questions evaluation. (See appendix for a detailed summary of results)\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"873c\\\"\\u003eThe terms ‘\\u003ci\\u003eraw\\u003c/i\\u003e’ and ‘\\u003ci\\u003eclean\\u003c/i\\u003e’, displayed beneath the graph, represent whether the question was used in its original form or with stop words removed before entering the pipeline. Various tests indicate that removing stop words (‘\\u003ci\\u003eclean\\u003c/i\\u003e’) from the question did not significantly influence the results. Therefore, we have chosen not to dwell extensively on this aspect.\\u003c/p\\u003e\\u003cfigure id=\\\"f70f\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NyUBhJAwKvq-oyOpxGgwzA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig5. E5 — Synthetic questions evaluation — Cumulative sum 5 first retrieved documents.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"2a25\\\"\\u003eFirst, we observe that the baseline (simple chunks without additional information) is outperformed by every other type of chunk. Adding keywords, particularly with potential questions, appears to significantly enhance performance, with keyword filtering outperforming its « \\u003ci\\u003esoftkeywords\\u003c/i\\u003e » counterpart. However, it’s worth noting that this technique (keyword filtering) is inherently ‘destructive’, as numerous documents are disregarded if they fail to contain any of the question’s keywords.\\u003c/p\\u003e\\u003cp id=\\\"b7eb\\\"\\u003eTop results are the rags containing potential questions + keywords in headers with and without pre filtering using the keywords. Their results are respectively 95.1 and 92.1 accuracy score for chunk retrieval in the top 5 documents using similarity score.\\u003c/p\\u003e\\u003cp id=\\\"e0b3\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eBGE Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"b81c\\\"\\u003eUsing the same preset as the previous section, we ran the 100 synthetic questions on the BGE embedded RAGs.\\u003c/p\\u003e\\u003cfigure id=\\\"3166\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aS7dA1zJgxGK5KVFIewlxw.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig6. BGE — Synthetic questions evaluation.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"9369\\\"\\u003eResults here are pretty self explanatory, as BGE seems to outperform E5 on every type of chunking. We will show a better comparison in a next section. Like E5, baseline seems to be the less effective, validating the fact that adding informations and even keywords and potential questions does help in the retrieval step.\\u003c/p\\u003e\\u003cp id=\\\"64ea\\\"\\u003eThe cumulative sum of the accuracy for the first 5 documents is shown below.\\u003c/p\\u003e\\u003cfigure id=\\\"ab07\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*moEP4flOEZIiCmABJStgGA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig7. BGE — Synthetic questions evaluation — Cumulative sum.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"5b7e\\\"\\u003eLooking at the top 5 of documents retrieved, every RAGs — excluding the baseline — seem to perform about the same. On the top 2, the best RAGs are the augmented baseline (baseline + title etc) and augmented baseline with potential questions.\\u003c/p\\u003e\\u003cp id=\\\"e42a\\\"\\u003eRunning the same RAGs on the human questions set, we get the following results:\\u003c/p\\u003e\\u003cfigure id=\\\"61e8\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8l9jJ1oUaWDCH4W4c3dkow.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig8. BGE — Human questions evaluation — Cumulative sum.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"35a8\\\"\\u003eInterestingly enough, the potential questions added doesn’t seem to have the impact it had with E5. However, keywords have the same positive impact, followed closely by soft keywords.\\u003c/p\\u003e\\u003cp id=\\\"0217\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eSolon Embeddings\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"3943\\\"\\u003eFor Solon, results are very close to BGE so we won’t go into details as the graph is quite self-explanatory.\\u003c/p\\u003e\\u003cfigure id=\\\"4d23\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KoSssEKmHdhxlFxPX2QQmw.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig9. Solon — Synthetic questions evaluation.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"54ec\\\"\\u003eOverall, BGE is better than both of the other embedding, for almost all types of RAGs.\\u003c/p\\u003e\\u003cp id=\\\"43df\\\"\\u003e\\u003cb\\u003e\\u003ci\\u003eE5 Vs BGE — With and without Reranker\\u003c/i\\u003e\\u003c/b\\u003e\\u003c/p\\u003e\\u003cp id=\\\"fafb\\\"\\u003eTo compare E5 and BGE, we choose to keep the baselines and the questions + keywords RAGs. More, we also apply a soft keywords approach for those, so no filtering is done before similarity search. For the reranker, we choose to get 20 documents from the similarity search, and keep the 5 best score from the reranker out of them.\\u003c/p\\u003e\\u003cfigure id=\\\"f1de\\\"\\u003e\\u003cimg src=\\\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*y6GpcLijBeEoIO5o574OVA.png\\\"\\u003e\\u003cfigcaption\\u003e\\u003ci\\u003eFig10. E5 vs BGE — With and without Reranker.\\u003c/i\\u003e\\u003c/figcaption\\u003e\\u003c/figure\\u003e\\u003cp id=\\\"8569\\\"\\u003eBGE baseline outperformed every E5 RAGs on the first document retrieval and is holding very well on the whole top 5. Furthermore, the addition of a reranker proved particularly beneficial, notably enhancing performance for BGE models with question and keywords added in the chunks headers.\\u003c/p\\u003e\\u003ch1 id=\\\"7288\\\"\\u003eConclusion\\u003c/h1\\u003e\\u003cp id=\\\"070c\\\"\\u003eIn this research, we embarked on a comprehensive exploration of leveraging Retrieval-Augmented Generation (RAG) and chunking techniques to enhance language model performance. The widespread utilization of Large Language Models (LLMs) in various applications underscores the importance of optimizing their capabilities for efficient data processing. Our study focused on three key aspects: embedding models, chunk augmentation techniques, and reranking methodologies.\\u003c/p\\u003e\\u003cp id=\\\"c939\\\"\\u003eFor embedding models, we conducted a comparative analysis of three prominent models: e5, BGE, and Solon. Each model exhibited unique characteristics and strengths, with BGE demonstrating superior performance across different chunking configurations. The evaluation of chunking techniques revealed that incorporating contextual information, such as keywords and potential questions, significantly improved document retrieval accuracy. However, it’s essential to acknowledge the inherent trade-off associated with keyword filtering, which can lead to the exclusion of relevant documents.\\u003c/p\\u003e\\u003cp id=\\\"67fe\\\"\\u003eFurthermore, our investigation into reranking mechanisms highlighted their efficacy in refining search results based on relevance to the query. The application of reranking, particularly in conjunction with BGE embeddings and augmented chunking, yielded notable improvements in document retrieval accuracy.\\u003c/p\\u003e\\u003cp id=\\\"bce3\\\"\\u003eOverall, our findings underscore the importance of considering various factors, including embedding models, chunking strategies, and reranking techniques, in optimizing LLM performance for data-driven tasks. By shedding light on the nuances of these approaches, our research provides useful insights into how they can help improve information retrieval.\\u003c/p\\u003e\\u003ch2 id=\\\"6a12\\\"\\u003eFuture researches\\u003c/h2\\u003e\\u003cp id=\\\"76d0\\\"\\u003eIn future research, we could delve into additional variations of chunking techniques to further refine our understanding of their effectiveness. For instance, exploring the potential of one-sentence chunking and embedding could provide insights into the granularity of information necessary for optimal document retrieval. Additionally, investigating the selective chunking of “potential questions”, either with or without accompanying keywords, offers an opportunity to tailor contextual information more precisely to the language model’s needs. By systematically evaluating these alternative approaches, we can gain a deeper understanding of their implications for enhancing LLM performance and information retrieval accuracy.\\u003c/p\\u003e\\u003cp id=\\\"564d\\\"\\u003eRessources: Code and Notebook available on \\u003ca href=\\\"https://gitlab.com/etalab-datalab/llm/challenge-datascience/-/tree/main/chunking_augmentation\\\"\\u003eGitLab\\u003c/a\\u003e\\u003c/p\\u003e\\u003cp id=\\\"84f3\\\"\\u003e\\u003ca href=\\\"https://www.etalab.gouv.fr\\\"\\u003e\\u003ci\\u003eCamille André — Data Scientist @ Etalab — Direction Interministérielle du Numérique\\u003c/i\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\u003c/article\\u003e\\u003c/body\\u003e\"])\n",
      "  </script>\n",
      "  <script>\n",
      "   self.__next_f.push([1,\"f:[[\\\"$\\\",\\\"nav\\\",null,{\\\"className\\\":\\\"fixed z-10 h-[54px] w-screen flex flex-grow-0 items-center justify-between bg-white dark:bg-black px-4 shadow-md dark:shadow-neutral-900\\\",\\\"children\\\":[[\\\"$\\\",\\\"$L11\\\",null,{\\\"href\\\":\\\"/\\\",\\\"className\\\":\\\"scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition\\\",\\\"children\\\":[\\\"$\\\",\\\"$L12\\\",null,{\\\"className\\\":\\\"mt-2 mb-2 h-6 w-auto sm:px-3\\\",\\\"width\\\":141,\\\"height\\\":37,\\\"src\\\":\\\"/images/logo/logo-black.webp\\\",\\\"alt\\\":\\\"Read Medium logo\\\"}]}],[\\\"$\\\",\\\"$L13\\\",null,{\\\"className\\\":\\\"px-3 flex justify-center items-center\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"flex h-full items-center shrink-0\\\",\\\"children\\\":[[\\\"$\\\",\\\"$L14\\\",null,{\\\"className\\\":\\\"sm:mr-4\\\"}],[\\\"$\\\",\\\"$L15\\\",null,{\\\"originLang\\\":\\\"en\\\",\\\"lang\\\":\\\"origin\\\",\\\"pathname\\\":\\\"enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\"}]]}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx pt-[72px] flex flex-col justify-start items-center bg-white dark:bg-black text-black dark:text-white min-h-full\\\",\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"read-medium-post max-w-[680px] max-w-full md:max-w-[680px] pb-16\\\",\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"flex justify-between items-center \\\",\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"flex justify-start items-center space-x-2\\\",\\\"children\\\":[[\\\"$\\\",\\\"img\\\",null,{\\\"src\\\":\\\"https://miro.readmedium.com/v2/resize:fill:88:88/0*RjxEMaY5JU6N_EsV.jpg\\\",\\\"alt\\\":\\\"avatar\\\",\\\"className\\\":\\\"w-11 h-11 rounded-full border border-solid border-[rgba(0,0,0,0.05)]\\\"}],[\\\"$\\\",\\\"span\\\",null,{\\\"className\\\":\\\"text-gray-950 dark:text-white text-xl max-w-[240px] md:max-w-[400px]\\\",\\\"children\\\":\\\"Andre Camille\\\"}]]}],[\\\"$\\\",\\\"$L16\\\",null,{}]]}],[\\\"$\\\",\\\"$L17\\\",null,{\\\"html\\\":\\\"$18\\\",\\\"content\\\":\\\"$19\\\",\\\"summarizelng\\\":\\\"en\\\",\\\"mediumPostId\\\":\\\"897ba15a04d6\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"$1a\\\"}}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\" text-sm font-bold mt-12 space-y-[4px]\\\",\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\\\",\\\"children\\\":\\\"Llm\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\\\",\\\"children\\\":\\\"Rags\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\\\",\\\"children\\\":\\\"NLP\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\\\",\\\"children\\\":\\\"Chunk\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\\\",\\\"children\\\":\\\"Embedding\\\"}]]}],[\\\"$\\\",\\\"$L1b\\\",null,{}]]}],[\\\"$\\\",\\\"span\\\",null,{\\\"className\\\":\\\"flex justify-center items-center w-full py-12 text-2xl divide-y border-t border-gray-300 dark:border-gray-700\\\",\\\"children\\\":\\\"Recommended from ReadMedium\\\"}],[\\\"$\\\",\\\"$L1c\\\",null,{\\\"className\\\":\\\"max-w-[680px]\\\",\\\"postUrl\\\":\\\"enhancing-language-model-performance-insights-into-rag-and-chunking-augmentation-techniques-897ba15a04d6\\\",\\\"recommendPosts\\\":[{\\\"uniqueSlug\\\":\\\"17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8\\\",\\\"title\\\":\\\"17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution\\\",\\\"subtitle\\\":\\\"A collection of RAG techniques to help you develop your RAG app into something robust that will last\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"Dominik Polzer\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/1*KqpicOFO7jh7FXGjoJ2Bcg.jpeg\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*DaXfsffAhnAzQMHH7nZ4ng.png\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":23.750943396226415,\\\"createdAt\\\":\\\"2024-06-26T19:12:54.601Z\\\",\\\"isEligibleForRevenue\\\":true},{\\\"uniqueSlug\\\":\\\"advanced-rag-retrieval-strategies-self-rag-3e9a4cd422a1\\\",\\\"title\\\":\\\"Advanced RAG Retrieval Strategies: Self-RAG\\\",\\\"subtitle\\\":\\\"Introduction to Self-RAG’s Implementation and Practical Application\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"zhaozhiming\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/1*MKcWgwWFa6EL01_JmxPAPQ.jpeg\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*4PhniGi9ow1wUyeK-sYYWg.jpeg\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":12.317924528301887,\\\"createdAt\\\":\\\"2024-07-03T20:02:54.093Z\\\",\\\"isEligibleForRevenue\\\":true},{\\\"uniqueSlug\\\":\\\"building-llm-applications-advanced-rag-part-10-ec0fe735aeb1\\\",\\\"title\\\":\\\"Building LLM Applications: Advanced RAG (Part 10)\\\",\\\"subtitle\\\":\\\"Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"Vipra Singh\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/1*LDjQS3c-G1gsojOf24ijGg@2x.jpeg\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*m4AXQjCnKe4Dz9pl2r6BaQ.png\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":47.88867924528302,\\\"createdAt\\\":\\\"2024-05-11T13:44:42.308Z\\\",\\\"isEligibleForRevenue\\\":true},{\\\"uniqueSlug\\\":\\\"the-resume-that-got-a-software-engineer-a-300-000-job-at-google-8c5a1ecff40f\\\",\\\"title\\\":\\\"The resume that got a software engineer a $300,000 job at Google.\\\",\\\"subtitle\\\":\\\"1-page. Well-formatted.\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"Alexander Nguyen\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/1*cwYWYCjbeXNc_pAtTeq_Zg.jpeg\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*-tpw_l_dLxBYJJ0JhHnf8A.png\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":3.5566037735849054,\\\"createdAt\\\":\\\"2024-06-15T08:29:38.537Z\\\",\\\"isEligibleForRevenue\\\":true},{\\\"uniqueSlug\\\":\\\"chunking-pdfs-and-multimodal-documents-efficient-methods-for-handling-text-tables-and-images-for-467472f02d34\\\",\\\"title\\\":\\\"Chunking PDFs and Multimodal Documents: Efficient Methods for Handling Text, Tables, and Images for…\\\",\\\"subtitle\\\":\\\"In this article, you will learn how to chunk documents like PDF, Word, and other multimodal documents for RAG applications.\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"Nishan Jain\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/1*TIj_sKKvGVPMjdPRCZvPMg.jpeg\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*Pi7pV1imazsAmz-qMYeumA.png\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":3.390566037735849,\\\"createdAt\\\":\\\"2024-06-10T16:46:07.281Z\\\",\\\"isEligibleForRevenue\\\":false},{\\\"uniqueSlug\\\":\\\"advanced-rag-multi-query-retriever-approach-ad8cd0ea0f5b\\\",\\\"title\\\":\\\"Advanced RAG: Multi-Query Retriever Approach\\\",\\\"subtitle\\\":\\\"A Simple Retrieval-Augmented Generation (RAG) generates final results through a two-step process. First, the query is transformed into an…\\\",\\\"authorInfo\\\":{\\\"name\\\":\\\"Kamal Dhungana\\\",\\\"avatarUrl\\\":\\\"https://miro.medium.com/v2/resize:fill:88:88/0*-DLO733P-PebUsO_\\\"},\\\"postImg\\\":\\\"https://miro.medium.com/v2/resize:fit:224/1*NKSogwLjiLPTtOp7EAynsg.png\\\",\\\"firstTag\\\":null,\\\"readingTime\\\":5.59811320754717,\\\"createdAt\\\":\\\"2024-02-16T20:17:57.811Z\\\",\\\"isEligibleForRevenue\\\":true}]}]]}]]\\n\"])\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all(\"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition\" href=\"/\"><img alt=\"Read Medium logo\" class=\"mt-2 mb-2 h-6 w-auto sm:px-3\" data-nimg=\"1\" decoding=\"async\" height=\"37\" loading=\"lazy\" src=\"/_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=384&amp;q=75\" srcset=\"/_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=256&amp;q=75 1x, /_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=384&amp;q=75 2x\" style=\"color:transparent\" width=\"141\"/></a>,\n",
       " <a class=\"a2a_button_twitter\"></a>,\n",
       " <a class=\"a2a_button_facebook\"></a>,\n",
       " <a class=\"a2a_button_linkedin\"></a>,\n",
       " <a class=\"a2a_button_wechat\"></a>,\n",
       " <a class=\"a2a_button_qzone\"></a>,\n",
       " <a href=\"https://unsplash.com/fr/@impatrickt?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\">Patrick Tomasso</a>,\n",
       " <a href=\"https://unsplash.com/fr/photos/lot-a-livre-ouvert-Oaqk7qqNh_c?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\">Unsplash</a>,\n",
       " <a href=\"https://www.service-public.fr\">service-public.fr</a>,\n",
       " <a href=\"https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json\">here</a>,\n",
       " <a href=\"https://huggingface.co/spaces/m-ric/chunk_visualizer\">https://huggingface.co/spaces/m-ric/chunk_visualizer</a>,\n",
       " <a href=\"https://huggingface.co/intfloat/multilingual-e5-large\">intfloat/e5-large</a>,\n",
       " <a href=\"https://arxiv.org/pdf/2402.05672\">https://arxiv.org/pdf/2402.05672</a>,\n",
       " <a href=\"https://huggingface.co/BAAI/bge-m3\">BAAI/bge-m3</a>,\n",
       " <a href=\"https://arxiv.org/pdf/2402.03216\">https://arxiv.org/pdf/2402.03216</a>,\n",
       " <a href=\"https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1\">OrdalieTech/Solon-embeddings-large-0.1</a>,\n",
       " <a href=\"https://huggingface.co/antoinelouis/crossencoder-electra-base-mmarcoFR\">crossencoder-electra-base-french-mmarcoFR</a>,\n",
       " <a href=\"https://huggingface.co/datasets/unicamp-dl/mmarco\">mMARCO</a>,\n",
       " <a href=\"https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.similarity_search_with_score\">similarity_search_with_score</a>,\n",
       " <a href=\"https://gitlab.com/etalab-datalab/llm/challenge-datascience/-/tree/main/chunking_augmentation\">GitLab</a>,\n",
       " <a href=\"https://www.etalab.gouv.fr\"><i>Camille André — Data Scientist @ Etalab — Direction Interministérielle du Numérique</i></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution</span><p class=\"text-[16px] font-[400] my-1\">A collection of RAG techniques to help you develop your RAG app into something robust that will last</p></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">Advanced RAG Retrieval Strategies: Self-RAG</span><p class=\"text-[16px] font-[400] my-1\">Introduction to Self-RAG’s Implementation and Practical Application</p></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">Building LLM Applications: Advanced RAG (Part 10)</span><p class=\"text-[16px] font-[400] my-1\">Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.</p></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">The resume that got a software engineer a $300,000 job at Google.</span><p class=\"text-[16px] font-[400] my-1\">1-page. Well-formatted.</p></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">Chunking PDFs and Multimodal Documents: Efficient Methods for Handling Text, Tables, and Images for…</span><p class=\"text-[16px] font-[400] my-1\">In this article, you will learn how to chunk documents like PDF, Word, and other multimodal documents for RAG applications.</p></a>,\n",
       " <a href=\"\"><span class=\"text-[20px] font-bold leading-[24px]\">Advanced RAG: Multi-Query Retriever Approach</span><p class=\"text-[16px] font-[400] my-1\">A Simple Retrieval-Augmented Generation (RAG) generates final results through a two-step process. First, the query is transformed into an…</p></a>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: https://unsplash.com/fr/@impatrickt?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash Text: Patrick Tomasso\n",
      "Link: https://unsplash.com/fr/photos/lot-a-livre-ouvert-Oaqk7qqNh_c?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash Text: Unsplash\n",
      "Link: https://www.service-public.fr Text: service-public.fr\n",
      "Link: https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json Text: here\n",
      "Link: https://huggingface.co/spaces/m-ric/chunk_visualizer Text: https://huggingface.co/spaces/m-ric/chunk_visualizer\n",
      "Link: https://huggingface.co/intfloat/multilingual-e5-large Text: intfloat/e5-large\n",
      "Link: https://arxiv.org/pdf/2402.05672 Text: https://arxiv.org/pdf/2402.05672\n",
      "Link: https://huggingface.co/BAAI/bge-m3 Text: BAAI/bge-m3\n",
      "Link: https://arxiv.org/pdf/2402.03216 Text: https://arxiv.org/pdf/2402.03216\n",
      "Link: https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1 Text: OrdalieTech/Solon-embeddings-large-0.1\n",
      "Link: https://huggingface.co/antoinelouis/crossencoder-electra-base-mmarcoFR Text: crossencoder-electra-base-french-mmarcoFR\n",
      "Link: https://huggingface.co/datasets/unicamp-dl/mmarco Text: mMARCO\n",
      "Link: https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.similarity_search_with_score Text: similarity_search_with_score\n",
      "Link: https://gitlab.com/etalab-datalab/llm/challenge-datascience/-/tree/main/chunking_augmentation Text: GitLab\n",
      "Link: https://www.etalab.gouv.fr Text: Camille André — Data Scientist @ Etalab — Direction Interministérielle du Numérique\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    link_url = link.get(\"href\")\n",
    "    if link_url is not None and link_url.startswith(\"http\"):\n",
    "        print(\"Link:\", link.get(\"href\"), \"Text:\", link.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep arxiv, most git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introducing Claude 3.5 Sonnet \\\\ AnthropicClaudeOverviewTeamAPIPricingResearchCompanyCareersNewsAnnouncementsClaude 3.5 SonnetJun 21, 2024●4 min readTry on Claude.aiToday, we’re launching Claude 3.5 Sonnet—our first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.Claude 3.5 Sonnet is now available for free on Claude.ai and the Claude iOS app, while Claude Pro and Team plan subscribers can access it with significantly higher rate limits. It is also available via the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window.Frontier intelligence at 2x the speedClaude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.In an internal agentic coding evaluation, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. Our evaluation tests the model’s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement. When instructed and provided with the relevant tools, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities. It handles code translations with ease, making it particularly effective for updating legacy applications and migrating codebases.State-of-the-art visionClaude 3.5 Sonnet is our strongest vision model yet, surpassing Claude 3 Opus on standard vision benchmarks. These step-change improvements are most noticeable for tasks that require visual reasoning, like interpreting charts and graphs. Claude 3.5 Sonnet can also accurately transcribe text from imperfect images—a core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic or illustration than from text alone.Artifacts—a new way to use ClaudeToday, we’re also introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.This preview feature marks Claude’s evolution from a conversational AI to a collaborative work environment. It’s just the beginning of a broader vision for Claude.ai, which will soon expand to support team collaboration. In the near future, teams—and eventually entire organizations—will be able to securely centralize their knowledge, documents, and ongoing work in one shared space, with Claude serving as an on-demand teammate.Commitment to safety and privacyOur models are subjected to rigorous testing and have been trained to reduce misuse. Despite Claude 3.5 Sonnet’s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at ASL-2. More details can be found in the model card addendum.As part of our commitment to safety and transparency, we’ve engaged with external experts to test and refine the safety mechanisms within this latest model. We recently provided Claude 3.5 Sonnet to the UK’s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation. The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs announced earlier this year.We have integrated policy feedback from outside subject matter experts to ensure that our evaluations are robust and take into account new trends in abuse. This engagement has helped our teams scale up our ability to evaluate 3.5 Sonnet against various types of misuse. For example, we used feedback from child safety experts at Thorn to update our classifiers and fine-tune our models.One of the core constitutional principles that guides our AI model development is privacy. We do not train our generative models on user-submitted data unless a user gives us explicit permission to do so. To date we have not used any customer or user-submitted data to train our generative models.Coming soonOur aim is to substantially improve the tradeoff curve between intelligence, speed, and cost every few months. To complete the Claude 3.5 model family, we’ll be releasing Claude 3.5 Haiku and Claude 3.5 Opus later this year.In addition to working on our next-generation model family, we are developing new modalities and features to support more use cases for businesses, including integrations with enterprise applications. Our team is also exploring features like Memory, which will enable Claude to remember a user’s preferences and interaction history as specified, making their experience even more personalized and efficient.We’re constantly working to improve Claude and love hearing from our users. You can submit feedback on Claude 3.5 Sonnet directly in-product to inform our development roadmap and help our teams to improve your experience. As always, we look forward to seeing what you build, create, and discover with Claude.ClaudeAPI TeamPricingResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"link that are the most relevant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(\"../input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_urls = {\n",
    "    \"http://www.anthropic.com/news/claude-3-5-sonnet\": \"claude_sonnet_intro.txt\",\n",
    "    \"https://www.rungalileo.io/hallucinationindex\": \"hallucination_index.txt\",\n",
    "    \"https://towardsdatascience.com/automl-with-autogluon-transform-your-ml-workflow-with-just-four-lines-of-code-1d4b593be129\": \"autogluon.txt\",\n",
    "    \"https://github.com/google-research/timesfm#readme\": \"time_series_fm.txt\",\n",
    "    \"https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-production.html?utm_source=substack&utm_medium=email\": \"deploy_llama3.txt\",\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, txt_filename in extract_urls.items():\n",
    "    req = Request(url, headers={'User-Agent': 'XYZ/3.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    soup.get_text()\n",
    "\n",
    "    with open(input_directory / txt_filename, \"w\") as txt_file:\n",
    "        txt_file.write(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
