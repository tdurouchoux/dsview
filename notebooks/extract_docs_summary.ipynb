{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py311_dsview (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py311_dsview ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "- Build graph using summary\n",
    "- extracted information (title + description): \n",
    "    - Key concepts \n",
    "    - summary \n",
    "    - Library \n",
    "    - Model \n",
    "- relation between information ?  > tag f\n",
    "- Some kind of ER, using both title and description \n",
    "\n",
    "Nodes : \n",
    "- Documents \n",
    "- Summary on hover ?\n",
    "    - Link to real document (link, doc) or embedded content ? \n",
    "    - summary of doc  \n",
    "    - link to extracted information\n",
    "- extracted information\n",
    "    - Description available on hover\n",
    "    \n",
    "    \n",
    "Obsidian functionalities : \n",
    "- Display tags \n",
    "- Display attachements (can be opened)\n",
    "\n",
    "Properties (dictionnay) / metadata\n",
    "\n",
    "groups with query \n",
    "\n",
    "\n",
    "Focus on research links for now (extension of technical knowledge ?) and extend to rest later ( work notes)\n",
    "\n",
    "Workflow : \n",
    "- add some content (web page article, pdf, mail )\n",
    "    - To be read (save indication)\n",
    "    - API ? Interface ? Watch files on icloud ? \n",
    "    - every day process it ? \n",
    "- Create a note (ex CS graph)\n",
    "\n",
    "Interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../input', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../input/deploy_llama3.txt'}, page_content='\\n\\n\\n\\n\\nHow to Install and Deploy LLaMA 3 Into Production?\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n NLP\\n                Cloud\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Platform\\n                \\n\\n\\n                        Dashboard\\n                    \\n\\n                        Playground\\n                    \\n\\n                        Documentation\\n                    \\n\\n\\n\\n                Pricing\\n            \\n\\n\\n                    Company\\n                \\n\\n\\n                        About Us\\n                    \\n\\n                        Security\\n                    \\n\\n                        Career\\n                    \\n\\n\\n\\n\\n        Use Cases\\n    \\n\\n\\n            Automatic Speech Recognition/Speech to text\\n        \\n\\n            Classification\\n        \\n\\n            Chatbot/Conversational AI\\n        \\n\\n            Code Generation\\n        \\n\\n            Dialogue Summarization\\n        \\n\\n            Embeddings\\n        \\n\\n            Generative AI (ChatGPT and GPT-4 Alternatives)\\n        \\n\\n            Grammar and Spelling Correction\\n        \\n\\n            Headline Generation\\n        \\n\\n            Image Generation/Text To Image\\n        \\n\\n            Intent Classification\\n        \\n\\n            Keyword and Keyphrase Extraction\\n        \\n\\n            Language Detection\\n        \\n\\n            Lemmatization\\n        \\n\\n            Named Entity Recognition (NER)\\n        \\n\\n            Noun Chunks\\n        \\n\\n            Paraphrasing/Rewriting\\n        \\n\\n            Part-Of-Speech tagging\\n        \\n\\n            Question Answering\\n        \\n\\n            Semantic Search\\n        \\n\\n            Semantic Similarity\\n        \\n\\n            Sentiment and Emotion Analysis\\n        \\n\\n            Speech Synthesis/Text-to-Speech\\n        \\n\\n            Summarization\\n        \\n\\n            Tokenization\\n        \\n\\n            Translation\\n        \\n\\n\\n\\n\\n        Blog\\n    \\n\\n\\n            Creating A Semantic Search Model With Sentence Transformers For A RAG Application\\n        \\n\\n            How to Install and Deploy LLaMA 3 Into Production?\\n        \\n\\n            GPT-4 and ChatGPT Open-Source Alternatives: LLaMA 3 and Mixtral 8x7b\\n        \\n\\n            How to Build a Chatbot with Generative Models like GPT-4, ChatGPT, LLaMA 3, and Mixtral 8x7b\\n        \\n\\n            Social Listening with AI: The KWatch.io Story Powered by NLP Cloud\\n        \\n\\n            The ChatGPT open-source alternatives\\n        \\n\\n            Deploy LLaMA 3, Mistral, and Mixtral, on AWS EC2 with vLLM\\n        \\n\\n            RAG: Question Answering On Domain Knowledge With Semantic Search And Generative AI\\n        \\n\\n            Edge AI / On-Premise AI Models For Sensitive Applications\\n        \\n\\n            How To Develop A Token Streaming UI For Your LLM With Go, FastAPI And JS\\n        \\n\\n            How To Fine-Tune LLaMA, OpenLLaMA, And XGen, With JAX On A GPU Or A TPU\\n        \\n\\n            OpenAI vs NLP Cloud\\n        \\n\\n            Effectively Using ChatDolphin, The ChatGPT Alternative, With Simple Instructions\\n        \\n\\n            Effectively Using GPT-3, GPT-4, ChatGPT, And More Generative Models With Few-Shot Learning\\n        \\n\\n            An Instruct Version Of GPT-J Using Stanford Alpaca\\'s Dataset\\n        \\n\\n            SpaCy Alternatives For Entity Extraction (NER)\\n        \\n\\n            How To Install And Deploy Whisper, The Best Open-Source Alternative To Google Speech-To-Text\\n        \\n\\n            Effectively Using Text To Image With Stable Diffusion, The DALL-E 2 Alternative\\n        \\n\\n            No-Code AI: Integrate the NLP Cloud API Into A Bubble.io App\\n        \\n\\n            Build a GPT-J/GPT-NeoX Discord Chatbot With NLP Cloud\\n        \\n\\n            Hugging Face API and AutoTrain: pricing and features comparison with NLP Cloud\\n        \\n\\n            How To Summarize Text With Python and Machine Learning\\n        \\n\\n            Few-shot NER: Entity Extraction Without Annotation And Training Based On GPT\\n        \\n\\n            Top 10 Natural Language Processing Frameworks, Services And Actors In 2022\\n        \\n\\n            Multilingual Natural Language Processing: NLP in non-English languages\\n        \\n\\n            Deploying GPT-NeoX 20B in production, and a focus on Deepspeed\\n        \\n\\n            How to speed up inference of Natural Language Processing Transformers\\n        \\n\\n            Fine-tuning GPT-J\\n        \\n\\n            Zero-shot learning for text classification\\n        \\n\\n            Google Cloud Natural Language VS NLP Cloud\\n        \\n\\n            Natural Language Processing introduction: what is Natural Language Processing?\\n        \\n\\n            Contextual ad targeting using text classification\\n        \\n\\n            Adding a text summarizer to Google Docs\\n        \\n\\n            Designing a classification Natural Language Processing API with FastAPI and Transformers\\n        \\n\\n            Introduction to the NLP Cloud API with the Python client\\n        \\n\\n\\n\\n                Learn\\n            \\n\\n\\n\\n\\n\\nTry for free\\n\\n\\n                        Log in\\n                    \\n\\n\\n\\nLang\\n\\n\\n\\n\\n\\n\\n\\n\\nБългарски\\nČesky\\nDánský\\nDeutsch\\nΕλληνική\\nEnglish\\nEspañol\\nEesti\\nSuomalainen\\nFrançais\\nMagyar\\nItaliano\\nやまと\\n한국어\\nLietuvių kalba\\nLatviešu\\nNorsk\\nHolandiešu\\nPolski\\nPortuguês\\nRomânesc\\nРусский\\nSlovenská\\nSlovenski\\nSvenska\\nTürkçe\\nYкраїнський\\n中国\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    How to Install and Deploy LLaMA 3 Into Production?\\n                \\nApril 23,\\n                        2024\\n\\n\\n\\n\\n\\nThe LLaMA 3 generative AI model was released by Meta a couple of days ago, and it already shows\\n                impressive capabilities.\\nLearn how to install and deploy LLaMA 3 into production with this step-by-step guide. From hardware\\n                requirements to deployment and scaling, we cover everything you need to know for a smooth\\n                implementation.\\n\\n\\n\\n\\n\\n\\n                Introduction to LLaMA 3\\n            \\nMeta has introduced initial versions of their Llama 3 open-source AI model, which can be utilized\\n                for text creation, programming, or chatbots. Furthermore, Meta announced its plans to incorporate LLaMA 3 into its primary social media applications. This\\n                move aims to compete with other AI assistants, such as OpenAI\\'s ChatGPT, Microsoft\\'s Copilot, and\\n                Google\\'s Gemini.\\nSimilar to Llama 2, Llama 3 stands out as a freely accessible large language model with open\\n                weights, offered by a leading AI company (although it doesn\\'t qualify as \"open source\" in the\\n                conventional sense).\\nCurrently, Llama 3 can be downloaded for free from Meta\\'s website in two different parameter sizes:\\n                8 billion (8B) and 70 billion (70B). Users can sign up to access these versions. Llama 3 is offered in\\n                two variants: pre-trained, which is a basic model for next token prediction, and instruction-tuned,\\n                which is fine-tuned to adhere to user commands. Both versions have a context limit of 8,192 tokens.\\nIn an interview with Dwarkesh Patel, Mark Zuckerberg, the CEO of Meta, mentioned that they trained\\n                two custom-built models using a 24,000-GPU cluster. The 70B model was trained with approximately 15\\n                trillion tokens of data, and it never reached a point of saturation or a limit to its capabilities.\\n                Afterward, Meta decided to focus on training other models. The company also revealed that they are\\n                currently working on a 400B parameter version of Llama 3, which experts like Nvidia\\'s Jim Fan believe\\n                could perform similarly to GPT-4 Turbo, Claude 3 Opus, and Gemini Ultra on benchmarks like MMLU, GPQA,\\n                HumanEval, and MATH.\\nAccording to Meta, Llama 3 has been assessed using various benchmarks, including MMLU (undergraduate\\n                level knowledge), GSM-8K (grade-school math), HumanEval (coding), GPQA (graduate-level questions), and\\n                MATH (math word problems). These benchmarks demonstrate that the 8B model outperforms open-weights\\n                models such as Google\\'s Gemma 7B and Mistral 7B Instruct, and the 70B model is competitive against\\n                Gemini Pro 1.5 and Claude 3 Sonnet.\\nMeta reports that the Llama 3 model has been improved with the ability to comprehend coding, similar\\n                to Llama 2, and for the first time, it has been trained using both images and text. However, its current\\n                output is limited to text.\\n\\nLLaMA 3 Benchmarks\\n\\n\\n\\n\\n\\n\\n                LLaMA 3 Hardware Requirements And Selecting the Right Instances on AWS EC2\\n            \\nAs many organizations use AWS for their production workloads, let\\'s see how to deploy LLaMA 3 on AWS\\n                EC2.\\nThere are multiple obstacles when it comes to implementing LLMs, such as VRAM (GPU memory)\\n                consumption, inference speed, throughput, and disk space utilization. In this scenario, we must ensure\\n                that we allocate a GPU instance on AWS EC2 with sufficient VRAM capacity to support the execution of our\\n                models.\\nLLaMA 3 8B requires around 16GB of disk space and 20GB of VRAM (GPU memory) in FP16. You could of\\n                course deploy LLaMA 3 on a CPU but the latency would be too high for a real-life production use case. As\\n                for LLaMA 3 70B, it requires around 140GB of disk space and 160GB of VRAM in FP16.\\nGetting your hands on 20GB of VRAM for LLaMA 3 8B is fairly easy. I recommend that you provision an\\n                NVIDIA A10 GPU: this GPU comes with 24GB of VRAM and it is a fast GPU based on the Ampere platform. On AWS\\n                EC2, you should select a G5 instance in order to provision an A10 GPU. A g5.xlarge will be enough.\\n            \\nDeploying the LLaMA 3 70B model is much more challenging though. No GPU has enough VRAM for this\\n                model so you will need to provision a multi-GPU instance. If you provision a g5.48xlarge instance on AWS\\n                you will get 192GB of VRAM (8 x A10 GPUs), which will be enough for LLaMA 3 70B. \\nIn such a configuration, you can expect the following latencies (response times): 50 tokens\\n                generated in 1 second for LLaMA 3 8B, and 50 tokens generated in 5 seconds for LLaMA 3 70B. \\nIn order to decrease the operating cost of these models and increase the latency, you can\\n                investigate quantization techniques\\n                but be aware that such optimizations can harm the accuracy of your model. Quantization is out of the\\n                scope of this article.\\nIn order to provision such instances, log into your AWS EC2 console, and launch a new instance:\\n                select the NVIDIA deep learning AMI, on a g5.xlarge or g5.48xlarge instance. Do not forget to provision\\n                enough disk\\n                space too.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Production Inference With vLLM\\n            \\nvLLM is a library designed for rapid and easy LLM inference and deployment. Its efficiency is\\n                attributed to various sophisticated methods, including paged attention for optimal management of\\n                attention key and value memory, real-time processing of incoming queries in batches, and personalized\\n                CUDA kernels.\\nIn addition, vLLM provides a high degree of adaptability by employing distributed computation (using\\n                tensor parallelism), real-time streaming, and compatibility with both NVIDIA and AMD graphics\\n                cards.\\nSpecifically, vLLM will greatly aid in deploying LLaMA 3, enabling us to utilize AWS EC2 instances\\n                equipped with several compact NVIDIA A10 GPUs. This is advantageous over using a single large GPU, such\\n                as the NVIDIA A100 or H100. Furthermore, vLLM will significantly enhance our model\\'s efficiency through\\n                continuous batch inference.\\nSetting up vLLM is quite simple. Let\\'s establish an SSH connection to our recently created AWS\\n                instance, and install vLLM using pip:\\n\\npip install vllm\\n\\nSince we plan to perform distributed inference using vLLM on 8 x A10 GPUs, the installation of Ray\\n                is required as well:\\n\\npip install ray\\n\\nIn case you encounter compatibility problems while installing, it may be simpler for you to compile\\n                vLLM from the source or use their Docker image: have a look at the vLLM installation instructions.\\n\\n\\n\\n\\n\\n                Launch the Inference Server\\n            \\nNow let\\'s create our Python inference script:\\n\\nfrom vllm import LLM\\n\\n# Use LLaMA 3 8B on 1 GPU\\nllm = LLM(\"meta-llama/Meta-Llama-3-8B-Instruct\")\\n\\n# Use LLaMA 3 70B on 8 GPUs\\n# llm = LLM(\"meta-llama/Meta-Llama-3-70B-Instruct\", tensor_parallel_size=8)\\n\\nprint(llm.generate(\"What are the most popular quantization techniques for LLMs?\"))\\n\\nYou can run the above script. If this is the first time you run this script, you will need to wait\\n                for the model to be downloaded and loaded on the GPU, then you will receive something like this:\\n            \\n\\nThe most popular quantization techniques for Large Language Models (LLMs) are:\\n1. Integer Quantization\\n2. Floating-Point Quantization\\n3. Mixed-Precision Training\\n4. Knowledge Distillation\\n\\nIt\\'s quite simple to understand. You simply need to adjust the tensor_parallel_size according to the\\n                number of GPUs that you possess.\\nWe are now looking to initiate an appropriate inference server capable of managing numerous requests\\n                and executing simultaneous inferences. To begin, start the server:\\n\\nFor LLaMA 3 8B:\\npython -m vllm.entrypoints.openai.api_server \\\\\\n--model meta-llama/Meta-Llama-3-8B-Instruct\\n\\nFor LLaMA 3 70B:\\npython -m vllm.entrypoints.openai.api_server \\\\\\n--model meta-llama/Meta-Llama-3-70B-Instruct\\n--tensor-parallel-size 8\\n\\nIt should take up to 1 minute for the model to load on the GPU. Then you can start a second terminal\\n                and\\n                start making some requests:\\n\\ncurl http://localhost:8000/v1/completions \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\\n    \"prompt\": \"What are the most popular quantization techniques for LLMs?\"\\n}\\'\\n\\nYou now have a proper production ready inference server that can handle many parallel requests\\n                thanks to continuous batching. At some point, if the number of requests is too high, the GPU will be\\n                overloaded though. In that case you will need to replicate the model on several GPU instances and load\\n                balance your requests (but this is out of the scope of this article).\\n\\n\\n\\n\\n\\n                Conclusion\\n            \\nAs you can see, deploying LLaMA 3 into production does not require any complex code thanks to\\n                inference servers like vLLM.\\nProvisioning the right hardware is challenging though. First because these GPUs are very costly, but\\n                also because of the current global GPU shortage. If this is your first time trying to provision a GPU server at\\n                AWS you might not have the permission to create a GPU server. In that case you will need to contact\\n                support and explain your use case. In this article we used AWS EC2 but other vendors are available of\\n                course (Azure, GCP, OVH, Scaleway...).\\nIf you\\'re not interested in deploying LLaMA 3 by yourself, we suggest utilizing our NLP Cloud\\n                API. This option can be more efficient and potentially much more cost-effective than managing your own\\n                LLaMA 3 infrastructure. Try LLaMA 3 on NLP Cloud now!\\nIf you have questions about LLaMA 3 and AI deployment in general, please don\\'t hesitate to ask us, it\\'s always a pleasure to help!\\n\\nJulien\\n                CTO at NLP Cloud\\n            \\n\\n\\n\\n\\n\\n\\nContact and support\\nPricing\\nAbout Us\\nCareer\\nLearn\\n\\n\\nOpenAI vs NLP Cloud\\nHugging Face vs NLP Cloud\\n\\n                Google Cloud Natural Language vs NLP Cloud\\n            \\n\\n\\nTerms and Conditions\\nPrivacy Policy\\nService Level Agreement\\n\\n\\nDashboard\\nPlayground\\nDocumentation\\nFAQ\\n API is Operational\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': '../input/claude_sonnet_intro.txt'}, page_content='Introducing Claude 3.5 Sonnet \\\\ AnthropicClaudeOverviewTeamAPIPricingResearchCompanyCareersNewsAnnouncementsClaude 3.5 SonnetJun 21, 2024●4 min readTry on Claude.aiToday, we’re launching Claude 3.5 Sonnet—our first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.Claude 3.5 Sonnet is now available for free on Claude.ai and the Claude iOS app, while Claude Pro and Team plan subscribers can access it with significantly higher rate limits. It is also available via the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window.Frontier intelligence at 2x the speedClaude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.In an internal agentic coding evaluation, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. Our evaluation tests the model’s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement. When instructed and provided with the relevant tools, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities. It handles code translations with ease, making it particularly effective for updating legacy applications and migrating codebases.State-of-the-art visionClaude 3.5 Sonnet is our strongest vision model yet, surpassing Claude 3 Opus on standard vision benchmarks. These step-change improvements are most noticeable for tasks that require visual reasoning, like interpreting charts and graphs. Claude 3.5 Sonnet can also accurately transcribe text from imperfect images—a core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic or illustration than from text alone.Artifacts—a new way to use ClaudeToday, we’re also introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.This preview feature marks Claude’s evolution from a conversational AI to a collaborative work environment. It’s just the beginning of a broader vision for Claude.ai, which will soon expand to support team collaboration. In the near future, teams—and eventually entire organizations—will be able to securely centralize their knowledge, documents, and ongoing work in one shared space, with Claude serving as an on-demand teammate.Commitment to safety and privacyOur models are subjected to rigorous testing and have been trained to reduce misuse. Despite Claude 3.5 Sonnet’s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at ASL-2. More details can be found in the model card addendum.As part of our commitment to safety and transparency, we’ve engaged with external experts to test and refine the safety mechanisms within this latest model. We recently provided Claude 3.5 Sonnet to the UK’s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation. The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs announced earlier this year.We have integrated policy feedback from outside subject matter experts to ensure that our evaluations are robust and take into account new trends in abuse. This engagement has helped our teams scale up our ability to evaluate 3.5 Sonnet against various types of misuse. For example, we used feedback from child safety experts at Thorn to update our classifiers and fine-tune our models.One of the core constitutional principles that guides our AI model development is privacy. We do not train our generative models on user-submitted data unless a user gives us explicit permission to do so. To date we have not used any customer or user-submitted data to train our generative models.Coming soonOur aim is to substantially improve the tradeoff curve between intelligence, speed, and cost every few months. To complete the Claude 3.5 model family, we’ll be releasing Claude 3.5 Haiku and Claude 3.5 Opus later this year.In addition to working on our next-generation model family, we are developing new modalities and features to support more use cases for businesses, including integrations with enterprise applications. Our team is also exploring features like Memory, which will enable Claude to remember a user’s preferences and interaction history as specified, making their experience even more personalized and efficient.We’re constantly working to improve Claude and love hearing from our users. You can submit feedback on Claude 3.5 Sonnet directly in-product to inform our development roadmap and help our teams to improve your experience. As always, we look forward to seeing what you build, create, and discover with Claude.ClaudeAPI TeamPricingResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC'),\n",
       " Document(metadata={'source': '../input/time_series_fm.txt'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub - google-research/timesfm: TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nBy size\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise platform\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Security\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\nPremium Support\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\nReseting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        google-research\\n \\n/\\n\\ntimesfm\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    285\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 3.4k\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n      \\n\\n\\n\\n\\n\\nresearch.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\\n\\n\\nLicense\\n\\n\\n\\n\\n\\n     Apache-2.0 license\\n    \\n\\n\\n\\n\\n\\n\\n3.4k\\n          stars\\n \\n\\n\\n\\n285\\n          forks\\n \\n\\n\\n\\nBranches\\n \\n\\n\\n\\nTags\\n \\n\\n\\n\\nActivity\\n \\n\\n\\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n50\\n\\n\\n\\n\\n\\n\\nPull requests\\n4\\n\\n\\n\\n\\n\\n\\nDiscussions\\n\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Discussions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\ngoogle-research/timesfm\\n\\n\\n\\n\\n\\n\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\xa0masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History128 Commits.github/workflows.github/workflows\\xa0\\xa0datasetsdatasets\\xa0\\xa0docsdocs\\xa0\\xa0experimentsexperiments\\xa0\\xa0notebooksnotebooks\\xa0\\xa0peftpeft\\xa0\\xa0srcsrc\\xa0\\xa0teststests\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0LICENSELICENSE\\xa0\\xa0README.mdREADME.md\\xa0\\xa0environment.ymlenvironment.yml\\xa0\\xa0environment_cpu.ymlenvironment_cpu.yml\\xa0\\xa0poetry.lockpoetry.lock\\xa0\\xa0pyproject.tomlpyproject.toml\\xa0\\xa0View all filesRepository files navigationREADMEApache-2.0 licenseTimesFM\\nTimesFM  (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google\\nResearch for time-series forecasting.\\n\\nPaper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024.\\nGoogle Research blog\\nHugging Face checkpoint repo\\n\\nThis repo contains the code to load public TimesFM checkpoints and run model\\ninference. Please visit our\\nHugging Face checkpoint repo\\nto download model checkpoints.\\nThis is not an officially supported Google product.\\nWe recommend at least 16GB RAM to load TimesFM dependencies.\\nUpdate - Aug. 6, 2024\\n\\nShoutout to @tanmayshishodia for checking in PEFT methods like LoRA and DoRA.\\nTo install TimesFM, you can now simply do: pip install timesfm.\\nLaunched finetuning support that lets you finetune the weights of the pretrained TimesFM model on your own data.\\nLaunched ~zero-shot covariate support with external regressors. More details here.\\n\\nCheckpoint timesfm-1.0-200m\\ntimesfm-1.0-200m is the first open model checkpoint:\\n\\nIt performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator.\\nIt focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\\nIt requires the context to be contiguous (i.e. no \"holes\"), and the context and the horizon to be of the same frequency.\\n\\nBenchmarks\\nPlease refer to our result tables on the extended benchmarks and the long horizon benchmarks.\\nPlease look into the README files in the respective benchmark directories within experiments/ for instructions for running TimesFM on the respective benchmarks.\\nInstallation\\nInstallation as a package\\nTo install the TimesFM as a package, you can run the following command without cloning this repo:\\npip install timesfm\\nInstallation using conda\\nFor calling TimesFM, We have two environment files. Inside timesfm, for\\nGPU installation (assuming CUDA 12 has been setup), you can create a conda\\nenvironment tfm_env from the base folder through:\\nconda env create --file=environment.yml\\n\\nFor a CPU setup please use,\\nconda env create --file=environment_cpu.yml\\n\\nto create the environment instead.\\nFollow by\\nconda activate tfm_env\\npip install -e .\\n\\nto install the package.\\nNote:\\n\\n\\nRunning the provided benchmarks would require additional dependencies.\\nPlease use the environment files under experiments instead.\\n\\n\\nThe dependency lingvo does not support ARM architectures, and the code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\\n\\n\\nLocal installation using poetry\\nTo from the current repository/local version (like you would have previously done with pip -e .), you can run the command\\npip install poetry # optional\\npoetry install\\n\\nThis will install the environment in the local .venv folder (depends on the configuration) and matches the python command to the poetry environment. If this is not the case, you can use poetry run python to use the local environment.\\nNotes\\n\\n\\nRunning the provided benchmarks would require additional dependencies.\\nPlease use the environment files under experiments instead.\\n\\n\\nThe dependency lingvo does not support ARM architectures, and the code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\\n\\n\\nBuilding the package and publishing to PyPI\\nThe package can be built using the command poetry build.\\nTo build and publish it to PyPI, the command poetry publish can be used. This command will require the user to have the necessary permissions to publish to the PyPI repository.\\nUsage\\nInitialize the model and load a checkpoint.\\nThen the base class can be loaded as,\\nimport timesfm\\n\\ntfm = timesfm.TimesFm(\\n    context_len=<context>,\\n    horizon_len=<horizon>,\\n    input_patch_len=32,\\n    output_patch_len=128,\\n    num_layers=20,\\n    model_dims=1280,\\n    backend=<backend>,\\n)\\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\\nNote that the four parameters are fixed to load the 200m model\\ninput_patch_len=32,\\noutput_patch_len=128,\\nnum_layers=20,\\nmodel_dims=1280,\\n\\n\\nThe context_len here can be set as the max context length of the model. It needs to be a multiplier of input_patch_len, i.e. a multiplier of 32. You can provide a shorter series to the tfm.forecast() function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have any context length. Padding / truncation will be handled by the inference code if needed.\\n\\n\\nThe horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\\n\\n\\nbackend is one of \"cpu\", \"gpu\" or \"tpu\", case sensitive.\\n\\n\\nPerform inference\\nWe provide APIs to forecast from either array inputs or pandas dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions tfm.forecast() and tfm.forecast_on_df() for detailed instructions.\\nIn particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\\n\\n0 (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\\n1: medium frequency time series. We recommend using this for weekly and monthly data.\\n2: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\\n\\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\\n\\n0: T, MIN, H, D, B, U\\n1: W, M\\n2: Q, Y\\n\\nNotice you do NOT have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\\nExamples:\\nArray inputs, with the frequencies set to low, medium and high respectively.\\nimport numpy as np\\nforecast_input = [\\n    np.sin(np.linspace(0, 20, 100)),\\n    np.sin(np.linspace(0, 20, 200)),\\n    np.sin(np.linspace(0, 20, 400)),\\n]\\nfrequency_input = [0, 1, 2]\\n\\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\\n    forecast_input,\\n    freq=frequency_input,\\n)\\npandas dataframe, with the frequency set to \"M\" monthly.\\nimport pandas as pd\\n\\n# e.g. input_df is\\n#       unique_id  ds          y\\n# 0     T1         1975-12-31  697458.0\\n# 1     T1         1976-01-31  1187650.0\\n# 2     T1         1976-02-29  1069690.0\\n# 3     T1         1976-03-31  1078430.0\\n# 4     T1         1976-04-30  1059910.0\\n# ...   ...        ...         ...\\n# 8175  T99        1986-01-31  602.0\\n# 8176  T99        1986-02-28  684.0\\n# 8177  T99        1986-03-31  818.0\\n# 8178  T99        1986-04-30  836.0\\n# 8179  T99        1986-05-31  878.0\\n\\nforecast_df = tfm.forecast_on_df(\\n    inputs=input_df,\\n    freq=\"M\",  # monthly\\n    value_name=\"y\",\\n    num_jobs=-1,\\n)\\nCovariates Support\\nWe now have an external regressors library on top of TimesFM that can support static covariates as well as dynamic covariates available in the future. We have an usage example in notebooks/covariates.ipynb.\\nLet\\'s take a toy example of forecasting sales for a grocery store:\\nTask: Given the observed the daily sales of this week (7 days), forecast the daily sales of next week (7 days).\\nProduct: ice cream\\nDaily_sales: [30, 30, 4, 5, 7, 8, 10]\\nCategory: food\\nBase_price: 1.99\\nWeekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]\\nHas_promotion: [Yes, Yes, No, No, No, Yes, Yes, No, No, No, No, No, No, No]\\nDaily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]\\n\\nProduct: sunscreen\\nDaily_sales: [5, 7, 12, 13, 5, 6, 10]\\nCategory: skin product\\nBase_price: 29.99\\nWeekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]\\nHas_promotion: [No, No, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes]\\nDaily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]\\n\\nIn this example, besides the Daily_sales, we also have covariates Category, Base_price, Weekday, Has_promotion, Daily_temperature. Let\\'s introduce some concepts:\\nStatic covariates are covariates for each time series.\\n\\nIn our example, Category is a static categorical covariate,\\nBase_price is a static numerical covariates.\\n\\nDynamic covariates are covaraites for each time stamps.\\n\\nDate / time related features can be usually treated as dynamic covariates.\\nIn our example, Weekday and Has_promotion are dynamic categorical covariates.\\nDaily_temperate is a dynamic numerical covariate.\\n\\nNotice: Here we make it mandatory that the dynamic covariates need to cover both the forecasting context and horizon. For example, all dynamic covariates in the example have 14 values: the first 7 correspond to the observed 7 days, and the last 7 correspond to the next 7 days.\\nWe can now provide the past data of the two products along with static and dynamic covariates as a batch input to TimesFM and produce forecasts that take into the account the covariates. To learn more, check out the example in notebooks/covariates.ipynb.\\nFinetuning\\nWe have provided an example of finetuning the model on a new dataset in notebooks/finetuning.ipynb.\\nContribution Style guide\\nIf you would like to submit a PR please make sure that you use our formatting style. We use yapf for formatting with the following options,\\n[style]\\nbased_on_style = google\\n# Add your custom style rules here\\nindent_width = 2\\nspaces_before_comment = 2\\n\\n\\nPlease run yapf --in-place --recursive <filename> on all affected files.\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n        TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n      \\n\\n\\n\\n\\n\\nresearch.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\\n\\n\\nResources\\n\\n\\n\\n\\n\\n        Readme\\n \\nLicense\\n\\n\\n\\n\\n\\n     Apache-2.0 license\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\nActivity\\n \\n\\n\\n\\n\\n\\nCustom properties\\n \\nStars\\n\\n\\n\\n\\n\\n3.4k\\n      stars\\n \\nWatchers\\n\\n\\n\\n\\n\\n33\\n      watching\\n \\nForks\\n\\n\\n\\n\\n\\n285\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n\\n\\n\\n\\n\\n\\n    Releases\\n\\nNo releases published\\n\\n\\n\\n\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Contributors\\n      10\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLanguages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n80.2%\\n\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n19.0%\\n\\n\\n\\n\\n\\n\\n\\nShell\\n0.8%\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': '../input/autogluon.txt'}, page_content=\"AutoML with AutoGluon | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inMastodonMember-only storyAutoML with AutoGluon: ML workflow with Just Four Lines of CodeHow AutoGluon Dominated Kaggle Competitions and How You Can Beat It. The algorithm that beats 99% of Data Scientists with 4 lines of code.Cristian Leo·FollowPublished inTowards Data Science·19 min read·Jul 3, 2024--3ShareImage generated by DALL-EIn two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data (AutoGluon Team. “AutoGluon: AutoML for Text, Image, and Tabular Data.” 2020)This statement, taken from the AutoGluon research paper, perfectly captures what we will explore today: a machine-learning framework that delivers impressive performance with minimal coding. You only need four lines of code to set up a complete ML pipeline, a task that could otherwise take hours. Yes, just four lines of code! See for yourself:from autogluon.tabular import TabularDataset, TabularPredictortrain_data = TabularDataset('train.csv')predictor = TabularPredictor(label='Target').fit(train_data, presets='best_quality')predictions = predictor.predict(train_data)These four lines handle data preprocessing by automatically recognizing the data type of each column, feature engineering by finding useful column combinations, and model training through ensembling to identify the best-performing model within a given…----3FollowWritten by Cristian Leo32K Followers·Writer for Towards Data ScienceA Data Scientist with a passion about recreating all the popular machine learning algorithm from scratch.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': '../input/hallucination_index.txt'}, page_content='LLM Hallucination Index RAG Special - Galileo - GalileoHomeHallucination Index 2023HomeMethodologyModel Insights claude-3-5-sonnet-20240620claude-3-haiku-20240307claude-3-opus-20240229command-r-plusgemini-1.0-progemini-1.5-flash-001gemini-1.5-pro-001gemma-7b-itgpt-3.5-turbo-0125gpt-4o-2024-05-13mistral-large-2402mistral-7b-instruct-v0.3mixtral-8x22b-instruct-v0.1mixtral-8x7b-instruct-v0.1dbrx-instructmeta-llama-3-70b-instructmeta-llama-3-8b-instructqwen2-1.5b-instructqwen2-72b-instructqwen2-7b-instructqwen1.5-32b-chatsnowflake-arctic-instructGithubGet the ReportLLM Hallucination Index RAG SpecialModel InsightsAnthropicclaude-3-5-sonnet-20240620claude-3-haiku-20240307claude-3-opus-20240229Coherecommand-r-plusGooglegemini-1.0-progemini-1.5-flash-001gemini-1.5-pro-001gemma-7b-itOpenAIgpt-3.5-turbo-0125gpt-4o-2024-05-13mistralmistral-large-2402mistral-7b-instruct-v0.3mixtral-8x22b-instruct-v0.1mixtral-8x7b-instruct-v0.1Databricksdbrx-instructMetameta-llama-3-70b-instructmeta-llama-3-8b-instructAlibabaqwen2-1.5b-instructqwen2-72b-instructqwen2-7b-instructqwen1.5-32b-chatSnowflakesnowflake-arctic-instructMethodologyGithubGet the ReportLLM Hallucination IndexRAG SPECIALBrought to you byA Ranking & Evaluation Framework For LLM HallucinationsGet The Full ReportWelcome to the Hallucination Index!The LLM landscape has changed a lot since launching our first Hallucination Index in November 2023, with larger, more powerful open and closed-sourced models being announced monthly. Since then, two things happened: the term \"hallucinate\" became Dictionary.com’s Word of the Year, and Retrieval-Augmented-Generation (RAG) has become one of the leading methods for building AI solutions. And while the parameters and context lengths of these models continue to grow, the risk of hallucinations remains.Our new Index evaluates how well 22 of the leading models adhere to given context, helping developers make informed decisions about balancing price and performance. We conducted rigorous testing of top LLMs with input ranging from 1,000 to 100,000 tokens to answer the question of how well they perform across short, medium, and long context lengths.\\xa0So let\\'s dive deep into the insights. Welcome to the new Hallucination Index - RAG Special!About the IndexWhat?Adding additional context has emerged as a new way to improve RAG performance and reduce reliability on vector databases. So, we tested each LLM across three scenarios each with varying context length.Short ContextLess than 5k tokensequivalent to RAG on few pagesMedium Context5k to 25k tokensequivalent to RAG on a book chapterLong Context40k to 100k tokensequivalent to RAG on a bookLearn more about task type selectionHow?We followed the following steps when testing each LLM:1. We gathered diverse datasets reflecting real-world scenarios across three different context lengths.2. We employed a high-performance evaluation model, called Context Adherence, to measure factual accuracy and closed-domain hallucinations - cases where the model said things that were not provided in the context data. Learn more about Context Adherence evaluation metric and the ChainPoll evaluation method.10closed-sourcemodels12open-sourcemodels3RAGtasksTrends01Open source is closing the gapWhile closed-source models still offer the best performance thanks to proprietary training data, open-source models like Gemini, Llama, and Qwen continue to improve in hallucination performance without the cost barriers of their close-sourced counterparts.02What context length?We were surprised to find models perform particularly well with extended context lengths without losing quality or accuracy, reflecting how far model training and architecture has come.03Larger is not always betterIn certain cases, smaller models outperformed larger models. For example Gemini-1.5-flash-001 outperformed larger models, which suggests that efficiency in model design can sometimes outweigh scale.04Anthropic outperforms OpenAlDuring testing, Anthropic\\'s latest Claude 3.5 Sonnet and Claude 3 Opus consistently scored close to perfect scores, beating out GPT-4o and GPT-3.5, especially in shorter context scenarios.Top Models for RAG ApplicationsOverall Winners for RAGBest performing modelClaude 3.5 Sonnet due to great performance on all tasks with context support up to 200k.Best performance for the costGemini 1.5 Flash due to great performance on all tasks with context support up to 1M.Best performing open-source modelQwen2-72B-Instruct due to great performance in SCR and MCR with context support up to 128k.CategoryProviderDescriptionScoreShort Context RAG(<5k tokens))Learn more Best closed source modelClaude 3.5 Sonnet0.97Best open source modelLlama-3-70b-chat0.95Best affordable modelGemini 1.5 Flash0.94Short Context RAG(<5k tokens))Learn more CategoryScoreBest closed source model0.97Claude 3.5 SonnetBest open source model0.95Llama-3-70b-chatBest affordable model0.94Gemini 1.5 FlashMedium Context RAG(5k to 25k tokens))Learn more Best closed source modelGemini 1.5 Flash1.00Best open source modelQwen1.5-32B-Chat0.99Best affordable modelGemini 1.5 Flash1.00Medium Context RAG(5k to 25k tokens))Learn more CategoryScoreBest closed source model1.00Gemini 1.5 FlashBest open source model0.99Qwen1.5-32B-ChatBest affordable model1.00Gemini 1.5 FlashLong Context RAG(40k to 100k tokens))Learn more Best closed source modelClaude 3.5 Sonnet1.0Best affordable modelGemini 1.5 Pro0.92Long Context RAG(40k to 100k tokens))Learn more CategoryScoreBest closed source model1.0Claude 3.5 SonnetBest affordable model0.92Gemini 1.5 ProOverall Winners for RAGOur team conducted extensive testing over a two month period, with many iterations as newer models were released. We picked three overall winners across all tests and all context lengths.Best ModelClaude 3.5 Sonnet\\n      We were extremely impressed by Anthropic’s latest set of models.\\n\\n      Not only was Sonnet able to perform excellently across short,\\n\\n      medium, and long context windows, scoring an average of 0.97, 1.0,\\n\\n      and 1.0 respectively across tasks, but the model\\'s support of up\\n\\n      to a 200k context window suggests it could support even larger\\n\\n      datasets than we tested.\\n      Best Performance for the CostGemini 1.5 Flash\\n        Gemini 1.5 Flash offered a great balance of performance and cost.\\n\\n        It earned a 0.94, 1.0, and 0.92 across short, medium, and long\\n\\n        context task types. While not as robust as other models, Gemini\\n\\n        did this at a fraction of the cost. The $ per Million prompt\\n\\n        tokens cost was $0.35 for Flash vs. $3 for Sonnet. Even more\\n\\n        starkly, the $ per Million response token cost was $1.05 for Flash\\n\\n        vs. $15 for Sonnet. For high-volume applications or use cases\\n\\n        where some margin of error is acceptable, Flash is a great choice.\\n      Best Open-Source ModelQwen2-72b-instruct\\n        A newcomer to the Index, Alibaba launched its Qwen-2 model series\\n\\n        in June 2024. The Qwen2-72b-instruct model performed on par with\\n\\n        Meta’s Llama-3-70b-instruct model during short and medium context\\n\\n        testing. What set Qwen2 apart from other open-source models was\\n\\n        its supported context length of 128K tokens. For context, the next\\n\\n        largest supported context length by an open source model was\\n\\n        Mistral’s Mixtral-8x22b model, which supports a context length of\\n\\n        64k tokens.\\n      Model Performance on RAG TasksShort Context RAG (SCR)Medium Context RAG (MCR)Long Context RAG (LCR)Short Context RAG (SCR)Less than 5k tokensThe Short Context RAG seeks to identify the most efficient model for understanding contexts up to 5k tokens. Its primary goal is to detect any loss of information or reasoning capability within these contexts. Similar to referencing select pages in a book, this method is especially suitable for tasks that demand domain-specific knowledge.FiltersPricing for open source models is based on Together.ai as of July 10Result SnapshotCategoryProviderDescriptionScoreBest closed source modelAnthropic’s Claude-3-5-SonnetClaude-3-5-Sonnet and Claude-3-opus tied with scores of 0.97 followed closely by Open AI’s GPT-4o with a score of 0.96. Sonnet was our choice as it achieved this performance at a lower cost than its close competitors. 0.97Worst closed-source modelCohere\\'s Command-r-plusCohere\\'s Command-r-plus earned a 0.86 during short context testing. While this was better than Open AI’s GPT-3.5-Turbo and Google’s Gemini which both scored a 0.84, Cohere’s price was on par with Anthropic’s Claude 3.5 Sonnet at $3/M prompt tokens and $15/M response tokens, making it our choice for the worst closed-source model.0.86Best open-source modelMeta\\'s llama-3-70b-instructAlibaba\\'s qwen2-72b-instruct and Meta\\'s llama-3-70b-instruct tied during testing with scores of 0.95. We chose llama-3-70b-instruct, as qwen2-72b-instruct has 60% longer responses, which could increase the cost.0.95Worst open-source modelGoogle’s Gemma-7b-itGoogle\\'s Gemma-7b-it demonstrated the poorest performance within the 7-billion-parameter category.0.65Best performance for the costGoogle’s Gemini-1.5-flash-001As mentioned in our overall results, Gemini-1.5-flash-001 performed the best at a fraction of the cost of models like Claude-3.5-sonnet, making it our choice for this category.0.94Best small open modelMeta\\'s llama-3-8b-instructMeta\\'s llama-3-8b-instruct surpassed several recent large models like Snowflake\\'s Arctic.0.89RAG Task ExamplesModelLlama-3-70b-chatIs response correct?YesContext Adherence1promptFor every 100 females age 18 and over, there were 94.5 males.Show fullresponseBased on the information, females are the larger gender group. For every 100 females, there were 97.3 males, and for every 100 females age 18 and over, there were 94.5 males. This indicates that there are more females than males in the population.Answer: FemaleModelGPT 3.5 turboIs response correct?NoContext Adherence0promptFor every 100 females age 18 and over, there were 94.5 males.Show fullresponseMaleAnswer: FemaleMedium Context RAG (MCR)5k to 25k tokensThe Medium Context RAG aims to determine the most effective model for comprehending long contexts spanning from 5k to 25k tokens. It focuses on identifying any loss of information and reasoning ability within these extensive contexts. Additionally, we experiment with a prompting technique known as Chain-of-Note to improve performance as it has worked for short context. This task is akin to doing RAG on a few book chapters.FiltersPricing for open source models is based on Together.ai as of July 10Result SnapshotCategoryProviderDescriptionScoreBest closed-source modelGoogle’s Gemini-1.5-flash-001During testing, many models scored a perfect 1.0 score: \\n\\n        • Anthropic: claude-3-5-sonnet, claude-3-opus,\\n• Cohere: command-r-plus \\n• Google: gemini-1.5-flash-001, gemini-1.5-pro-001 \\n• Mistral: mistral-large \\n• OpenAI: gpt-4o-2024-05-13\\n\\n    We ultimately chose Gemini-1.5-flash-001 for its low cost.\\n 1.00Worst closed-source modelAnthropic’s Claude-3-HaikuClaude-3-haiku performed the worst but still scored a 0.96.0.96Best open-source modelAlibaba\\'s qwen2-72b-instructAlibaba\\'s qwen2-72b-instruct scored a perfect 1.0 and had flawless performance up to 25k tokens. Note that llama-3-70b-instruct does not support beyond 8K context length.1.00Worst open-source modelMistral-7b-instruct-v0.3Mistral-7b-instruct-v0.3 had good performance however when considering cost, we felt there were better options.0.94Best performance for the costGoogle’s Gemini-1.5-flash-001Gemini-1.5-flash-001 scored a perfect 1.0. Similar to our rationale for when evaluating models for small context testing, Gemini-1.5-flash-001 also performed the best at a fraction of the cost, making it our choice for this category.1.00Best small open modelAlibaba\\'s qwen2-7b-instructAlibaba\\'s qwen2-7b-instruct scored the best among the 7b models.0.96Recall heatmapThis heatmap shows the model\\'s ability to recall information in different parts of the context. The x-axis represents the length of the context during the experiment, and the y-axis represents the location of the information. Green indicates successful recall, while red indicates failure.claude-3-5-sonnet-20240620SimpleHeatmap for claude-3-5-sonnet-20240620RAG Task ExamplesModelGemini FlashIs response correct?YesContext Adherence1promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe answer is **2.7%**. \\n\\n\\nThis information is found in **Document 2**, which states: \"In fiscal year 2024, our overall turnover rate was 2.7%.Answer: 2.7%ModelMistral-7B-Instruct-v0.3Is response correct?NoContext Adherence0promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe documents do not provide the information about the percentage of employees who left the company during FY 2024.Answer: 2.7%Long Context RAG (LCR)40k to 100k tokensThis heatmap shows the model\\'s ability to recall information in different parts of the context. The x-axis represents the length of the context during the experiment, and the y-axis represents the location of the information. Green indicates successful recall, while red indicates failure.FiltersResult SnapshotCategoryProviderDescriptionScoreBest closed-source modelAnthropic’s Claude-3-5-SonnetDuring long context testing, many models scored a perfect 1.0 score, highlighting the power and quality of models from top-tier providers. Among those scoring a perfect 1.0 were:\\n\\n    • Anthropic: claude-3-5-sonnet, claude-3-opus\\n• Google: gemini-1.5-pro-001\\n• Open AI: gpt-4o\\n\\n    Between these models, it came down to Claude-3-5-Sonnet and Gemini-1.5-pro-001, which were both comparable on performance and price. Ultimately we chose Claude-3-5-Sonnet due to its better performance across short and medium-context testing.\\n 1.00Worst closed-source modelAnthropic’s Claude-3-HaikuClaude-3-haiku performed the worst with a score of 0.7. During testing, the model struggled to maintain accuracy with context lengths greater than 60k tokens.0.70Best performance for the costGoogle’s Gemini-1.5-flash-001Google\\'s gemini-1.5-flash-001 scored an impressive 0.92 and only faced issues when provided with 80000 token context length.0.92Recall heatmapAs the name suggests, this heatmap indicates the model\\'s success in recalling information at different locations in the context. Green signifies success, while red indicates failure.claude-3-5-sonnet-20240620SimpleHeatmap for claude-3-5-sonnet-20240620RAG Task ExamplesModelClaude 3.5 SonnetIs response correct?YesContext Adherence1promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseAccording to the document, NVIDIA\\'s overall employee turnover rate in fiscal year 2024 was 2.7%.Answer: 2.7%ModelCommand R PlusIs response correct?NoContext Adherence0promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe overall turnover rate was 5.3%.Answer: 2.7%Model comparisonDatasetclaude-3-5-sonnet-20240620claude-3-haiku-20240307Model InfoDeveloperAnthropicAnthropicLicenseNA (private model)NA (private model)Model ParametersNA (private model)NA (private model)Supported context length200k200k$/M prompt tokens30.25$/M response tokens151.25Cost insightPrompt cost compared to Claude 3.5 Sonnet1.00x0.08xPrompt cost compared to GPT-4o0.60x0.05xPrompt cost compared to Gemini 1.5 Pro0.86x0.07xPrompt cost compared to Llama-3-70b3.33x0.28xPerformanceShort context RAG0.970.92Medium context RAG1.000.96Long context RAG1.000.70Task InsightsSCR insightThe model demonstrates exceptional reasoning and comprehension skills, excelling at short context RAG. It outperforms other models in mathematical proficiency, as evidenced by its strong performance on DROP and ConvFinQA benchmarks. This makes it the most affordable top tier model for RAG.The model demonstrates exceptional reasoning and comprehension skills, excelling at short context RAG. It shows good mathematical proficiency, as evidenced by its performance on DROP and ConvFinQA benchmarks. It comes out as one of the best small closed source model.MCR insightFlawless performance making it suitable for any context length upto 25000 tokens.Great powerformance overall with minor problems for context more than 10000 tokens.LCR insightFlawless performance making it suitable for any context length upto 100000 tokens.Model shows issues at all context lengths and shows poor performance after 60000 making it unsitable for long context use.Performance on datasetsHere is the performance of models on each dataset. The datasets are selected to test for different capabilities ranging from robustness to noise to the ability to do math.claude-3-5-sonnet-20240620 Read the full reportLLM Hallucination IndexHomeMethodology')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarization_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mYou are part of a knowledge management system, your role is to extract information regarding Data Science news, articles and courses.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThe source could be about mathematics, Python, Machine Learning, Large Language Model, Data engineering or any related subject.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mTo perform this task, please create a concise summary of the following source. The summary should quickly extract the theme and context,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mand describe the key concepts and takeaways.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mData Science information source :\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m{input}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m summary_generator \u001b[38;5;241m=\u001b[39m summarization_prompt \u001b[38;5;241m|\u001b[39m llm\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "summarization_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are part of a knowledge management system, your role is to extract information regarding Data Science news, articles and courses.\n",
    "The source could be about mathematics, Python, Machine Learning, Large Language Model, Data engineering or any related subject.\n",
    "To perform this task, please create a concise summary of the following source. The summary should quickly extract the theme and context,\n",
    "and describe the key concepts and takeaways.\n",
    "\n",
    "Data Science information source :\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "summary_generator = summarization_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_generator.invoke({\"input\": docs[0].page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: How to Install and Deploy LLaMA 3 Into Production**\n",
    "\n",
    "The article provides a comprehensive guide on installing and deploying Meta's LLaMA 3, a generative AI model, into production. Released recently, LLaMA 3 is designed for various applications, including text generation, programming, and chatbots, and is available in two sizes: 8 billion and 70 billion parameters. The model is notable for its open weights and competitive performance against other AI models like OpenAI's ChatGPT and Google's Gemini.\n",
    "\n",
    "**Key Concepts and Takeaways:**\n",
    "\n",
    "1. **Model Overview**: LLaMA 3 is an open-source-like model with two variants: pre-trained for basic tasks and instruction-tuned for user commands. It has a context limit of 8,192 tokens and has been trained on a vast dataset using a 24,000-GPU cluster.\n",
    "\n",
    "2. **Hardware Requirements**: \n",
    "   - LLaMA 3 (8B) requires 20GB of VRAM and 16GB of disk space.\n",
    "   - LLaMA 3 (70B) necessitates 160GB of VRAM and 140GB of disk space, requiring a multi-GPU setup for deployment.\n",
    "\n",
    "3. **Deployment on AWS EC2**: The article details how to provision AWS EC2 instances, recommending the use of NVIDIA A10 GPUs for the 8B model and a g5.48xlarge instance for the 70B model.\n",
    "\n",
    "4. **Inference with vLLM**: vLLM is highlighted as a library that facilitates efficient LLM inference and deployment. It supports distributed computation and real-time processing, making it suitable for handling multiple requests.\n",
    "\n",
    "5. **Setting Up the Inference Server**: The article provides a step-by-step process for setting up an inference server using Python scripts, allowing users to generate responses from the LLaMA 3 model.\n",
    "\n",
    "6. **Conclusion**: While deploying LLaMA 3 is straightforward with tools like vLLM, challenges include hardware costs and availability. For those not wanting to manage infrastructure, using the NLP Cloud API is suggested as a more efficient alternative.\n",
    "\n",
    "Overall, the article serves as a practical resource for data scientists and engineers looking to implement LLaMA 3 in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_generator.invoke({\"input\": docs[1].page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Claude 3.5 Sonnet Release**\n",
    "\n",
    "On June 21, 2024, Anthropic announced the launch of Claude 3.5 Sonnet, the first model in the Claude 3.5 family, which significantly enhances AI capabilities in reasoning, knowledge, and coding proficiency. This model outperforms its predecessor, Claude 3 Opus, and other competitors in various evaluations, achieving faster processing speeds and cost-effective pricing. Claude 3.5 Sonnet is available for free on Claude.ai and the Claude iOS app, with premium access for subscribers.\n",
    "\n",
    "**Key Features:**\n",
    "- **Performance:** Claude 3.5 Sonnet excels in graduate-level reasoning, undergraduate knowledge, and coding tasks, solving 64% of coding problems compared to 38% by Claude 3 Opus.\n",
    "- **Vision Capabilities:** It surpasses previous models in visual reasoning tasks, such as interpreting charts and transcribing text from images, making it valuable for industries like retail and finance.\n",
    "- **Artifacts Feature:** A new interactive feature allows users to generate and edit content in real-time, transforming Claude from a conversational AI into a collaborative workspace.\n",
    "- **Safety and Privacy:** The model has undergone rigorous safety evaluations and maintains a commitment to user privacy, not using customer data for training without explicit consent.\n",
    "\n",
    "**Future Developments:**\n",
    "Anthropic plans to release additional models in the Claude 3.5 family and is exploring new features, including Memory for personalized user experiences. The company encourages user feedback to guide future improvements.\n",
    "\n",
    "Overall, Claude 3.5 Sonnet represents a significant advancement in AI technology, focusing on enhancing user interaction, safety, and performance across various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_generator.invoke({\"input\": docs[2].page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of TimesFM (Time Series Foundation Model)**\n",
    "\n",
    "**Theme and Context:**\n",
    "TimesFM is a pretrained time-series foundation model developed by Google Research, designed specifically for time-series forecasting. It aims to enhance the accuracy and efficiency of forecasting tasks by leveraging advanced machine learning techniques.\n",
    "\n",
    "**Key Concepts and Takeaways:**\n",
    "\n",
    "1. **Model Overview:**\n",
    "   - TimesFM is a decoder-only foundation model tailored for time-series data, capable of handling univariate forecasting with context lengths up to 512 timepoints.\n",
    "   - It focuses on point forecasts and does not support probabilistic forecasts, although experimental quantile heads are available.\n",
    "\n",
    "2. **Installation and Usage:**\n",
    "   - Users can install TimesFM via pip (`pip install timesfm`) or conda, with specific environment files provided for GPU and CPU setups.\n",
    "   - The model can be initialized and used for forecasting with simple API calls, allowing for both array and pandas DataFrame inputs.\n",
    "\n",
    "3. **Covariate Support:**\n",
    "   - TimesFM supports both static and dynamic covariates, enabling users to incorporate additional contextual information (e.g., promotions, temperature) into their forecasts.\n",
    "   - Dynamic covariates must cover both the forecasting context and horizon.\n",
    "\n",
    "4. **Finetuning:**\n",
    "   - The model allows for finetuning on custom datasets, providing flexibility for users to adapt the model to specific forecasting needs.\n",
    "\n",
    "5. **Benchmarks and Performance:**\n",
    "   - The model has been benchmarked against various datasets, with results available in the repository for users to assess its performance.\n",
    "\n",
    "6. **Documentation and Community:**\n",
    "   - Comprehensive documentation is provided, including examples and guidelines for installation, usage, and contribution.\n",
    "   - The project is open-source, encouraging community contributions and collaboration.\n",
    "\n",
    "Overall, TimesFM represents a significant advancement in time-series forecasting, offering a robust tool for data scientists and researchers in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm specialized on Data Science related subject. \"\n",
    "            \"Only extract relevant information from the text.\"\n",
    "            #  The extracted information should be the minimum amount necessary to get a full understanding of the provided source.#\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_tags = [\n",
    "    \"Machine Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"AutoML\",\n",
    "    \"Cloud Computing\", \n",
    "    \"Statistics\",\n",
    "    \"Mathematics\",\n",
    "    \"Model Deployment\",\n",
    "    \"MlOps\",\n",
    "    \"Data Engineering\",\n",
    "    \"Data Visualization\",\n",
    "    \"DevOps\",\n",
    "    \"Python\",\n",
    "    \"Large Language Model\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Generative AI\",\n",
    "    \"Time Series\",\n",
    "    \"Graph (networks)\",\n",
    "    \"Computer Vision\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Semi-supervised Learning\",\n",
    "    \"Active Learning\",\n",
    "    \"AI regulation\",\n",
    "    \"Data Quality\",\n",
    "    \"Model evaluation\"\n",
    "]\n",
    "\n",
    "data_science_concepts = [\n",
    "    \"Organization\",\n",
    "    \"AI Company\",\n",
    "    \"Model\",\n",
    "    \"Python library\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScienceSubject(BaseModel):\n",
    "    type: str = Field(\n",
    "        default=None,\n",
    "        description=\"Type of the described subject in the source.\", \n",
    "        enum=data_science_concepts,\n",
    "    )\n",
    "    name: str = Field(\n",
    "        default=None,\n",
    "        description=\"Name of the described subject in the source.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"General description of the subject, it should not \"\n",
    "            \"be a description of the source or how the source \"\n",
    "            \"tackle this subject. But the description must be \"\n",
    "            \"created using the provided context or any prior knowledge.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class DataScienceTag(BaseModel):\n",
    "    name: str = Field(\n",
    "        default=None,\n",
    "        description=\"Data science topic discussed in the source, it should be as precise as possible\",\n",
    "        enum=data_science_tags\n",
    "    )\n",
    "    \n",
    "class TopicExtraction(BaseModel):\n",
    "    # summary: str = Field(description=\"A summary of the source, organized in 2 parts. First a quick overview of source, then a bullet point list of key concepts and takeaways.\")\n",
    "    tags: List[DataScienceTag]\n",
    "    subjects: List[DataScienceSubject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_extraction = topic_prompt | llm.with_structured_output(schema=TopicExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_extraction.invoke({\"text\": docs[4].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../input/time_series_fm.txt'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub - google-research/timesfm: TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nBy size\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise platform\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Security\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\nPremium Support\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\nReseting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        google-research\\n \\n/\\n\\ntimesfm\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    285\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 3.4k\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n      \\n\\n\\n\\n\\n\\nresearch.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\\n\\n\\nLicense\\n\\n\\n\\n\\n\\n     Apache-2.0 license\\n    \\n\\n\\n\\n\\n\\n\\n3.4k\\n          stars\\n \\n\\n\\n\\n285\\n          forks\\n \\n\\n\\n\\nBranches\\n \\n\\n\\n\\nTags\\n \\n\\n\\n\\nActivity\\n \\n\\n\\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n50\\n\\n\\n\\n\\n\\n\\nPull requests\\n4\\n\\n\\n\\n\\n\\n\\nDiscussions\\n\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Discussions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\ngoogle-research/timesfm\\n\\n\\n\\n\\n\\n\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\xa0masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History128 Commits.github/workflows.github/workflows\\xa0\\xa0datasetsdatasets\\xa0\\xa0docsdocs\\xa0\\xa0experimentsexperiments\\xa0\\xa0notebooksnotebooks\\xa0\\xa0peftpeft\\xa0\\xa0srcsrc\\xa0\\xa0teststests\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0LICENSELICENSE\\xa0\\xa0README.mdREADME.md\\xa0\\xa0environment.ymlenvironment.yml\\xa0\\xa0environment_cpu.ymlenvironment_cpu.yml\\xa0\\xa0poetry.lockpoetry.lock\\xa0\\xa0pyproject.tomlpyproject.toml\\xa0\\xa0View all filesRepository files navigationREADMEApache-2.0 licenseTimesFM\\nTimesFM  (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google\\nResearch for time-series forecasting.\\n\\nPaper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024.\\nGoogle Research blog\\nHugging Face checkpoint repo\\n\\nThis repo contains the code to load public TimesFM checkpoints and run model\\ninference. Please visit our\\nHugging Face checkpoint repo\\nto download model checkpoints.\\nThis is not an officially supported Google product.\\nWe recommend at least 16GB RAM to load TimesFM dependencies.\\nUpdate - Aug. 6, 2024\\n\\nShoutout to @tanmayshishodia for checking in PEFT methods like LoRA and DoRA.\\nTo install TimesFM, you can now simply do: pip install timesfm.\\nLaunched finetuning support that lets you finetune the weights of the pretrained TimesFM model on your own data.\\nLaunched ~zero-shot covariate support with external regressors. More details here.\\n\\nCheckpoint timesfm-1.0-200m\\ntimesfm-1.0-200m is the first open model checkpoint:\\n\\nIt performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator.\\nIt focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\\nIt requires the context to be contiguous (i.e. no \"holes\"), and the context and the horizon to be of the same frequency.\\n\\nBenchmarks\\nPlease refer to our result tables on the extended benchmarks and the long horizon benchmarks.\\nPlease look into the README files in the respective benchmark directories within experiments/ for instructions for running TimesFM on the respective benchmarks.\\nInstallation\\nInstallation as a package\\nTo install the TimesFM as a package, you can run the following command without cloning this repo:\\npip install timesfm\\nInstallation using conda\\nFor calling TimesFM, We have two environment files. Inside timesfm, for\\nGPU installation (assuming CUDA 12 has been setup), you can create a conda\\nenvironment tfm_env from the base folder through:\\nconda env create --file=environment.yml\\n\\nFor a CPU setup please use,\\nconda env create --file=environment_cpu.yml\\n\\nto create the environment instead.\\nFollow by\\nconda activate tfm_env\\npip install -e .\\n\\nto install the package.\\nNote:\\n\\n\\nRunning the provided benchmarks would require additional dependencies.\\nPlease use the environment files under experiments instead.\\n\\n\\nThe dependency lingvo does not support ARM architectures, and the code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\\n\\n\\nLocal installation using poetry\\nTo from the current repository/local version (like you would have previously done with pip -e .), you can run the command\\npip install poetry # optional\\npoetry install\\n\\nThis will install the environment in the local .venv folder (depends on the configuration) and matches the python command to the poetry environment. If this is not the case, you can use poetry run python to use the local environment.\\nNotes\\n\\n\\nRunning the provided benchmarks would require additional dependencies.\\nPlease use the environment files under experiments instead.\\n\\n\\nThe dependency lingvo does not support ARM architectures, and the code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\\n\\n\\nBuilding the package and publishing to PyPI\\nThe package can be built using the command poetry build.\\nTo build and publish it to PyPI, the command poetry publish can be used. This command will require the user to have the necessary permissions to publish to the PyPI repository.\\nUsage\\nInitialize the model and load a checkpoint.\\nThen the base class can be loaded as,\\nimport timesfm\\n\\ntfm = timesfm.TimesFm(\\n    context_len=<context>,\\n    horizon_len=<horizon>,\\n    input_patch_len=32,\\n    output_patch_len=128,\\n    num_layers=20,\\n    model_dims=1280,\\n    backend=<backend>,\\n)\\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\\nNote that the four parameters are fixed to load the 200m model\\ninput_patch_len=32,\\noutput_patch_len=128,\\nnum_layers=20,\\nmodel_dims=1280,\\n\\n\\nThe context_len here can be set as the max context length of the model. It needs to be a multiplier of input_patch_len, i.e. a multiplier of 32. You can provide a shorter series to the tfm.forecast() function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have any context length. Padding / truncation will be handled by the inference code if needed.\\n\\n\\nThe horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\\n\\n\\nbackend is one of \"cpu\", \"gpu\" or \"tpu\", case sensitive.\\n\\n\\nPerform inference\\nWe provide APIs to forecast from either array inputs or pandas dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions tfm.forecast() and tfm.forecast_on_df() for detailed instructions.\\nIn particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\\n\\n0 (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\\n1: medium frequency time series. We recommend using this for weekly and monthly data.\\n2: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\\n\\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\\n\\n0: T, MIN, H, D, B, U\\n1: W, M\\n2: Q, Y\\n\\nNotice you do NOT have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\\nExamples:\\nArray inputs, with the frequencies set to low, medium and high respectively.\\nimport numpy as np\\nforecast_input = [\\n    np.sin(np.linspace(0, 20, 100)),\\n    np.sin(np.linspace(0, 20, 200)),\\n    np.sin(np.linspace(0, 20, 400)),\\n]\\nfrequency_input = [0, 1, 2]\\n\\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\\n    forecast_input,\\n    freq=frequency_input,\\n)\\npandas dataframe, with the frequency set to \"M\" monthly.\\nimport pandas as pd\\n\\n# e.g. input_df is\\n#       unique_id  ds          y\\n# 0     T1         1975-12-31  697458.0\\n# 1     T1         1976-01-31  1187650.0\\n# 2     T1         1976-02-29  1069690.0\\n# 3     T1         1976-03-31  1078430.0\\n# 4     T1         1976-04-30  1059910.0\\n# ...   ...        ...         ...\\n# 8175  T99        1986-01-31  602.0\\n# 8176  T99        1986-02-28  684.0\\n# 8177  T99        1986-03-31  818.0\\n# 8178  T99        1986-04-30  836.0\\n# 8179  T99        1986-05-31  878.0\\n\\nforecast_df = tfm.forecast_on_df(\\n    inputs=input_df,\\n    freq=\"M\",  # monthly\\n    value_name=\"y\",\\n    num_jobs=-1,\\n)\\nCovariates Support\\nWe now have an external regressors library on top of TimesFM that can support static covariates as well as dynamic covariates available in the future. We have an usage example in notebooks/covariates.ipynb.\\nLet\\'s take a toy example of forecasting sales for a grocery store:\\nTask: Given the observed the daily sales of this week (7 days), forecast the daily sales of next week (7 days).\\nProduct: ice cream\\nDaily_sales: [30, 30, 4, 5, 7, 8, 10]\\nCategory: food\\nBase_price: 1.99\\nWeekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]\\nHas_promotion: [Yes, Yes, No, No, No, Yes, Yes, No, No, No, No, No, No, No]\\nDaily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]\\n\\nProduct: sunscreen\\nDaily_sales: [5, 7, 12, 13, 5, 6, 10]\\nCategory: skin product\\nBase_price: 29.99\\nWeekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]\\nHas_promotion: [No, No, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes]\\nDaily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]\\n\\nIn this example, besides the Daily_sales, we also have covariates Category, Base_price, Weekday, Has_promotion, Daily_temperature. Let\\'s introduce some concepts:\\nStatic covariates are covariates for each time series.\\n\\nIn our example, Category is a static categorical covariate,\\nBase_price is a static numerical covariates.\\n\\nDynamic covariates are covaraites for each time stamps.\\n\\nDate / time related features can be usually treated as dynamic covariates.\\nIn our example, Weekday and Has_promotion are dynamic categorical covariates.\\nDaily_temperate is a dynamic numerical covariate.\\n\\nNotice: Here we make it mandatory that the dynamic covariates need to cover both the forecasting context and horizon. For example, all dynamic covariates in the example have 14 values: the first 7 correspond to the observed 7 days, and the last 7 correspond to the next 7 days.\\nWe can now provide the past data of the two products along with static and dynamic covariates as a batch input to TimesFM and produce forecasts that take into the account the covariates. To learn more, check out the example in notebooks/covariates.ipynb.\\nFinetuning\\nWe have provided an example of finetuning the model on a new dataset in notebooks/finetuning.ipynb.\\nContribution Style guide\\nIf you would like to submit a PR please make sure that you use our formatting style. We use yapf for formatting with the following options,\\n[style]\\nbased_on_style = google\\n# Add your custom style rules here\\nindent_width = 2\\nspaces_before_comment = 2\\n\\n\\nPlease run yapf --in-place --recursive <filename> on all affected files.\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n        TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\\n      \\n\\n\\n\\n\\n\\nresearch.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\\n\\n\\nResources\\n\\n\\n\\n\\n\\n        Readme\\n \\nLicense\\n\\n\\n\\n\\n\\n     Apache-2.0 license\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\nActivity\\n \\n\\n\\n\\n\\n\\nCustom properties\\n \\nStars\\n\\n\\n\\n\\n\\n3.4k\\n      stars\\n \\nWatchers\\n\\n\\n\\n\\n\\n33\\n      watching\\n \\nForks\\n\\n\\n\\n\\n\\n285\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n\\n\\n\\n\\n\\n\\n    Releases\\n\\nNo releases published\\n\\n\\n\\n\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Contributors\\n      10\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLanguages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n80.2%\\n\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n19.0%\\n\\n\\n\\n\\n\\n\\n\\nShell\\n0.8%\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataScienceTag(name='Large Language Model'),\n",
       " DataScienceTag(name='Model Deployment'),\n",
       " DataScienceTag(name='AI regulation')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataScienceSubject(type='Model', name='Claude 3.5 Sonnet', description='A model developed by Anthropic, known for its high performance across various context lengths.'),\n",
       " DataScienceSubject(type='Model', name='Gemini 1.5 Flash', description='A cost-effective model that balances performance and affordability, excelling in RAG tasks.'),\n",
       " DataScienceSubject(type='Model', name='Qwen2-72B-Instruct', description='An open-source model from Alibaba that performs well in RAG tasks with a high context length.'),\n",
       " DataScienceSubject(type='Model', name='GPT-4o', description='A model developed by OpenAI, known for its strong performance but higher cost.'),\n",
       " DataScienceSubject(type='Model', name='Llama-3-70b-instruct', description='An open-source model from Meta that performs well in RAG tasks but has limitations in context length.'),\n",
       " DataScienceSubject(type='Model', name='Mistral-7b-instruct-v0.3', description='An open-source model that shows good performance but is less cost-effective compared to others.')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API : CLI \n",
    "    # please ingest URL with following information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_graph_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
