LLM Hallucination Index RAG Special - Galileo - GalileoHomeHallucination Index 2023HomeMethodologyModel Insights claude-3-5-sonnet-20240620claude-3-haiku-20240307claude-3-opus-20240229command-r-plusgemini-1.0-progemini-1.5-flash-001gemini-1.5-pro-001gemma-7b-itgpt-3.5-turbo-0125gpt-4o-2024-05-13mistral-large-2402mistral-7b-instruct-v0.3mixtral-8x22b-instruct-v0.1mixtral-8x7b-instruct-v0.1dbrx-instructmeta-llama-3-70b-instructmeta-llama-3-8b-instructqwen2-1.5b-instructqwen2-72b-instructqwen2-7b-instructqwen1.5-32b-chatsnowflake-arctic-instructGithubGet the ReportLLM Hallucination Index RAG SpecialModel InsightsAnthropicclaude-3-5-sonnet-20240620claude-3-haiku-20240307claude-3-opus-20240229Coherecommand-r-plusGooglegemini-1.0-progemini-1.5-flash-001gemini-1.5-pro-001gemma-7b-itOpenAIgpt-3.5-turbo-0125gpt-4o-2024-05-13mistralmistral-large-2402mistral-7b-instruct-v0.3mixtral-8x22b-instruct-v0.1mixtral-8x7b-instruct-v0.1Databricksdbrx-instructMetameta-llama-3-70b-instructmeta-llama-3-8b-instructAlibabaqwen2-1.5b-instructqwen2-72b-instructqwen2-7b-instructqwen1.5-32b-chatSnowflakesnowflake-arctic-instructMethodologyGithubGet the ReportLLM Hallucination IndexRAG SPECIALBrought to you byA Ranking & Evaluation Framework For LLM HallucinationsGet The Full ReportWelcome to the Hallucination Index!The LLM landscape has changed a lot since launching our first Hallucination Index in November 2023, with larger, more powerful open and closed-sourced models being announced monthly. Since then, two things happened: the term "hallucinate" became Dictionary.com’s Word of the Year, and Retrieval-Augmented-Generation (RAG) has become one of the leading methods for building AI solutions. And while the parameters and context lengths of these models continue to grow, the risk of hallucinations remains.Our new Index evaluates how well 22 of the leading models adhere to given context, helping developers make informed decisions about balancing price and performance. We conducted rigorous testing of top LLMs with input ranging from 1,000 to 100,000 tokens to answer the question of how well they perform across short, medium, and long context lengths. So let's dive deep into the insights. Welcome to the new Hallucination Index - RAG Special!About the IndexWhat?Adding additional context has emerged as a new way to improve RAG performance and reduce reliability on vector databases. So, we tested each LLM across three scenarios each with varying context length.Short ContextLess than 5k tokensequivalent to RAG on few pagesMedium Context5k to 25k tokensequivalent to RAG on a book chapterLong Context40k to 100k tokensequivalent to RAG on a bookLearn more about task type selectionHow?We followed the following steps when testing each LLM:1. We gathered diverse datasets reflecting real-world scenarios across three different context lengths.2. We employed a high-performance evaluation model, called Context Adherence, to measure factual accuracy and closed-domain hallucinations - cases where the model said things that were not provided in the context data. Learn more about Context Adherence evaluation metric and the ChainPoll evaluation method.10closed-sourcemodels12open-sourcemodels3RAGtasksTrends01Open source is closing the gapWhile closed-source models still offer the best performance thanks to proprietary training data, open-source models like Gemini, Llama, and Qwen continue to improve in hallucination performance without the cost barriers of their close-sourced counterparts.02What context length?We were surprised to find models perform particularly well with extended context lengths without losing quality or accuracy, reflecting how far model training and architecture has come.03Larger is not always betterIn certain cases, smaller models outperformed larger models. For example Gemini-1.5-flash-001 outperformed larger models, which suggests that efficiency in model design can sometimes outweigh scale.04Anthropic outperforms OpenAlDuring testing, Anthropic's latest Claude 3.5 Sonnet and Claude 3 Opus consistently scored close to perfect scores, beating out GPT-4o and GPT-3.5, especially in shorter context scenarios.Top Models for RAG ApplicationsOverall Winners for RAGBest performing modelClaude 3.5 Sonnet due to great performance on all tasks with context support up to 200k.Best performance for the costGemini 1.5 Flash due to great performance on all tasks with context support up to 1M.Best performing open-source modelQwen2-72B-Instruct due to great performance in SCR and MCR with context support up to 128k.CategoryProviderDescriptionScoreShort Context RAG(<5k tokens))Learn more Best closed source modelClaude 3.5 Sonnet0.97Best open source modelLlama-3-70b-chat0.95Best affordable modelGemini 1.5 Flash0.94Short Context RAG(<5k tokens))Learn more CategoryScoreBest closed source model0.97Claude 3.5 SonnetBest open source model0.95Llama-3-70b-chatBest affordable model0.94Gemini 1.5 FlashMedium Context RAG(5k to 25k tokens))Learn more Best closed source modelGemini 1.5 Flash1.00Best open source modelQwen1.5-32B-Chat0.99Best affordable modelGemini 1.5 Flash1.00Medium Context RAG(5k to 25k tokens))Learn more CategoryScoreBest closed source model1.00Gemini 1.5 FlashBest open source model0.99Qwen1.5-32B-ChatBest affordable model1.00Gemini 1.5 FlashLong Context RAG(40k to 100k tokens))Learn more Best closed source modelClaude 3.5 Sonnet1.0Best affordable modelGemini 1.5 Pro0.92Long Context RAG(40k to 100k tokens))Learn more CategoryScoreBest closed source model1.0Claude 3.5 SonnetBest affordable model0.92Gemini 1.5 ProOverall Winners for RAGOur team conducted extensive testing over a two month period, with many iterations as newer models were released. We picked three overall winners across all tests and all context lengths.Best ModelClaude 3.5 Sonnet
      We were extremely impressed by Anthropic’s latest set of models.

      Not only was Sonnet able to perform excellently across short,

      medium, and long context windows, scoring an average of 0.97, 1.0,

      and 1.0 respectively across tasks, but the model's support of up

      to a 200k context window suggests it could support even larger

      datasets than we tested.
      Best Performance for the CostGemini 1.5 Flash
        Gemini 1.5 Flash offered a great balance of performance and cost.

        It earned a 0.94, 1.0, and 0.92 across short, medium, and long

        context task types. While not as robust as other models, Gemini

        did this at a fraction of the cost. The $ per Million prompt

        tokens cost was $0.35 for Flash vs. $3 for Sonnet. Even more

        starkly, the $ per Million response token cost was $1.05 for Flash

        vs. $15 for Sonnet. For high-volume applications or use cases

        where some margin of error is acceptable, Flash is a great choice.
      Best Open-Source ModelQwen2-72b-instruct
        A newcomer to the Index, Alibaba launched its Qwen-2 model series

        in June 2024. The Qwen2-72b-instruct model performed on par with

        Meta’s Llama-3-70b-instruct model during short and medium context

        testing. What set Qwen2 apart from other open-source models was

        its supported context length of 128K tokens. For context, the next

        largest supported context length by an open source model was

        Mistral’s Mixtral-8x22b model, which supports a context length of

        64k tokens.
      Model Performance on RAG TasksShort Context RAG (SCR)Medium Context RAG (MCR)Long Context RAG (LCR)Short Context RAG (SCR)Less than 5k tokensThe Short Context RAG seeks to identify the most efficient model for understanding contexts up to 5k tokens. Its primary goal is to detect any loss of information or reasoning capability within these contexts. Similar to referencing select pages in a book, this method is especially suitable for tasks that demand domain-specific knowledge.FiltersPricing for open source models is based on Together.ai as of July 10Result SnapshotCategoryProviderDescriptionScoreBest closed source modelAnthropic’s Claude-3-5-SonnetClaude-3-5-Sonnet and Claude-3-opus tied with scores of 0.97 followed closely by Open AI’s GPT-4o with a score of 0.96. Sonnet was our choice as it achieved this performance at a lower cost than its close competitors. 0.97Worst closed-source modelCohere's Command-r-plusCohere's Command-r-plus earned a 0.86 during short context testing. While this was better than Open AI’s GPT-3.5-Turbo and Google’s Gemini which both scored a 0.84, Cohere’s price was on par with Anthropic’s Claude 3.5 Sonnet at $3/M prompt tokens and $15/M response tokens, making it our choice for the worst closed-source model.0.86Best open-source modelMeta's llama-3-70b-instructAlibaba's qwen2-72b-instruct and Meta's llama-3-70b-instruct tied during testing with scores of 0.95. We chose llama-3-70b-instruct, as qwen2-72b-instruct has 60% longer responses, which could increase the cost.0.95Worst open-source modelGoogle’s Gemma-7b-itGoogle's Gemma-7b-it demonstrated the poorest performance within the 7-billion-parameter category.0.65Best performance for the costGoogle’s Gemini-1.5-flash-001As mentioned in our overall results, Gemini-1.5-flash-001 performed the best at a fraction of the cost of models like Claude-3.5-sonnet, making it our choice for this category.0.94Best small open modelMeta's llama-3-8b-instructMeta's llama-3-8b-instruct surpassed several recent large models like Snowflake's Arctic.0.89RAG Task ExamplesModelLlama-3-70b-chatIs response correct?YesContext Adherence1promptFor every 100 females age 18 and over, there were 94.5 males.Show fullresponseBased on the information, females are the larger gender group. For every 100 females, there were 97.3 males, and for every 100 females age 18 and over, there were 94.5 males. This indicates that there are more females than males in the population.Answer: FemaleModelGPT 3.5 turboIs response correct?NoContext Adherence0promptFor every 100 females age 18 and over, there were 94.5 males.Show fullresponseMaleAnswer: FemaleMedium Context RAG (MCR)5k to 25k tokensThe Medium Context RAG aims to determine the most effective model for comprehending long contexts spanning from 5k to 25k tokens. It focuses on identifying any loss of information and reasoning ability within these extensive contexts. Additionally, we experiment with a prompting technique known as Chain-of-Note to improve performance as it has worked for short context. This task is akin to doing RAG on a few book chapters.FiltersPricing for open source models is based on Together.ai as of July 10Result SnapshotCategoryProviderDescriptionScoreBest closed-source modelGoogle’s Gemini-1.5-flash-001During testing, many models scored a perfect 1.0 score: 

        • Anthropic: claude-3-5-sonnet, claude-3-opus,
• Cohere: command-r-plus 
• Google: gemini-1.5-flash-001, gemini-1.5-pro-001 
• Mistral: mistral-large 
• OpenAI: gpt-4o-2024-05-13

    We ultimately chose Gemini-1.5-flash-001 for its low cost.
 1.00Worst closed-source modelAnthropic’s Claude-3-HaikuClaude-3-haiku performed the worst but still scored a 0.96.0.96Best open-source modelAlibaba's qwen2-72b-instructAlibaba's qwen2-72b-instruct scored a perfect 1.0 and had flawless performance up to 25k tokens. Note that llama-3-70b-instruct does not support beyond 8K context length.1.00Worst open-source modelMistral-7b-instruct-v0.3Mistral-7b-instruct-v0.3 had good performance however when considering cost, we felt there were better options.0.94Best performance for the costGoogle’s Gemini-1.5-flash-001Gemini-1.5-flash-001 scored a perfect 1.0. Similar to our rationale for when evaluating models for small context testing, Gemini-1.5-flash-001 also performed the best at a fraction of the cost, making it our choice for this category.1.00Best small open modelAlibaba's qwen2-7b-instructAlibaba's qwen2-7b-instruct scored the best among the 7b models.0.96Recall heatmapThis heatmap shows the model's ability to recall information in different parts of the context. The x-axis represents the length of the context during the experiment, and the y-axis represents the location of the information. Green indicates successful recall, while red indicates failure.claude-3-5-sonnet-20240620SimpleHeatmap for claude-3-5-sonnet-20240620RAG Task ExamplesModelGemini FlashIs response correct?YesContext Adherence1promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe answer is **2.7%**. 


This information is found in **Document 2**, which states: "In fiscal year 2024, our overall turnover rate was 2.7%.Answer: 2.7%ModelMistral-7B-Instruct-v0.3Is response correct?NoContext Adherence0promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe documents do not provide the information about the percentage of employees who left the company during FY 2024.Answer: 2.7%Long Context RAG (LCR)40k to 100k tokensThis heatmap shows the model's ability to recall information in different parts of the context. The x-axis represents the length of the context during the experiment, and the y-axis represents the location of the information. Green indicates successful recall, while red indicates failure.FiltersResult SnapshotCategoryProviderDescriptionScoreBest closed-source modelAnthropic’s Claude-3-5-SonnetDuring long context testing, many models scored a perfect 1.0 score, highlighting the power and quality of models from top-tier providers. Among those scoring a perfect 1.0 were:

    • Anthropic: claude-3-5-sonnet, claude-3-opus
• Google: gemini-1.5-pro-001
• Open AI: gpt-4o

    Between these models, it came down to Claude-3-5-Sonnet and Gemini-1.5-pro-001, which were both comparable on performance and price. Ultimately we chose Claude-3-5-Sonnet due to its better performance across short and medium-context testing.
 1.00Worst closed-source modelAnthropic’s Claude-3-HaikuClaude-3-haiku performed the worst with a score of 0.7. During testing, the model struggled to maintain accuracy with context lengths greater than 60k tokens.0.70Best performance for the costGoogle’s Gemini-1.5-flash-001Google's gemini-1.5-flash-001 scored an impressive 0.92 and only faced issues when provided with 80000 token context length.0.92Recall heatmapAs the name suggests, this heatmap indicates the model's success in recalling information at different locations in the context. Green signifies success, while red indicates failure.claude-3-5-sonnet-20240620SimpleHeatmap for claude-3-5-sonnet-20240620RAG Task ExamplesModelClaude 3.5 SonnetIs response correct?YesContext Adherence1promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseAccording to the document, NVIDIA's overall employee turnover rate in fiscal year 2024 was 2.7%.Answer: 2.7%ModelCommand R PlusIs response correct?NoContext Adherence0promptExport controls targeting GPUs and semiconductors associated with AI, which have been imposed and are increasingly likely to be further tightened, would further restrict our ability to export our technology, products, or services even though competitors may not be subject to similar restrictions, creating a competitive disadvantage for us and negatively impacting our business and financial results.Show fullresponseThe overall turnover rate was 5.3%.Answer: 2.7%Model comparisonDatasetclaude-3-5-sonnet-20240620claude-3-haiku-20240307Model InfoDeveloperAnthropicAnthropicLicenseNA (private model)NA (private model)Model ParametersNA (private model)NA (private model)Supported context length200k200k$/M prompt tokens30.25$/M response tokens151.25Cost insightPrompt cost compared to Claude 3.5 Sonnet1.00x0.08xPrompt cost compared to GPT-4o0.60x0.05xPrompt cost compared to Gemini 1.5 Pro0.86x0.07xPrompt cost compared to Llama-3-70b3.33x0.28xPerformanceShort context RAG0.970.92Medium context RAG1.000.96Long context RAG1.000.70Task InsightsSCR insightThe model demonstrates exceptional reasoning and comprehension skills, excelling at short context RAG. It outperforms other models in mathematical proficiency, as evidenced by its strong performance on DROP and ConvFinQA benchmarks. This makes it the most affordable top tier model for RAG.The model demonstrates exceptional reasoning and comprehension skills, excelling at short context RAG. It shows good mathematical proficiency, as evidenced by its performance on DROP and ConvFinQA benchmarks. It comes out as one of the best small closed source model.MCR insightFlawless performance making it suitable for any context length upto 25000 tokens.Great powerformance overall with minor problems for context more than 10000 tokens.LCR insightFlawless performance making it suitable for any context length upto 100000 tokens.Model shows issues at all context lengths and shows poor performance after 60000 making it unsitable for long context use.Performance on datasetsHere is the performance of models on each dataset. The datasets are selected to test for different capabilities ranging from robustness to noise to the ability to do math.claude-3-5-sonnet-20240620 Read the full reportLLM Hallucination IndexHomeMethodology