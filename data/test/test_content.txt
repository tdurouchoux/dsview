The Orange Book of Machine Learning
The essentials of making predictions using supervised
regression and classification for tabular data
Carl McBride Ellis
Copyright © 2024 Carl McBride Ellis
ORCiD: 0000-0003-2966-7530
email: carl.mcbride@protonmail.ch
LinkedIn: www.linkedin.com/in/carl-mcbride-ellis
Book GitHub: github.com/Carl-McBride-Ellis/TOBoML
First ‘Orange’ edition, June 2024
(version 1.0.1)
Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.1
The ˆ
β and the ˆ
y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2
Interpolation and curve fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3
Errors and residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.4
Sources of uncertainty: aleatoric and epistemic . . . . . . . . . . . . . . . . 10
1.5
Confidence and prediction intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.6
Explainability and interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2
Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1
Centrality: Mean, median, and mode . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2
Dispersion: Variance, MAD, and quartiles . . . . . . . . . . . . . . . . . . . . . . 14
2.2.1
Quantiles, quartiles and the interquartile range (IQR) . . . . . . . . . . . . . . . 15
2.3
Gaussian distribution: additive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3.1
Tests for normality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.4
Chebyshev’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.5
Galton distribution: multiplicative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.6
Skewness and kurtosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3
Exploratory data analysis (EDA) . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.1
Data quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2
Getting to know your dataframe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.1
The curse of dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.2.2
Descriptive statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3
Anscombe’s quartet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.4
Box, violin and raincloud plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.5
Outliers, inliers and extreme values . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.6
Correlation coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.6.1
Mutual information (MI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.7
Scatter plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.8
Histograms and eCDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.8.1
Kolmogorov-Smirnov test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.9
Pairplots (or not) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4
Data cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1
Missing values: NULL and NaN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1.1
Visualization of NaN with missingno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1.2
MCAR, MAR, and MNAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.1.3
Global fill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1.4
Global delete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1.5
Average value imputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1.6
Multiple imputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.1.7
Do nothing! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.1.8
Binary indicator column . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2
Outliers and inliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.1
Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.2
Inliers: Isolation forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.3
Duplicated rows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.4
Boolean columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.5
Zero variance columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.6
Feature scaling: standardization and normalization . . . . . . . . . . . . . 42
4.7
Categorical features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.7.1
Ordinal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.7.2
Nominal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
5
Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.1
Train test split . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.2
Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.3
Nested cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.4
Data leakage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.5
Covariate shift and Concept drift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
6
Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
6.1
Regression baseline model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
6.2
Univariate linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6.3
Calculating β1 and β0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
6.3.1
Ordinary least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
6.3.2
Normal equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
6.3.3
Scikit-learn LinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
6.4
Assumptions of linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
6.5
Polynomial regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
6.6
Extrapolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6.6.1
Convex hull . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6.7
Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
6.8
The loss and cost functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
6.8.1
Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
6.9
Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.9.1
Root mean square error (RMSE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.9.2
Mean absolute error (MAE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.9.3
The R2 metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.10
Decision tree regressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.10.1 Hyperparameter: max_depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
6.11
Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
6.11.1 Parametric models: regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
6.11.2 Tree models: min_samples_leaf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
6.12
Quantile regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
6.12.1 Pinball loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
6.13
Conformal prediction intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6.13.1 Conformalized quantile regression (CQR) . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.13.2 Locally-weighted conformal regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6.13.3 Prediction interval metric: Winkler interval score . . . . . . . . . . . . . . . . . . . . 86
6.14
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
7
Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
7.1
Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
7.1.1
Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.2
Log-loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.3
Decision tree classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
7.4
Classification baseline model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
7.5
Classification metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
7.5.1
Strictly proper scoring rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
7.5.2
Accuracy score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
7.5.3
Confusion matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
7.5.4
Precision and recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
7.5.5
Decision threshold
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
7.5.6
AUC ROC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
7.6
Imbalanced classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
7.6.1
What to do about imbalanced data? . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7.7
Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7.8
No free lunch theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
7.9
Classifier calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
7.9.1
Reliability diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.9.2
Venn-ABERS calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
7.10
Multiclass classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
7.10.1 Multiclass metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
8
Ensemble estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
8.1
Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
8.1.1
Bootstrapping: row subsampling with replacement . . . . . . . . . . . . . . . . 109
8.1.2
Feature subsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
8.1.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
8.2
Weak learners and boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
8.2.1
AdaBoost (Adaptive Boosting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
8.3
Gradient boosted decision trees (GBDT) . . . . . . . . . . . . . . . . . . . . . . 113
8.3.1
Extrapolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
8.4
Convex combination of model predictions (CCMP) . . . . . . . . . . . . 116
8.5
Stacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
9
Hyperparameter optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 119
10
Feature engineering and selection . . . . . . . . . . . . . . . . . . . . . 123
10.1
Feature engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
10.1.1 Interaction and cross features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
10.1.2 Bucketing of continuous features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
10.1.3 Power transforms: Yeo-Johnson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
10.1.4 User defined transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
10.1.5 External secondary features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
10.2
Feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
10.2.1 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
10.2.2 Permutation importance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
10.2.3 Stepwise regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
10.2.4 LASSO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.2.5 Boruta trick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.2.6 Native feature importance plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.3
Principal component analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . . . 129
11
Why no neural networks/deep learning? . . . . . . . . . . . . . . 131
11.0.1 Single neuron regressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
11.0.2 Single neuron classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Essential reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
This book is based on my five day course which I had the pleasure of teaching in the fol-
lowing Spanish cities: A Coruña, Algeciras, Alicante, Bilbao, Cáceres, Granada, Huesca,
Jaén, Madrid, Málaga, Murcia, Sevilla, Valencia, Valladolid, and Zaragoza. I would like to
take this opportunity to say a big thank you to all of my students ¡un gran placer!
This book uses the python programming language largely in conjunction with the scikit-learn
machine learning library, and pandas for data manipulation. All example notebooks use
the Jupyter Notebook environment.
Special thanks to: Carlos Ortega, Santiago Mota Herce, Norberto Malpica, Charles H.
Martin, Roland Stevenson, Adrian Olszewski, Valeriy Manokhin, and everyone on Kaggle.
For Cristina
1. Introduction
Without data, you’re just another
person with an opinion.
William Edwards Deming
1.1
The ˆ
β and the ˆ
y
A statistical model, designed for explanation or inference, will use a simple predefined
function (based on the assumption that the data can be modelled by a parametric distri-
bution) and will then seek the best parameters, ˆ
β, for said function. A non-parametric
(i.e. making no assumptions about the distribution of the data) machine learning model
forgoes explanatory power for predictive power, with the emphasis on producing the best
predictions, ˆ
y.
Equation: Fundamental assumption: our data is composed of signal + noise
y = f(x)+ε
(1.1)
where f(x) is the ground truth and ε is the irreducible error.
In multiple dimensions f(x) will form a surface, and our objective is to produce the best
possible fit to this surface. This short book is dedicated to introducing us to the essentials
of a number of techniques for producing such a fit: this book is all about the ˆ
y.
1.2
Interpolation and curve fitting
Interpolation passes through every single data point, whereas curve fitting does not have to
do this. Indeed to some great extent machine learning can be thought of as little more than
glorified cure fitting. Ideally we fit a curve to all of the signal, and to none of the noise. If
it were not for noise, often leading to y being multivalued, then interpolation would have
done the job just fine from a ˆ
y point of view (but would be useless from a ˆ
β viewpoint).
10
Chapter 1. Introduction
1.3
Errors and residuals
The error is the difference between the observed value and the ideal value. For example,
a chocolate bar may have the ideal value printed on the packet, say 100g. However, no
chocolate bar has ever been made that weights exactly 100g, and the difference between the
actual weight and ideal 100g is the error. On the other hand the residual is the difference
between the predicted value and the observed value. Note however that in this text when
we use the word error correctly speaking we should really use the word residual, and we do
this simply because of the prevalence of the word error in the machine learning literature,
for example the mean absolute error (MAE) or the root mean square error (RMSE).
1.4
Sources of uncertainty: aleatoric and epistemic
Aleatoric, or stochastic, uncertainty is due to inherent random noise in a system/generating
process. Epistemic uncertainty is due to a lack of data.
Figure 1.1: Aleatoric and epistemic uncertainty in a dataset.
1.5
Confidence and prediction intervals
Confidence intervals quantify the uncertainty in the parameters β. Prediction intervals
quantify the uncertainty in the predictions ˆ
y. We shall see how to calculate regression
prediction intervals using conformal prediction in Section 6.13.
1.6
Explainability and interpretability
• Explainability: the ability to understand both the ‘how’ and the ‘why’ of a prediction
• Interpretability: the ability of understand the ‘why’ of a prediction
Explainability is the forte of statistical models; the ‘how’ is in-built from the very start
by using simple models for the data. On the other hand the focus of machine learning
(ML) is almost exclusively on prediction performance. Only the simplest ML models are
interpretable; decisions trees are in principle very easy to explain, but even at depth 3 (8
leaves) and having several features then in reality they quickly become hard to interpret.
There are packages such as SHAP (SHapley Additive exPlanations) and LIME (Local
Interpretable Model-Agnostic Explanations) to facilitate interpretability. However, it has
1.6 Explainability and interpretability
11
been seen that often even the data scientists themselves do not know how to use these
packages properly1:
“...results indicate that data scientists over-trust and misuse interpretability
tools. Furthermore, few of our participants were able to accurately describe
the visualizations output by these tools”.
which does not bode well. To make matters worse of late some of the uses that people make
of the SHAP technique, such as posing counterfactual questions, have been brought into
question2, 3. At the end of the day machine learning models are designed for performance,
and generally interpretability takes a back seat. However, in many circumstances model
interpretability is of paramount importance; indeed under the EU General Data Protection
Regulation (GDPR) 2016/679 (Article 15 1h)
“The data subject shall have the right to obtain...the following information:
the existence of automated decision-making...meaningful information about
the logic involved, as well as the significance and the envisaged consequences
of such processing for the data subject.”
With the ever increasing rôle that machine learning models play in modern society the
creation of interpretability tools is an active area of development.
1Kaur et al. “Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools
for Machine Learning”, Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
pp. 1-14 (2020)
2Huang, Marques-Silva “The Inadequacy of Shapley Values for Explainability”, arXiv:2302.08160
(2023)
3Bilodeau, Jaques, Wei Koh, Kim “Impossibility Theorems for Feature Attribution”, arXiv:2212.11870
(2024)
12
Chapter 1. Introduction
Recommended reading
Papers
• Leo Breiman “Statistical Modeling: The Two Cultures”, Statistical Science 16 pp.
199-231 (2001)
• Galit Shmueli “To Explain or to Predict?”, Statistical Science 25 pp. 289-310
(2010)
• Gruber et al. “Sources of Uncertainty in Machine Learning - A Statisticians’ View”,
arXiv:2305.16703 (2023)
Books
• Denis Rothman “Hands-On Explainable AI (XAI) with Python”, Packt Publishing
Limited (2020)
• Serg Masís “Interpretable Machine Learning with Python”, Packt Publishing Lim-
ited (2023)
• Christoph Molnar “Interpretable Machine Learning”, (online book)
Packages
• SHAP
• LIME
2. Statistics
60% of the time, it works every time
Brian Fantana in ‘Anchorman’
In this chapter we briefly cover some essential statistical concepts that are very useful
when it comes to understanding our data and machine learning estimators.
2.1
Centrality: Mean, median, and mode
The mean and the median are measures of the central tendency of a distribution.
Equation: The arithmetic mean of a sample is given by
¯
y := 1
n
n
∑
i=1
yi
(2.1)
The mean of the hypothetical population, from which the sample was supposedly
obtained, is denoted by µ.
The median (˜
y) of a set of values is a number such that half of the values are below the
median value, and the other half are above the median value. The median is said to be a
robust statistic in that it is less influenced by a few extreme or outlier values. For example
here we calculate the mean and the median for an array of values using the python numpy
library
import numpy as np
np.mean([1,2,3,4,5,6,1001])
146.0
np.median([1,2,3,4,5,6,1001])
4.0
14
Chapter 2. Statistics
The mode is the most frequently occurring value, and can be useful when working with a
discrete distribution (such as count data), categorical features, or for quickly identifying a
zero-inflated distribution.
2.2
Dispersion: Variance, MAD, and quartiles
In Figure 2.1 we have two distributions that have exactly the same mean value.
Figure 2.1: Two distributions with same central tendency.
We can see that centrality only tells part of the story and evidently we also need to be able
to describe how spread out a distribution is as well.
Equation: The variance, σ2, is given by
σ2(y) := 1
n
n
∑
i=1
(yi −y)2
(2.2)
and σ is known as the standard deviation.
Remark: When calculating the variance or standard deviation by default, unlike numpy,
pandas applies the Bessel correction of n−1.
Equation: The sample covariance, between the ordered pair x and y, is given by
cov(x,y) =
1
(n−1)
n
∑
i=1
(xi −x)(yi −y)
(2.3)
In 2-dimensions we can now also construct a variance-covariance matrix:
K =
 σ2(x)
cov(x,y)
cov(y,x)
σ2(y)

(2.4)
(note that cov(x,y) = cov(y,x) so this matrix is symmetric).
The median absolute deviation is the robust statistic for dispersion.
2.3 Gaussian distribution: additive
15
Equation: The median absolute deviation (MAD) is given by
MAD(y) = median(|yi −˜
y|)
(2.5)
Remark: For some reason the pandas mad is actually the mean absolute deviation
and not the median absolute deviation.
2.2.1
Quantiles, quartiles and the interquartile range (IQR)
The median is a special case of a quantile, and is denoted as Q2. Two other quantiles of
note are Q1, below which lies 25% of the values, and Q3, above which also lies 25% of
the data. Suffice to say the other 50% of the data has values between Q1 and Q3 and is
known as the interquartile range (IQR).
import numpy as np
values = [1,2,3,4,5,6,7,8,9,10,11,12]
np.quantile(values, 0.25) # Q1
3.75
np.quantile(values, 0.5)
# Q2
6.5
np.quantile(values, 0.75) # Q3
9.25
np.quantile(values, 0.75) - np.quantile(values, 0.25) # IQR
5.5
Quantiles are excellent for producing prediction intervals as we shall see later in quantile
regression.
2.3
Gaussian distribution: additive
Equation: The Gaussian probability density function (PDF) is given by
f(x) =
1
σ
√
2π e−1
2( x−µ
σ )
2
(2.6)
where µ is the mean and σ is the standard deviation.
